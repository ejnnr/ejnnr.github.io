<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>Building Blocks of RL Part II: Policy Optimization :: Erik Jenner</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="This is part 2 of a three-part series. Part 1 covers value-based methods and also gives some introduction and defines some notation.
So far, we have looked at the building blocks necessary to learn value functions for a given policy, called policy evaluation. We have also seen that with GPI, we can use policy evaluation for control, i.e. to find optimal value functions. The policy was always derived from the value function, by picking actions (\(\varepsilon\)-)greedily."/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://ejnnr.github.io/posts/rl-building-blocks-2/" />





<link rel="stylesheet" href="https://ejnnr.github.io/assets/style.css">


<link rel="stylesheet" href="https://ejnnr.github.io/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://ejnnr.github.io/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://ejnnr.github.io/img/favicon.png">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Building Blocks of RL Part II: Policy Optimization"/>
<meta name="twitter:description" content="  Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is part 2
  of a series with such an overview, covering some policy optimization methods.
  "/>



<meta property="og:title" content="Building Blocks of RL Part II: Policy Optimization" />
<meta property="og:description" content="  Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is part 2
  of a series with such an overview, covering some policy optimization methods.
  " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ejnnr.github.io/posts/rl-building-blocks-2/" />
<meta property="article:published_time" content="2021-02-03T07:39:00+01:00" />
<meta property="article:modified_time" content="2021-02-03T07:39:00+01:00" /><meta property="og:site_name" content="Erik Jenner" />






  </head>
  <body class="">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">Erik Jenner</span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title"><a href="https://ejnnr.github.io/posts/rl-building-blocks-2/">Building Blocks of RL Part II: Policy Optimization</a></h1>
    <div class="post-meta">
      
        <span class="post-date">
          2021-02-03
        </span>

        
          
        
      

      
      
        <span class="post-read-time">— 11 min read</span>
      
    </div>

    

    

    <div class="post-content">
      
      <p><em>This is part 2 of a three-part series. <a href="/posts/rl-building-blocks-1/">Part 1</a> covers value-based methods and
also gives some introduction and defines some notation.</em></p>
<p>So far, we have looked at the building blocks necessary to learn value functions
for a given policy, called policy evaluation. We have also seen that with GPI, we can use policy evaluation
for control, i.e. to find optimal value functions. The policy was always derived
from the value function, by picking actions (\(\varepsilon\)-)greedily.</p>
<p>In this post, we take a more direct approach to control: what we really
want to learn is a good policy, so why not optimize the policy directly, without
the detour of learning a value function?</p>
<h2 id="policy-optimization-and-policy-gradient-methods">Policy optimization and policy gradient methods</h2>
<p>Policy optimization in general means that we have a parameterized
family of policies \(\pi_\theta(a|s)\) and want to maximize the expected
return with respect to the parameters \(\theta\):
\[\operatorname*{argmax}_{\theta} J(\theta)\]
where \(J\) is the expected return:
\[J(\theta) := \mathbb{E}_{\tau \sim \pi_\theta, \mu} R(\tau)\]
Here \(\tau\) is a trajectory which is sampled using the initial state distribution
\(\mu\) and policy \(\pi_\theta\). \(R(\tau)\) is the return of that trajectory.</p>
<p>In principle, there are many ways we could solve this optimization problem.
For example, we could perform a grid search over parameters \(\theta\) and evaluate
the expected return for each parameter by sampling lots of episodes. But this
wouldn&rsquo;t scale well (\(\theta\) might very well be a vector with millions of dimensions
if we use Deep RL). In practice, most methods instead use stochastic gradient
ascent or variations thereof and that is all we will cover in this post.</p>
<p>One sidenote before we dive in: why do we use a parameterized policy at
all? For value-based methods, we started in a tabular setting, where we
could directly assign values to each state. The difference is that even
in a tabular setting, the policy is not an arbitrary function
&ndash; it has to be normalized over actions. So we can&rsquo;t just update a single
probability \(\pi(a|s)\) without also adjusting others.</p>
<h2 id="some-theory-the-policy-gradient-theorem">Some theory: the policy gradient theorem</h2>
<p>If you&rsquo;re only interested in a description of some policy optimization methods, you can
skip this and the next section. But it sheds some light onto why these methods are designed
the way they are and why they work.</p>
<p>We want to optimize the expected return \(J(\theta)\). To see what that entails, we can
write it out explicitly as
\[J(\theta) := \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} q^{\pi_\theta}(s, a) \pi_\theta(a|s)\]
where \(\mu^\pi\) is the on-policy state distribution of \(\pi\). To optimize
this function using gradient ascent, we need to find \(\nabla_\theta J(\theta)\).
But finding the gradient of \(\mu^{\pi_\theta}\) with respect to \(\theta\) is not really possible
if we don&rsquo;t know the dynamics of the environment.</p>
<p>Fortunately, the policy gradient theorem comes to the rescue. It states
that
\[\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} q^{\pi_\theta}(s, a) \nabla_\theta \pi_\theta(a|s) \]
i.e. we can just apply the gradient to the policy itself and don&rsquo;t need to know
how the state distribution depends on the policy. Section 13.2 of
<a href="http://incompleteideas.net/book/the-book.html">Sutton and Barto&rsquo;s textbook</a> contains more details and a proof.</p>
<p>Another very useful fact is that we can subtract a baseline from the state value:
\[\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \left(q^{\pi_\theta}(s, a) - b(s)\right) \nabla_\theta \pi_\theta(a|s) \]
(the only difference to the previous equation is the \(- b(s)\) term).
This fact is also sometimes called the policy gradient theorem. \(b(s)\) may be any function
or random variable, as long as it doesn&rsquo;t depend on \(a\).</p>
<h2 id="more-theory-score-function-estimators">More theory: score function estimators</h2>
<p>Typically, we will still be unable to evaluate the gradient
\[\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} q^{\pi_\theta}(s, a) \nabla_\theta \pi_\theta(a|s) \]
analytically. Some types of gradients of an expected value
can be estimated by sampling:
\[\nabla_x \mathbb{E}_{x \sim p} f(x) = \mathbb{E}_{x \sim p} \nabla_x f(x)\]
so if we can sample from \(p\) and can calculate \(\nabla f(x)\), we can
estimate this gradient. But our case is different: ignoring the expectation
over states (which doesn&rsquo;t pose a problem), we want to evaluate a gradient
of the form
\[\nabla_\theta \mathbb{E}_{a \sim \pi_\theta(a)} f(a)\]
The variable \(\theta\), with respect to which we differentiate, appears in the
distribution, so we can&rsquo;t just approximate this gradient by sampling as
we did in the other case.</p>
<p>Gradients of this form (called <em>stochastic gradients</em>) appear often in machine
learning, not just in RL. One method to calculate them is the reparameterization
trick, which you might know from variational autoencoders, but that requires
assumptions that often aren&rsquo;t met in RL. What we will use instead is
the REINFORCE method or <em>score function estimation</em>.
We can use the fact that \(\nabla g(x) = g(x) \nabla \log g(x)\) for any \(g\) and write
\[\nabla_\theta \mathbb{E}_{a \sim \pi_\theta} f(a) = \int_a f(a)\nabla \pi_\theta(a) da
= \int_a \pi_\theta(a) f(a) \nabla \log \pi_\theta(a) da = \mathbb{E}_{a \sim \pi_\theta} \left[f(a)\nabla \log \pi_\theta(a)\right]\]
The right hand side has the form we can deal with: an expectation over
some term, with a probability distribution we can sample from.
As long as we can evaluate \(\nabla \log \pi_\theta\), we can now estimate
the gradient we need.</p>
<p>Recall that \(f(s, a) = q^{\pi_\theta}(s, a) - b(s)\) where \(b\) is an arbitrary
baseline. But more generally, we can use any function \(f(s, a)\) which has
\(q^{\pi_\theta}(s, a) - b(s)\) as its expected value, and we will get an unbiased
estimator for the gradient \(\nabla J(\theta)\). Keep this in mind and the various
update targets we will soon see should make sense.</p>
<h2 id="the-general-formula">The general formula</h2>
<p>Similar to value-based methods, we can generate many algorithms
for policy optimization using a single update equation.</p>
<p>Using the estimator for the gradient \(\nabla J\), we can learn
the parameter \(\theta\) of the policy with stochastic gradient ascent.
We will use samples \(s_1, a_1, r_2, s_2, a_2, \ldots\) and then
update according to
\[\theta \gets \theta + \alpha \sum_t \Psi_t \nabla \log p_\theta(a_t|s_t)\]
where \(\Psi_t\) is some estimate of \(q^{\pi_\theta}(s_t, a_t)\) and \(\alpha\) is a learning rate.</p>
<p>Later, we will generalize this to
\[\theta \gets \theta + \alpha \sum_t \Psi_t g_t\]
where the gradient \(\nabla \log \pi_\theta\) is replaced by a more
general vector \(g_t\) that determines the direction of the update.</p>
<p>In the next section, we cover possible choices for \(\Psi_t\), and in the
section after that we will look at choices for \(g_t\).</p>
<h2 id="targets">Targets</h2>
<p>Recall from the section on score function estimators that \(\Psi_t\)
should be an estimate of \(q^{\pi_\theta}(s_t, a_t)\).
This means that we can use many of the targets we&rsquo;ve already seen in part 1.
In addition, we can subtract a <em>baseline</em> \(b(s_t)\) without changing
the expected value of the update. In principle, \(b(s_t\)) can be any function
of the state, but to reduce variance as much as possible, we usually use a learned
state-value function, leading to so-called Actor-Critic methods. The baseline
can be learned using any of the methods for learning \(v_\pi\) from part 1 (or other policy evaluation
methods).</p>
<p>Here then are typical targets we can use:</p>
<dl>
<dt>Monte Carlo</dt>
<dd>\(\Psi_t = G_0\) or \(\Psi_t = G_t\) i.e. total or future return</dd>
<dt>MC with baseline</dt>
<dd>\(\Psi_t = G_0 - V(s_t)\) or \(\Psi_t = G_t - V(s_t)\)</dd>
<dt>n-step TD with baseline</dt>
<dd>\(\Psi_t = G_{t:t+n} - V(s_t)\) (of course the baseline is optional)</dd>
<dt>Generalized Advantage Estimation</dt>
<dd>\(\Psi_t = G_t^{\lambda} - V(s_t)\) where
\[G_t^{\lambda} := (1 - \lambda) \sum_{n = 1}^\infty \lambda^{n - 1} G_{t:t + n}\]
is the \(\lambda\)-return. This is the TD(\(\lambda\)) target with a baseline (which again
is in principle optional but helps to reduce variance)</dd>
</dl>
<p>This is where the &ldquo;building blocks&rdquo; perspective really starts paying off: value-based
methods and policy optimization are very different approaches in terms of their
large-scale design, but on a smaller level, they are composed of some of the same
parts.</p>
<h2 id="updating-methods-vpg-npg-trpo">Updating methods: VPG, NPG, TRPO</h2>
<p>As promised, we can not only choose the target \(\Psi_t\) but also
have some freedom when it comes to the vector \(g_t\) in whose
direction we update the parameter \(\theta\).</p>
<p>The simplest option is Vanilla policy gradient (VPG), which uses
\(g_t = \nabla \log \pi_\theta(a_t|s_t)\). This is what we&rsquo;ve already seen,
it corresponds to stochastic gradient ascent on \(J(\theta)\).</p>
<p>But this simple method has its drawbacks: gradient descent leads
to small changes in the parameter \(\theta\), but it doesn&rsquo;t make
any guarantees about the changes in the policy \(\pi\) itself. If the
policy is very sensitive to the parameter around some value \(\theta_0\),
then taking a gradient step from there might change the policy a lot
and actually make it worse. To avoid that, we&rsquo;ll need to use a small
learning rate, which slows down convergence.</p>
<p>The solution is to use the Natural Policy Gradient (NPG<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>) instead of
the usual gradient. Instead of limiting the size of the step in parameter
space, it directly limits the change of the policy at each step (well, not
really<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>
but that&rsquo;s the intuition).
Natural gradients are a general method for finding optimal probability distributions,
not specific to Reinforcement Learning,
but NPG is probably their most well-known application.</p>
<p>Computationally, the natural gradient is just the normal gradient multiplied by
the inverse Fisher matrix \(F^{-1}\) of the policy. If you want to know
more, <a href="http://www.scholarpedia.org/article/Policy%5Fgradient%5Fmethods#Natural%5FPolicy%5FGradients">the Scholarpedia article</a> has some details.</p>
<p>For both of these methods, we use a constant learning rate \(\alpha\)
(or one that is adapted using a fixed schedule, \(\alpha = \alpha(t)\)).
The update vector \(g_t\) is given by:</p>
<dl>
<dt>VPG</dt>
<dd>\(g_t = \nabla \log \pi_\theta(a_t|s_t)\)</dd>
<dt>NPG</dt>
<dd>\(g_t = F^{-1} \nabla \log \pi_\theta(a_t|s_t)\)
where \(F\) is the Fisher matrix of the policy</dd>
</dl>
<p>A third option is Trust-Region Policy Optimization (TRPO)<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.
The motivation is similar to that of NPG: limit how much the policy changes (in terms of the
KL divergence). But it takes that idea further and actually guarantees
an upper bound on how much the policy will change.</p>
<p>We can fit TRPO into our framework by using the same update vector as NPG
with a learning rate that adapts at each step:</p>
<dl>
<dt>TRPO</dt>
<dd>\(g_t = F^{-1} \nabla \log \pi_\theta(a_t|s_t)\) and the adaptive learning rate
\(\alpha = \beta^j \sqrt{\frac{2\delta}{\tilde{g} F^{-1} \tilde{g}}}\)
where \(\tilde{g} := \Psi_t g_t\)
<p>\(\beta \in (0, 1)\) and \(\delta\) are hyperparameters and \(j \in \mathbb{N}_0\)
is chosen minimally such that a constraint on the KL divergence between old and
new policy is satisfied</p>
</dd>
</dl>
<p>Each of these updating methods can be combined with any of the targets,
yielding a 2D grid of algorithms. In practice, some combinations are of course
preferred, for example TRPO is typically used together with GAE. But these
two aren&rsquo;t connected in a fundamental way, it&rsquo;s simply a choice that works
well.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We&rsquo;ve seen that just like for value-based methods, we can get many different
policy optimization methods by plugging in different terms into a single
update equation. Moreover, the targets we can plug in aren&rsquo;t new: we&rsquo;ve
used them for value-based methods too.</p>
<p>On the other hand, we&rsquo;ve of course only scratched the surface when it comes
to policy optimization methods. For example, we didn&rsquo;t look at deterministic policy
gradients or at PPO. And admittedly these methods don&rsquo;t fit into the framework
presented here as neatly as the ones we did consider. Furthermore, as I already discussed
in Part 1, there are many details that determine whether a method will actually
work in practice that we didn&rsquo;t consider at all.</p>
<p>So I don&rsquo;t want to create the false impression that all of policy optimization can
be reduced to picking a target and an update vector. My goal is rather to convince
you that thinking of RL methods as a combination of several composable building
blocks is a better mental model than thinking about each method individually.
The methods presented here simply fit this mental model especially well: you can combine
any of the targets with any of the updating methods, so the building blocks are
in some sense independent pieces.</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up</a> by OpenAI has explanations and implementations for several policy optimization
algorithms. If you&rsquo;d like a more practical perspective, this is a good place to start</li>
<li>Lilian Weng has a long <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">list of many policy gradient methods</a> that might serve as an overview
or as a quick reference for how a given method works</li>
<li>More on score function estimators: <a href="http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/">http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/</a>
and <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Sham Kakade, 2001. <a href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf">(pdf link)</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Vanilla gradient descent can be interpreted as follows: we want to find \(\Delta \theta\) such that \(J(\theta + \Delta \theta)\) is maximized, but under the constraint that the update isn&rsquo;t too large, \(\Vert\Delta \theta\Vert_2 \leq c\). As \(c\) gets smaller, the optimal update \(\Delta \theta\) aligns more and more with the gradient \(\nabla J(\theta)\). NPG does the same using the Kullback-Leibler divergence between \(\pi_\theta\) and \(\pi_{\theta + \Delta \theta}\) instead of the \(L^2\) distance between the parameters. So <em>infinitesimally</em> (i.e. as the learning rate approaches zero), it limits the change in the policy, but it doesn&rsquo;t actually give any guarantees in the finite regime. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>John Schulman et al., 2015. <a href="https://arxiv.org/abs/1502.05477">(arxiv link)</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
          </div>
          <div class="pagination__buttons">
            
              <span class="button previous">
                <a href="https://ejnnr.github.io/posts/boring-numbers/">
                  <span class="button__icon">←</span>
                  <span class="button__text">Boring numbers, complexity and Chaitin&#39;s incompleteness theorem</span>
                </a>
              </span>
            
            
              <span class="button next">
                <a href="https://ejnnr.github.io/posts/too-much-structure/">
                  <span class="button__text">Too much structure</span>
                  <span class="button__icon">→</span>
                </a>
              </span>
            
          </div>
        </div>
      
    


    
      
        

      
    

    </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright copyright--user"><div><a href='/legal'>Legal</a></div><div>Powered by Hugo. Theme: Hello Friend by panr</div></div>
    
  </div>
</footer>

<script src="https://ejnnr.github.io/assets/main.js"></script>
<script src="https://ejnnr.github.io/assets/prism.js"></script><script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>
<script async src="/mathjax/tex-chtml.js"></script>




      
    </div>

    
  </body>
</html>
