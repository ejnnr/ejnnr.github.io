<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>Trading off speed against the probability of success in the Karger-Stein Algorithm :: Erik Jenner</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="(Note: this is an analysis of one aspect of the Karger-Stein algorithm, it&amp;rsquo;s not meant to be a beginner-friendly introduction)
Karger&amp;rsquo;s algorithm randomly contracts a graph and surprisingly, this can be used to find a minimum cut with probability \(\mathcal{O}(n^{-2})\), where \(n\) is the number of vertices of the graph. This is a much, much higher probability than sampling a graph cut uniformly at random would give! But it still means we need to run the algorithm \(\mathcal{O}(n^2\log n)\) times to get a high success probability."/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://ejnnr.github.io/posts/karger-stein/" />





<link rel="stylesheet" href="https://ejnnr.github.io/assets/style.css">


<link rel="stylesheet" href="https://ejnnr.github.io/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://ejnnr.github.io/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://ejnnr.github.io/img/favicon.png">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Trading off speed against the probability of success in the Karger-Stein Algorithm"/>
<meta name="twitter:description" content="  The Karger-Stein algorithm is an improvement over Karger&#39;s beautiful contraction
  algorithm for minimum graph cuts. In this post, I show how it finds the perfect
  tradeoff between finding a mincut with high probability and finding it quickly.
  In the course of doing so, we will also understand where the somewhat opaque
  factor of sqrt(2) comes from.
  "/>



<meta property="og:title" content="Trading off speed against the probability of success in the Karger-Stein Algorithm" />
<meta property="og:description" content="  The Karger-Stein algorithm is an improvement over Karger&#39;s beautiful contraction
  algorithm for minimum graph cuts. In this post, I show how it finds the perfect
  tradeoff between finding a mincut with high probability and finding it quickly.
  In the course of doing so, we will also understand where the somewhat opaque
  factor of sqrt(2) comes from.
  " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ejnnr.github.io/posts/karger-stein/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-06T18:00:00&#43;01:00" />
<meta property="article:modified_time" content="2020-12-06T18:00:00&#43;01:00" /><meta property="og:site_name" content="Erik Jenner" />







  </head>
  <body class="">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">Erik Jenner</span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title"><a href="https://ejnnr.github.io/posts/karger-stein/">Trading off speed against the probability of success in the Karger-Stein Algorithm</a></h1>
    <div class="post-meta">
      
        <span class="post-date">
          2020-12-06
        </span>

        
          
        
      

      
      
        <span class="post-read-time">— 7 min read</span>
      
    </div>

    

    

    <div class="post-content">
      
      <p><em>(Note: this is an analysis of one aspect of the Karger-Stein algorithm, it&rsquo;s not meant to be a beginner-friendly introduction)</em></p>
<p><a href="https://en.wikipedia.org/wiki/Karger%27s%5Falgorithm">Karger&rsquo;s algorithm</a> randomly contracts a graph and surprisingly, this can be used
to find a minimum cut with probability \(\mathcal{O}(n^{-2})\), where \(n\) is
the number of vertices of the graph.  This is a much, much higher probability
than sampling a graph cut uniformly at random would give! But it still means we
need to run the algorithm \(\mathcal{O}(n^2\log n)\) times to get a high success
probability. Karger&rsquo;s algorithm can be implemented in \(\mathcal{O}(n^2)\)
time<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, which gives an overall runtime of \(\mathcal{O}(n^4 \log n)\) &ndash; not great,
there are much faster deterministic algorithms.</p>
<p>To understand how the Karger-Stein algorithm improves upon that, we need
the following key result that forms the foundation for Karger&rsquo;s algorithm:</p>
<p><strong>Theorem</strong><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>:
When Karger&rsquo;s algorithm contracts a graph from \(n\) to \(r\) vertices,
any given mincut survives with probability \(\geq {r \choose 2}/{n \choose 2} = \frac{r (r - 1)}{n (n - 1)}\).</p>
<p>In particular, for \(r = 2\), we get the \(\mathcal{O}(n^{-2})\) probability mentioned above.</p>
<p>But note the following: if we make only a few contractions, \(r \lesssim n\), then
mincuts are almost guaranteed to survive! This is the key insight that allows us
to improve the runtime of Karger&rsquo;s algorithm, leading to the improved <em>Karger-Stein algorithm</em>.</p>
<p>The idea is the following: first we contract the graph down to roughly \(\frac{n}{b}\) vertices,
where \(b\) is small enough that mincuts are very likely to survive. Then we branch: we again
contract the graph down by a factor of \(b\), but we do so \(a\) times independently
from one another. \(a\) needs to be chosen high enough that mincuts are very likely
to survive <em>in at least one of the branches</em>. We repeat this process until we have contracted
the graph down to just 2 vertices<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.
If we chose \(a\) and \(b\) right, at least one of the final leaves of our computational
tree will likely contain a mincut. So we return the best cut we&rsquo;ve found among all
the leaves.</p>
<p>The Karger-Stein algorithm as it was originally described and as it is usually presented
uses \(a = 2\) and \(b = \sqrt{2}\). So we always split the computation into two branches
and reduce the number of vertices by a factor of \(\sqrt{2}\) before branching again.
But in this post, I would like to motivate where these numbers come from, as well
as show that they&rsquo;re not the only ones that work. So in the following, we&rsquo;re going to
analyze the &ldquo;generalized Karger-Stein algorithm&rdquo; with arbitrary \(a\) and \(b\).</p>
<h2 id="success-probability">Success probability</h2>
<p>As mentioned above, any minimum cut survives a contraction from \(n\)
to \(r\) vertices with probability \(\geq {r \choose 2}/{n \choose 2} = \frac{r (r - 1)}{n (n - 1)}\).
I said we first contract to &ldquo;roughly&rdquo; \(\frac{n}{b}\) vertices &ndash; to be precise we contract
until \(\lceil \frac{n}{b} + 1\rceil\) vertices are left, this will give us a nice bound.
The probability that a mincut survives this contraction is<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>:</p>
<p>\begin{equation}
\begin{split}
p &amp;\geq \frac{\lceil \frac{n}{b} + 1 \rceil \lceil \frac{n}{b} \rceil}{n (n - 1)}\\\<br>
&amp;\geq \frac{(\frac{n}{b} + 1) \cdot \frac{n}{b}}{n (n - 1)}\\\<br>
&amp;\geq \frac{1}{b^2}
\end{split}
\end{equation}</p>
<p>We can now apply this bound recursively: After we have contracted to \(\lceil \frac{n}{b} + 1 \rceil\)
vertices, we can forget that this is a partially contracted graph, and just treat
this number as the &ldquo;new \(n\)&rdquo;.</p>
<p>We will write \(p_k\) for the survival probability if there are \(k\) levels of recursion
left before we reach the leaves of the tree. So \(p_0\) will be the probability in the
leaves of the recursion tree. Depending on when precisely we stop the recursion
and what method we use to finish the contraction, \(p_0\) might take different values,
but all that matters for us is that it is some constant.</p>
<p>Using the bound we found above, we get the following recurrence:
\[p_{k + 1} \geq 1 - \left( 1 - \frac{p_k}{b^2} \right)^a\]
What&rsquo;s going on here? \(\frac{p_k}{b^2}\) is a lower bound on the probability
that any given mincut survives in one particular branch. So \(\left(1 - \frac{p_k}{b^2}\right)^a\)
is an upper bound on the probability that the mincut survives in none of the
\(a\) branches, and consequently \(1 - \left( 1 - \frac{p_k}{b^2} \right)^a\) is a lower
bound on the probability that it survives in at least one.</p>
<p>This recurrence doesn&rsquo;t have an obious solution we can just read off but with
some rewriting, we can get something that&rsquo;s good enough for our purposes.
Substituting \(z_k := \frac{b^2}{p_k} - 1\), we get</p>
<p>\begin{equation}
\begin{split}
z_{k + 1} &amp;= \frac{b^2}{p_{k + 1}} - 1 \\\<br>
&amp;\leq \frac{b^2}{1 - \left(1 - \frac{1}{z_k + 1}\right)^a} - 1\\\<br>
&amp;= \frac{b^2 \left(z_k + 1\right)^a}{\left(z_k + 1\right)^a - z_k^a} - 1\\\<br>
&amp;\leq \frac{b^2 \left(z_k + 1 \right)^a}{a z_k^{a - 1}} - 1\\\<br>
&amp;\leq \frac{b^2}{a} z_k + \text{const}
\end{split}
\end{equation}</p>
<p>where we used \(z_k \geq 1\) in the last step. The constant term may depend
on \(a\) and \(b\) but not on \(z_k\).</p>
<p>If \(a \geq b^2\), then \(z_k \in \mathcal{O}(k)\) which means that \(p_k \in \Omega\left(\frac{1}{k}\right)\).
The depth of recursion for a graph with \(n\) vertices is \(\Theta(\log n)\),
so the overall success probability is \(\Omega\left(\frac{1}{\log n}\right)\).</p>
<p>What this means in words: if we create enough branches (at least \(b^2\)) compared
to how long we contract before branching again, then we get quite a high success
probability &ndash; \(\Omega\left(\frac{1}{\log n}\right)\) means that \(\log^2 n\) runs are enough
to get an overall success probability that approaches 1 as \(n \to \infty\).</p>
<p>But what if \(a &lt; b^2\), i.e. if we don&rsquo;t have enough branches at each stage?
Then the inequality derived above only yields
\(z_k \in \mathcal{O}\left(\left(\frac{b^2}{a}\right)^k\right)\)
so the success probability \(p_k\) can be exponentially low in \(k\).
This means we&rsquo;d have to repeat the algorithm a potentially exponential
number of times, which would make it useless.</p>
<p>That still leaves the question: why does the Karger-Stein algorithm use \(a = b^2\)
in particular, when \(a &gt; b^2\) would give a success probability at least as high?
For that we need to turn to the runtime complexity.</p>
<h2 id="runtime">Runtime</h2>
<p>The runtime of the Karger-Stein algorithm can be described with the following
recurrence:
\[T(n) = aT\left(\frac{n}{b}\right) + \mathcal{O}(n^2)\]
The \(\mathcal{O}(n^2)\) term is for contracting down to roughly \(\frac{n}{b}\)
vertices. At that point, we solve \(a\) smaller version of the original problem,
each of size \(\frac{n}{b}\). That leads to the \(aT\left(\frac{n}{b}\right)\) term.</p>
<p>This kind of recurrence is exactly what the <a href="https://en.wikipedia.org/wiki/Master%5Ftheorem%5F(analysis%5Fof%5Falgorithms)">Master theorem</a> is for. In this case, if we
choose \(a = b^2\), we get a runtime of \(\Theta(n^2 \log n)\) for a single run of
the Karger-Stein algorithm. We already saw that with \(a &lt; b^2\), we get an exponentially
low success probability, so that choice isn&rsquo;t interesting anyway. Finally, if \(a &gt; b^2\),
we get a high success probability, but the runtime becomes \(\Theta(n^c)\),
where \(c := \frac{\log a}{\log b} &gt; 2\).</p>
<p>We can summarize all our results (and a few I didn&rsquo;t mention) in one table:</p>
<table>
<thead>
<tr>
<th>Condition</th>
<th>Time for single run</th>
<th>Success probability</th>
<th>Total runtime</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>\(a &lt; b^2\)</td>
<td>\(\Theta(n^2)\)</td>
<td>exponentially low in \(n\)</td>
<td>exponential in \(n\)</td>
<td>Too little branching</td>
</tr>
<tr>
<td>\(a = b^2\)</td>
<td>\(\Theta(n^2 \log n)\)</td>
<td>\(\Omega\left(\frac{1}{\log n}\right)\)</td>
<td>\(\mathcal{O}(n^2 \log^3 n)\)</td>
<td>Just right</td>
</tr>
<tr>
<td>\(a &gt; b^2\)</td>
<td>\(\Theta(n^c)\) with \(c &gt; 2\)</td>
<td>\(\Omega(1)\)</td>
<td>\(\Theta(n^c)\) with \(c &gt; 2\)</td>
<td>Unnecessarily much branching</td>
</tr>
</tbody>
</table>
<p>The &ldquo;total runtime&rdquo; column contains the runtime that is needed to achieve a high success probability
by repeating the Karger-Stein algorithm often enough (at least if \(n\) is large enough). This is the complexity
that we want to minimize in practice.</p>
<p>We can now see that combining the analysis of the success probability with the runtime analysis
explains why the Karger-Stein algorithm uses \(a = b^2\): in the other cases, we are either very
unlikely to succeed and therefore need too many runs, or we are taking unnecessarily long for
a single run.</p>
<p>But also note that any choice of a &ldquo;branching factor&rdquo; \(a\) works, as long as we then choose
\(b = \sqrt{a}\). So splitting the computation up into just two subproblems is a reasonable
and simple choice, but from a purely asymptotic perspective it is arbitrary.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>There is also an implementation in \(\mathcal{O}(m)\), where \(m\) is the number of edges, but for the Karger-Stein algorithm that won&rsquo;t make a difference and we&rsquo;ll ignore it.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>David Karger: <em>Global min-cuts in RNC, and other ramifications of a simple min-cut algorithm</em>, SODA 1993&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Usually we stop a bit before 2 vertices and just compute the mincut from there using other methods but that doesn&rsquo;t matter here&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>This and the following analysis is based on the paper by Karger and Stein: <em>A new approach to the minimum cut problem</em>, Journal of the ACM 1996. The difference is just that I consider arbitrary \(a\) and \(b\), rather than just \(a = 2\) and \(b = \sqrt{2}\).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
          </div>
          <div class="pagination__buttons">
            
              <span class="button previous">
                <a href="https://ejnnr.github.io/posts/perspectives-on-structure/">
                  <span class="button__icon">←</span>
                  <span class="button__text">Ways to think about structure in mathematics</span>
                </a>
              </span>
            
            
              <span class="button next">
                <a href="https://ejnnr.github.io/posts/discounting-relativistic-universe/">
                  <span class="button__text">Discounting in a relativistic universe</span>
                  <span class="button__icon">→</span>
                </a>
              </span>
            
          </div>
        </div>
      
    


    
      
        

      
    

    </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright copyright--user"><div><a href='/legal'>Legal</a></div><div>Powered by Hugo. Theme: Hello Friend by panr</div></div>
    
  </div>
</footer>

<script src="https://ejnnr.github.io/assets/main.js"></script>
<script src="https://ejnnr.github.io/assets/prism.js"></script><script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>
<script async src="/mathjax/tex-chtml.js"></script>




      
    </div>

    
  </body>
</html>
