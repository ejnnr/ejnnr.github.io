<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Erik Jenner</title>
    
    
    
    <link>https://ejnnr.github.io/posts/</link>
    <description>Recent content in Posts on Erik Jenner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;div&gt;&lt;a href=&#39;https://ejnnr.github.io/legal&#39;&gt;Legal&lt;/a&gt;&lt;/div&gt;&lt;div&gt;Powered by Hugo. Theme: Hello Friend by panr&lt;/div&gt;</copyright>
    <lastBuildDate>Wed, 31 Mar 2021 14:49:00 +0200</lastBuildDate>
    
	<atom:link href="https://ejnnr.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Collection of quick computer tips</title>
      <link>https://ejnnr.github.io/posts/computer-tips/</link>
      <pubDate>Wed, 31 Mar 2021 14:49:00 +0200</pubDate>
      
      <guid>https://ejnnr.github.io/posts/computer-tips/</guid>
      <description>
        
          
          
          
        
        
        
            Many of us spend a lot of time working with our computer, so it&#39;s worth
  spending some time to make that experience as pleasent and productive
  as possible. This is a collection of tips that are relatively quick
  to implement and still very valuable in the long run in my opinion.
  Mainly geared towards developers and others who work with the shell
  a lot.
  
          
        
        </description>
    </item>
    
    <item>
      <title>State formally, reason informally</title>
      <link>https://ejnnr.github.io/posts/state-formally-reason-informally/</link>
      <pubDate>Wed, 24 Mar 2021 11:00:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/state-formally-reason-informally/</guid>
      <description>
        
          
          
          
        
        
        
            There&#39;s a style of teaching mathematics that I really like: stating definitions
  and theorems as formally as in any textbook, but focusing on informal arguments
  for why they should be true.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Emacs as an amazing LaTeX editor</title>
      <link>https://ejnnr.github.io/posts/latex-emacs/</link>
      <pubDate>Wed, 17 Mar 2021 14:14:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/latex-emacs/</guid>
      <description>
        
          
          
          
        
        
        
            Emacs has some really amazing features for writing LaTeX; this post gives
  an overview of some of them, either to convince you to give Emacs a try,
  or to make you aware that these features exist if you&#39;re already using
  Emacs but didn&#39;t know about them.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Perspectives on spherical harmonics</title>
      <link>https://ejnnr.github.io/posts/spherical-harmonics/</link>
      <pubDate>Wed, 10 Mar 2021 17:07:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/spherical-harmonics/</guid>
      <description>
        
          
          
          
        
        
        
            Spherical harmonics are ubiquitous in math and physics, in part because
  they naturally appear as solutions to several problems; in particular they
  are the eigenfunctions of the spherical Laplacian and the irreducible
  representations of SO(3). But why should the solutions to these problems
  be the same? And why are they called spherical harmonics?
  
          
        
        </description>
    </item>
    
    <item>
      <title>Deep Implicit layers</title>
      <link>https://ejnnr.github.io/posts/implicit-layers/</link>
      <pubDate>Wed, 03 Mar 2021 14:48:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/implicit-layers/</guid>
      <description>
        
          
          
          
        
        
        
            Several new architectures for neural networks, such as Neural ODEs and
  deep equlibirum models can be understood as replacing classical layers
  that explicitly specify how to compute the output with implicit layers.
  These layers describe which conditions the output should specify but
  leave the actual computation up to some solver that can be chosen arbitrarily.
  This post contains a brief introduction to the main ideas behind implicit layers.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Building Blocks of RL Part III: Model-based RL</title>
      <link>https://ejnnr.github.io/posts/rl-building-blocks-3/</link>
      <pubDate>Wed, 24 Feb 2021 10:41:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/rl-building-blocks-3/</guid>
      <description>
        
          
          
          
        
        
        
            Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is
  the conclusion of a series with such an overview, covering model-based RL.
  
          
        
        </description>
    </item>
    
    <item>
      <title>L1 regularization: sparsity through singularities</title>
      <link>https://ejnnr.github.io/posts/sparsity-singularities/</link>
      <pubDate>Wed, 17 Feb 2021 09:33:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/sparsity-singularities/</guid>
      <description>
        
          
          
          
        
        
        
            L1 regularization is famous for leading to sparse optima, in contrast to
  L2 regularization. There are several ways of understanding this but I&#39;ll
  argue that it&#39;s really all about one fact: the L1 norm has a singularity
  at the origin, while the L2 norm does not. And this is not just true
  for L1 and L2 regularization: singularities are always necessary to get sparse weights.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Boring numbers, complexity and Chaitin&#39;s incompleteness theorem</title>
      <link>https://ejnnr.github.io/posts/boring-numbers/</link>
      <pubDate>Wed, 10 Feb 2021 16:27:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/boring-numbers/</guid>
      <description>
        
          
          
          
        
        
        
            There is a &#34;complexity barrier&#34;: a number such that we can&#39;t prove
  the Kolmogorov complexity of any specific string to be larger than
  that. The proof of this astonishing fact is closely related to some
  famous paradoxa and we&#39;ll use this connection to get a better intuition
  for why the complexity barrier exists.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Building Blocks of RL Part II: Policy Optimization</title>
      <link>https://ejnnr.github.io/posts/rl-building-blocks-2/</link>
      <pubDate>Wed, 03 Feb 2021 07:39:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/rl-building-blocks-2/</guid>
      <description>
        
          
          
          
        
        
        
            Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is part 2
  of a series with such an overview, covering some policy optimization methods.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Too much structure</title>
      <link>https://ejnnr.github.io/posts/too-much-structure/</link>
      <pubDate>Wed, 27 Jan 2021 08:53:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/too-much-structure/</guid>
      <description>
        
          
          
          
        
        
        
            Proving things for object that have a lot of structure can be harder
  than for object with less structure, simply because the tree of possible
  proofs is much wider. This is probably why trying to prove a more general
  case is sometimes a helpful strategy.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Asymmetry between position and momentum in physics</title>
      <link>https://ejnnr.github.io/posts/position-momentum-asymmetry/</link>
      <pubDate>Tue, 19 Jan 2021 10:52:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/position-momentum-asymmetry/</guid>
      <description>
        
          
          
          
        
        
        
            In both classical mechanics and QM, there are transformations between position-based
  and momentum-based representations that preserve the dynamical laws. So from
  a mathematical perspective, position and momentum seem to play equivalent roles
  in physics. But they don&#39;t play equivalent roles in our cognition, which is part of
  the physical universe -- seemingly a paradox.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Building Blocks of RL Part I: Value-based methods</title>
      <link>https://ejnnr.github.io/posts/rl-building-blocks-1/</link>
      <pubDate>Wed, 13 Jan 2021 16:58:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/rl-building-blocks-1/</guid>
      <description>
        
          
          
          
        
        
        
            Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is part 1
  of a series with such an overview, covering value-based methods (mainly in a tabular
  setting).
  
          
        
        </description>
    </item>
    
    <item>
      <title>VAEs from a generative perspective</title>
      <link>https://ejnnr.github.io/posts/vae-generative/</link>
      <pubDate>Wed, 06 Jan 2021 14:45:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/vae-generative/</guid>
      <description>
        
          
          
          
        
        
        
            Variational autoencoders are usually introduced as a probabilistic extension of autoencoders
  with regularization. An alternative view is that the encoder arises naturally as a tool
  for efficiently training the decoder. This is the perspective I take in this post, deriving
  VAEs without assuming an autoencoder architecture a priori.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Ways to think about structure in mathematics</title>
      <link>https://ejnnr.github.io/posts/perspectives-on-structure/</link>
      <pubDate>Tue, 29 Dec 2020 14:03:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/perspectives-on-structure/</guid>
      <description>
        
          
          
          
        
        
        
            &#34;Structure&#34; is a concept that keeps popping up when thinking about mathematics
  but it&#39;s hard to pin down what it is exactly. I discuss several different perspectives
  for thinking about it.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Trading off speed against the probability of success in the Karger-Stein Algorithm</title>
      <link>https://ejnnr.github.io/posts/karger-stein/</link>
      <pubDate>Sun, 06 Dec 2020 18:00:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/karger-stein/</guid>
      <description>
        
          
          
          
        
        
        
            The Karger-Stein algorithm is an improvement over Karger&#39;s beautiful contraction
  algorithm for minimum graph cuts. In this post, I show how it finds the perfect
  tradeoff between finding a mincut with high probability and finding it quickly.
  In the course of doing so, we will also understand where the somewhat opaque
  factor of sqrt(2) comes from.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Discounting in a relativistic universe</title>
      <link>https://ejnnr.github.io/posts/discounting-relativistic-universe/</link>
      <pubDate>Sat, 20 Jun 2020 12:25:00 +0200</pubDate>
      
      <guid>https://ejnnr.github.io/posts/discounting-relativistic-universe/</guid>
      <description>
        
          
          
          
        
        
        
            For people who want to discount the future, special relativity creates
  some challenges. There are different ways to handle those but none
  seem completely satisfactory which may be yet another argument against
  discounting pure utilities.
  
          
        
        </description>
    </item>
    
  </channel>
</rss>