<!DOCTYPE html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.81.0" />
    
      <title>Erik Jenner</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://ejnnr.github.io/" />





<link rel="stylesheet" href="https://ejnnr.github.io/assets/style.css">


<link rel="stylesheet" href="https://ejnnr.github.io/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://ejnnr.github.io/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://ejnnr.github.io/img/favicon.png">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Erik Jenner"/>
<meta name="twitter:description" content=""/>



<meta property="og:title" content="Erik Jenner" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://ejnnr.github.io/" /><meta property="og:site_name" content="Erik Jenner" />




<link rel="alternate" type="application/rss+xml" href="/index.xml" title="Erik Jenner" />



  </head>
  <body class="">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">Erik Jenner</span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  
  
    
  
  

  <div class="posts">
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/state-formally-reason-informally/">State formally, reason informally</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-03-24
          </span>

          
        
        
        
          <span class="post-read-time">— 4 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          There&rsquo;s a style of teaching mathematics that I really like: stating definitions
and theorems as formally as in any textbook, but focusing on informal arguments
for why they should be true.
        
      </div>
      
        <div><a class="read-more button" href="/posts/state-formally-reason-informally/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/latex-emacs/">Emacs as an amazing LaTeX editor</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-03-17
          </span>

          
        
        
        
          <span class="post-read-time">— 6 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          Emacs has some really amazing features for writing LaTeX; this post gives
an overview of some of them, either to convince you to give Emacs a try,
or to make you aware that these features exist if you&rsquo;re already using
Emacs but didn&rsquo;t know about them.
        
      </div>
      
        <div><a class="read-more button" href="/posts/latex-emacs/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/spherical-harmonics/">Perspectives on spherical harmonics</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-03-10
          </span>

          
        
        
        
          <span class="post-read-time">— 5 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          Spherical harmonics are ubiquitous in math and physics, in part because
they naturally appear as solutions to several problems; in particular they
are the eigenfunctions of the spherical Laplacian and the irreducible
representations of SO(3). But why should the solutions to these problems
be the same? And why are they called spherical harmonics?
        
      </div>
      
        <div><a class="read-more button" href="/posts/spherical-harmonics/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/implicit-layers/">Deep Implicit layers</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-03-03
          </span>

          
        
        
        
          <span class="post-read-time">— 6 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          Several new architectures for neural networks, such as Neural ODEs and
deep equlibirum models can be understood as replacing classical layers
that explicitly specify how to compute the output with implicit layers.
These layers describe which conditions the output should specify but
leave the actual computation up to some solver that can be chosen arbitrarily.
This post contains a brief introduction to the main ideas behind implicit layers.
        
      </div>
      
        <div><a class="read-more button" href="/posts/implicit-layers/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/rl-building-blocks-3/">Building Blocks of RL Part III: Model-based RL</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-02-24
          </span>

          
        
        
        
          <span class="post-read-time">— 9 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          Reinforcement Learning consists of a few key building blocks that can be combined to create
many of the well-known algorithms. Framing RL in terms of these building blocks
can give a good overview and better understanding of these algorithms. This is
the conclusion of a series with such an overview, covering model-based RL.
        
      </div>
      
        <div><a class="read-more button" href="/posts/rl-building-blocks-3/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/sparsity-singularities/">L1 regularization: sparsity through singularities</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-02-17
          </span>

          
        
        
        
          <span class="post-read-time">— 7 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          L1 regularization is famous for leading to sparse optima, in contrast to
L2 regularization. There are several ways of understanding this but I&rsquo;ll
argue that it&rsquo;s really all about one fact: the L1 norm has a singularity
at the origin, while the L2 norm does not. And this is not just true
for L1 and L2 regularization: singularities are always necessary to get sparse weights.
        
      </div>
      
        <div><a class="read-more button" href="/posts/sparsity-singularities/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/boring-numbers/">Boring numbers, complexity and Chaitin&rsquo;s incompleteness theorem</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-02-10
          </span>

          
        
        
        
          <span class="post-read-time">— 7 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          There is a &ldquo;complexity barrier&rdquo;: a number such that we can&rsquo;t prove
the Kolmogorov complexity of any specific string to be larger than
that. The proof of this astonishing fact is closely related to some
famous paradoxa and we&rsquo;ll use this connection to get a better intuition
for why the complexity barrier exists.
        
      </div>
      
        <div><a class="read-more button" href="/posts/boring-numbers/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/rl-building-blocks-2/">Building Blocks of RL Part II: Policy Optimization</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-02-03
          </span>

          
        
        
        
          <span class="post-read-time">— 11 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          Reinforcement Learning consists of a few key building blocks that can be combined to create
many of the well-known algorithms. Framing RL in terms of these building blocks
can give a good overview and better understanding of these algorithms. This is part 2
of a series with such an overview, covering some policy optimization methods.
        
      </div>
      
        <div><a class="read-more button" href="/posts/rl-building-blocks-2/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/too-much-structure/">Too much structure</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-01-27
          </span>

          
        
        
        
          <span class="post-read-time">— 6 min read</span>
        
      </div>

      
        <span class="post-tags">
          
            #<a href="https://ejnnr.github.io/tags/structure/">structure</a>&nbsp;
          
        </span>
      

      

      <div class="post-content">
        
          Proving things for object that have a lot of structure can be harder
than for object with less structure, simply because the tree of possible
proofs is much wider. This is probably why trying to prove a more general
case is sometimes a helpful strategy.
        
      </div>
      
        <div><a class="read-more button" href="/posts/too-much-structure/">Read more →</a></div>
      
    </div>
    
    <div class="post on-list">
      <h1 class="post-title"><a href="https://ejnnr.github.io/posts/position-momentum-asymmetry/">Asymmetry between position and momentum in physics</a></h1>
      <div class="post-meta">
        
          <span class="post-date">
            2021-01-19
          </span>

          
        
        
        
          <span class="post-read-time">— 7 min read</span>
        
      </div>

      

      

      <div class="post-content">
        
          In both classical mechanics and QM, there are transformations between position-based
and momentum-based representations that preserve the dynamical laws. So from
a mathematical perspective, position and momentum seem to play equivalent roles
in physics. But they don&rsquo;t play equivalent roles in our cognition, which is part of
the physical universe &ndash; seemingly a paradox.
        
      </div>
      
        <div><a class="read-more button" href="/posts/position-momentum-asymmetry/">Read more →</a></div>
      
    </div>
    
    <div class="pagination">
  <div class="pagination__buttons">
    
    
      <span class="button next">
        <a href="/page/2/">
          <span class="button__text">Older posts</span>
          <span class="button__icon">→</span>
        </a>
      </span>
    
  </div>
</div>

  </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright copyright--user"><div><a href='/legal'>Legal</a></div><div>Powered by Hugo. Theme: Hello Friend by panr</div></div>
    
  </div>
</footer>

<script src="https://ejnnr.github.io/assets/main.js"></script>
<script src="https://ejnnr.github.io/assets/prism.js"></script>


      
    </div>

    
  </body>
</html>
