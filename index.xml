<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Erik Jenner</title>
    
    
    
    <link>https://ejnnr.github.io/</link>
    <description>Recent content on Erik Jenner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;div&gt;&lt;a href=&#39;https://ejnnr.github.io/legal&#39;&gt;Legal&lt;/a&gt;&lt;/div&gt;&lt;div&gt;Powered by Hugo. Theme: Hello Friend by panr&lt;/div&gt;</copyright>
    <lastBuildDate>Wed, 06 Jan 2021 14:45:00 +0100</lastBuildDate>
    
	<atom:link href="https://ejnnr.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>VAEs from a generative perspective</title>
      <link>https://ejnnr.github.io/posts/vae-generative/</link>
      <pubDate>Wed, 06 Jan 2021 14:45:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/vae-generative/</guid>
      <description>
        
          
          
          
        
        
        
            Variational autoencoders are usually introduced as a probabilistic extension of autoencoders
  with regularization. An alternative view is that the encoder arises naturally as a tool
  for efficiently training the decoder. This is the perspective I take in this post, deriving
  VAEs without assuming an autoencoder architecture a priori.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Ways to think about structure in mathematics</title>
      <link>https://ejnnr.github.io/posts/perspectives-on-structure/</link>
      <pubDate>Tue, 29 Dec 2020 14:03:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/perspectives-on-structure/</guid>
      <description>
        
          
          
          
        
        
        
            &#34;Structure&#34; is a concept that keeps popping up when thinking about mathematics
  but it&#39;s hard to pin down what it is exactly. I discuss several different perspectives
  for thinking about it.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Trading off speed against the probability of success in the Karger-Stein Algorithm</title>
      <link>https://ejnnr.github.io/posts/karger-stein/</link>
      <pubDate>Sun, 06 Dec 2020 18:00:00 +0100</pubDate>
      
      <guid>https://ejnnr.github.io/posts/karger-stein/</guid>
      <description>
        
          
          
          
        
        
        
            The Karger-Stein algorithm is an improvement over Karger&#39;s beautiful contraction
  algorithm for minimum graph cuts. In this post, I show how it finds the perfect
  tradeoff between finding a mincut with high probability and finding it quickly.
  In the course of doing so, we will also understand where the somewhat opaque
  factor of sqrt(2) comes from.
  
          
        
        </description>
    </item>
    
    <item>
      <title>Discounting in a relativistic universe</title>
      <link>https://ejnnr.github.io/posts/discounting-relativistic-universe/</link>
      <pubDate>Sat, 20 Jun 2020 12:25:00 +0200</pubDate>
      
      <guid>https://ejnnr.github.io/posts/discounting-relativistic-universe/</guid>
      <description>
        
          
          
          
        
        
        
            For people who want to discount the future, special relativity creates
  some challenges. There are different ways to handle those but none
  seem completely satisfactory which may be yet another argument against
  discounting pure utilities.
  
          
        
        </description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://ejnnr.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ejnnr.github.io/about/</guid>
      <description>
        
          
          
          
        
        
        
          I&amp;rsquo;m trying to have a positive impact on the world by making sure that artificial intelligence will be aligned with humanity&amp;rsquo;s interests.
Currently, I&amp;rsquo;m doing a Master&amp;rsquo;s in Artificial Intelligence at the University of Amsterdam.
Among other things, I&amp;rsquo;m interested in causality, foundational questions in physics, information theory, paradoxa, Emacs, chess and personal productivity. Only time will tell which of those things I will blog about here.
          
        
        </description>
    </item>
    
    <item>
      <title>Impressum</title>
      <link>https://ejnnr.github.io/legal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ejnnr.github.io/legal/</guid>
      <description>
        
          
          
          
        
        
        
          Contact me at erik.jenner99@gmail.com
Privacy I don&amp;rsquo;t use cookies, track your visits or collect any personal information about you. The website does remember whether you use the dark or light theme but this information stays on your computer.
Because this blog is hosted with Github Pages, Github may log visits, including IP adresses as per their privacy policy.
          
        
        </description>
    </item>
    
  </channel>
</rss>