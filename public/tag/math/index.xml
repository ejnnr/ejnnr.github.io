<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math | Erik Jenner</title>
    <link>https://ejenner.com/tag/math/</link>
      <atom:link href="https://ejenner.com/tag/math/index.xml" rel="self" type="application/rss+xml" />
    <description>Math</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Jul 2021 14:15:00 +0200</lastBuildDate>
    <image>
      <url>https://ejenner.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Math</title>
      <link>https://ejenner.com/tag/math/</link>
    </image>
    
    <item>
      <title>Distributions Part I: the Delta distribution</title>
      <link>https://ejenner.com/post/distributions-intro/</link>
      <pubDate>Tue, 06 Jul 2021 14:15:00 +0200</pubDate>
      <guid>https://ejenner.com/post/distributions-intro/</guid>
      <description>&lt;p&gt;Schwartz distributions are a generalization of functions from \(\mathbb{R}^n\) to \(\mathbb{R}\):
strictly speaking, they aren&amp;rsquo;t such functions themselves, but you can do a lot of the same
stuff with them that you can do with normal functions, such as taking derivatives, computing convolutions,
and even Fourier transforms (at least in certain cases).
And in some ways, they even make life easier compared to functions. For example, every distribution
is infinitely differentiable! But of course, we do have to give up some things: distributions
can&amp;rsquo;t be evaluated at a single point and it&amp;rsquo;s in general impossible to multiply two distributions.&lt;/p&gt;
&lt;p&gt;In this series, we&amp;rsquo;ll try to understand all of these properties of distributions and more.
I will focus on intuition but still &lt;a href=&#34;https://ejenner.com/post/state-formally-reason-informally&#34;&gt;give formal definitions&lt;/a&gt; of all the concepts we look at.
As a secondary purpose, studying distributions will also be an excellent opportunity to
practice finding good definitions. We will introduce many different operations on distributions
and in each case, we will try to understand how one could come up with the definition
in a natural way.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In electrostatics, &lt;em&gt;charge densities&lt;/em&gt; are used to model the amount
of electric charge in different places. Such a charge density is a function
\(\rho: \mathbb{R}^3 \to \mathbb{R}\) that assigns an amount of charge per volume
to every point \(x \in \mathbb{R}^3\). From an experimental standpoint,
these densities are only useful abstractions; what we can measure is at best
the &lt;em&gt;total charge in some volume&lt;/em&gt;. This charge \(Q\) is given by the integral
of the density over the volume:
\[Q(V) = \int_V \rho(x) dx\]
for any subset \(V \subseteq \mathbb{R}^3\). You can even think of this as
the &lt;em&gt;definition&lt;/em&gt; of the density \(\rho\): the only thing we care about is that
when we measure the charge \(Q(V)\) in any volume \(V\), we get \(\int_V \rho(x) dx\).&lt;/p&gt;
&lt;p&gt;Now assume we observe the following: \(Q(V) = 1\) for any volume \(V\) that contains
the origin but \(Q(V) = 0\) if \(V\) does not contain the origin. Intuitively, we
conclude that there is a point charge with value 1 in the origin and no charge anywhere else.
But how can we model this using a density \(\rho\)? If \(\rho\) is any (integrable)
function, as we originally assumed, then we must have \(\rho(x) = 0\) for \(x \neq 0\).&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
But in that case, \(\int_V \rho(x) dx = 0\) for all volumes \(V\), which contradicts
our first observation.&lt;/p&gt;
&lt;p&gt;For now, let&amp;rsquo;s just &amp;ldquo;define this problem away&amp;rdquo;: we&amp;rsquo;ll say that \(\rho(x) = \delta(x)\),
where \(\delta(x)\) is an object such that
\[\int_V \delta(x) dx := 1 \text{ if } 0 \in V, \text{ otherwise } 0.\]
The word &amp;ldquo;object&amp;rdquo; here is code for &amp;ldquo;we&amp;rsquo;re pretty confused and don&amp;rsquo;t know what this thing
is but we&amp;rsquo;d like to have something that behaves this way&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll develop a formal definition of \(\delta\) soon. But first, let&amp;rsquo;s extend
the original example a bit: suppose instead of being interested only in the charge inside
some volume, we now introduce a charged test particle and want to know the potential
energy it has due to the charge density \(\rho\). This potential is given by
\[\Phi \propto \int_{\mathbb{R}^3} \frac{1}{|x_0 - x|} \rho(x) dx\]
for a test particle at position \(x_0\). So what is the potential energy if we have the point charge from
before, \(\rho(x) = \delta(x)\)? So far, we have only defined \(\int_V \delta(x) dx\),
and if \(\delta(x)\) appears anywhere else, we don&amp;rsquo;t really know what to
do with it. Remember, \(\int_V \delta(x) dx\) is just a notation we introduced to mean
&amp;ldquo;1 if \(0 \in V\) and 0 otherwise&amp;rdquo;, it&amp;rsquo;s not actually an integral in any usual sense.&lt;/p&gt;
&lt;p&gt;So we will apply a powerful technique &amp;ndash; wishful thinking. We just assume that
\(\delta(x)\) behaves the way we would intuitively like it to, and then worry later
about constructing something that actually does behave that way. Since for \(\rho(x) = \delta(x)\),
there is no charge outside the origin, all parts of the integral above except for \(x = 0\)
ought to vanish. So let&amp;rsquo;s just write
\[\int_{\mathbb{R}^3} \frac{1}{|x_0 - x|}\delta(x) dx = \int_{\{0\}}\frac{1}{|x_0 - x|}\delta(x)dx.\]
Since we&amp;rsquo;re only integrating over \(\{0\}\) now, we can set \(x = 0\) in \(|x_0 - x|\). Then
this part doesn&amp;rsquo;t depend on \(x\) anymore and we get
\[\int_{\{0\}}\frac{1}{|x_0 - x|}\delta(x)dx = \frac{1}{|x_0|}\int_{\{0\}}\delta(x)dx.\]
But we know what to do with that last part, its&#39; 1! So the potential should be \(\Phi \propto \frac{1}{|x_0|}\).&lt;/p&gt;
&lt;p&gt;We can apply the same argument more generally to \(\int \varphi(x) \delta(x)dx\) for other functions \(\varphi\).
So let&amp;rsquo;s &amp;ldquo;wish&amp;rdquo; that
\[\int_{\mathbb{R}^3} \varphi(x) \delta(x) dx := \varphi(0)\]
hold for all functions \(\varphi\). This contains our original definition of \(\delta(x)\) as a special
case, namely for the indicator function \(\varphi = 1_V\).&lt;/p&gt;
&lt;h2 id=&#34;schwartz-distributions&#34;&gt;Schwartz distributions&lt;/h2&gt;
&lt;p&gt;The defining property of \(\delta(x)\) that we would like to have is
\[\int_{\mathbb{R}^3} \delta(x) \varphi(x) dx := \varphi(0)\]
for arbitrary functions \(\varphi\). We have already noted that this cannot
be an actual (Lebesgue) integral, so it makes sense to get rid of that
notation. Instead, we will write
\[\langle \delta, \varphi\rangle := \varphi(0).\]
This hightlights the important part: \(\delta\) lets us take
any function \(\varphi\) and maps it to its value \(\varphi(0)\) at the origin. So \(\delta\)
is a function after all; just not from \(\mathbb{R}^3\) to \(\mathbb{R}\) but from
the space of &lt;em&gt;functions on&lt;/em&gt; \(\mathbb{R}^3\) to \(\mathbb{R}\)!&lt;/p&gt;
&lt;p&gt;\(\delta\) is one example of &lt;em&gt;Schwartz distributions&lt;/em&gt; or &lt;em&gt;distributions&lt;/em&gt; for short,
which are all maps from a space of functions to the real numbers. Let&amp;rsquo;s make this
more precise:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition:&lt;/strong&gt; Let \(U \subseteq \mathbb{R}^n\) be an open subset. A &lt;em&gt;test function&lt;/em&gt; on \(U\)
is a smooth, compactly supported function \(\varphi: U \to \mathbb{R}\) and we write
\(\mathcal{D}(U)\) for the space of all such test functions. A &lt;em&gt;Schwartz distribution&lt;/em&gt;
on \(U\) is then a &lt;em&gt;continuous linear function&lt;/em&gt; \(T: \mathcal{D}(U) \to \mathbb{R}\).
We write \(\mathcal{D}&#39;(U)\) for the space of all such distributions on \(U\).&lt;/p&gt;
&lt;p&gt;This definition requires some clarifications. First, Schwartz distributions are not
at all the same thing as probability distributions, and when I say &amp;ldquo;distribution&amp;rdquo;
in this series, I will always mean a Schwartz distribution. Second, if we want to
talk about continuity, we of course need to define a topology on the space \(\mathcal{D}(U)\)
of test functions. The topology we use here is called the &lt;em&gt;canonical LF topology&lt;/em&gt; but
we won&amp;rsquo;t discuss that any further in this post.&lt;/p&gt;
&lt;p&gt;The name &lt;em&gt;test function&lt;/em&gt; comes from the fact that these are the functions on which
we can &amp;ldquo;test&amp;rdquo;, i.e. evaluate distributions. In our first example about the total
charge in some volume, we used indicator functions \(1_V\) as test functions.
The \(\delta\) distribution would in principle work on &lt;em&gt;any&lt;/em&gt; space of test functions.
But it turns out that a good choice for the general definition are smooth
compactly supported functions because this makes a lot of the theory very nice.&lt;/p&gt;
&lt;p&gt;We will write \(\langle T, \varphi \rangle\) for the distribution \(T\) applied to
the test function \(\varphi\). But why did we write \(\int \delta(x) \varphi(x) dx\) before?
What does all of this have to do with integrals? The reason is the following:
let \(f : U \to \mathbb{R}\) be any locally integrable (read &amp;ldquo;somewhat reasonable&amp;rdquo;) function. Then the map
\[\varphi \mapsto \int_U f(x) \varphi(x) dx\]
defines a distribution on \(U\), which we denote by \(T_f\). This is the sense in which
distributions are &lt;em&gt;generalized&lt;/em&gt; functions; each classical function induces a distribution.
So when we write \(\int \delta(x) \varphi(x) dx\), we are essentially pretending that
the delta distribution is induced by a function \(\delta(x)\). There is no such function,
but the notation is used very often anyway; probably in part for historical reasons
and in part because it turns out to work surprisingly well, as we&amp;rsquo;ll see next.&lt;/p&gt;
&lt;p&gt;We will revisit distributions in general in the next post but for now, we focus on the
\(\delta\) distribution again.&lt;/p&gt;
&lt;h2 id=&#34;variations-of-the--delta--distribution&#34;&gt;Variations of the \(\delta\) distribution&lt;/h2&gt;
&lt;p&gt;We now have a formal understanding of terms of the form \(\int \delta(x) \varphi(x)dx\).
But in practice, the \(\delta\) distribution often appears in modified versions,
such as in terms like
\[\int \delta(x - x_0)\varphi(x) dx\]
or
\[\int \delta(ax)\varphi(x)dx.\]
So far, we haven&amp;rsquo;t formally defined these terms. That means it&amp;rsquo;s time to apply
the Power of Wishful Thinking again, in order to find good definitions for them.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s pretty clear what \(\delta(x - x_0)\) should mean: it&amp;rsquo;s just a shifted version
of \(\delta(x)\), with its &amp;ldquo;peak&amp;rdquo; at \(x_0\) instead of \(0\). More explicitly, it
makes sense to demand that
\[\int \delta(x - x_0)\varphi(x) dx = \int \delta(x) \varphi(x + x_0) dx\]
as would be the case if \(\delta\) was a regular function (all integrations are assumed
to be over all of \(\mathbb{R}^n\)). Then we can see that
\[\int \delta(x - x_0)\varphi(x) dx = \varphi(x_0).\]&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider \(\int \delta(ax)\varphi(x)dx\) instead. You might argue as follows:
&amp;ldquo;\(\delta(ax) = 0\) for \(x \neq 0\), so we only need to consider \(x = 0\). In that case,
\(ax = 0 = x\), so \(\delta(ax)\) should be the same as \(\delta(x)\)&amp;rdquo;.
But this is a misunderstanding caused by the (admittedly very confusing) notation
often used for the \(\delta\) distribution: \(\delta(x)\) doesn&amp;rsquo;t mean that anything
is actually being evaluated at \(x\), it&amp;rsquo;s just a notational convention &lt;em&gt;that only makes sense inside integrals&lt;/em&gt;.
We don&amp;rsquo;t want to demand that \(\delta(\cdot)\) behaves like functions when we plug in different things because
we never have \(\delta(x)\) appearing on its own anyway.&lt;/p&gt;
&lt;p&gt;What we do want is that \(\delta(x)\) behaves like functions &lt;em&gt;inside an integral&lt;/em&gt;.
In particular, for functions \(f\) and \(\varphi\) and a scalar \(a \neq 0\), we have
\[\int f(ax)\varphi(x)dx = \frac{1}{|a|^n}\int f(x)\varphi\left(\frac{x}{a}\right)dx.\]
So since we want \(\delta(x)\) to behave the way that functions behave inside integrals,
we define
\[\int \delta(ax)\varphi(x)dx := \frac{1}{|a|^n}\int\delta(x)\varphi\left(\frac{x}{a}\right)dx = \frac{1}{|a|^n}\varphi(0).\]&lt;/p&gt;
&lt;p&gt;In fact, we can generalize this argument: for any diffeomorphism \(g\) of \(\mathbb{R}^n\), we have
\[\int f(g(x))\varphi(x)dx = \int |\operatorname{det} Dg(x)|^{-1} f(x)\varphi(g^{-1}(x))dx\]
where \(Dg\) is the derivative (Jacobian) of \(g\).
So in analogy, we can define \(\delta(g(x))\) for any diffeomorphism \(g\) by
\[\int \delta(g(x))\varphi(x)dx := \int |\operatorname{det} Dg(x)|^{-1} \delta(x)\varphi(g^{-1}(x))dx
= |\operatorname{det} Dg(0)|^{-1}\varphi(g^{-1}(0)).\]&lt;/p&gt;
&lt;p&gt;I want to stress again that none of these arguments are &amp;ldquo;proofs&amp;rdquo; or &amp;ldquo;derivations&amp;rdquo; &amp;ndash; in the end, we have
to choose how to define all of these terms. But clearly some definitions make more sense than others
and in the examples here there is clearly one &amp;ldquo;right&amp;rdquo; way to define what \(\delta(g(x))\) etc. should
mean. This will become even more clear in the next post: we will continue the theme of finding
good definitions via &amp;ldquo;wishful thinking&amp;rdquo;, only this time for arbitrary distributions and for many
more types of operations.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Actually, only for almost all \(x\) but that doesn&amp;rsquo;t change anything.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>State formally, reason informally</title>
      <link>https://ejenner.com/post/state-formally-reason-informally/</link>
      <pubDate>Wed, 24 Mar 2021 11:00:00 +0100</pubDate>
      <guid>https://ejenner.com/post/state-formally-reason-informally/</guid>
      <description>&lt;p&gt;Evan Chen&amp;rsquo;s &lt;a href=&#34;https://web.evanchen.cc/napkin.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Infinitely Large Napkin&lt;/a&gt; is my go-to resource when I want to learn
about some new area of mathematics (or at least it used to be; I&amp;rsquo;m increasingly
often running into the issue that it doesn&amp;rsquo;t have a chapter about what I want
to learn &amp;ndash; at barely over 900 pages it&amp;rsquo;s just way too short). I recently asked
myself what it is about the Napkin that I like so much, and this post is part
of my answer.&lt;/p&gt;
&lt;p&gt;Note that I&amp;rsquo;m describing the way &lt;em&gt;I&lt;/em&gt; like to learn math, not the objectively
best way. If you happen to like learning that way too, maybe this post can give
you a more explicit idea of what &amp;ldquo;that way&amp;rdquo; is. Otherwise, it will at least
give you some insight into the brain of someone who learns differently
than you do, and maybe that will help if you want to teach math to other
people.&lt;/p&gt;
&lt;p&gt;One way of teaching mathematical concepts is the approach typically employed
by textbooks: give some definitions, probably a few examples, state a few theorems,
prove them. In some cases, you might also get an intuitive explanation of what
these definitions are all about or why a theorem is interesting or how the proof
roughly works. But often, these explanations are sparse and confined to the beginning
of a section, so as not to dilute the mathematical purity of the remaining text.&lt;/p&gt;
&lt;p&gt;Another style of explanation is the one that consists almost entirely of hand-waving. Evan Chen
describes it as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Someone tells you about the hairy ball theorem in the form “you can’t comb the
hair on a spherical cat”, then doesn’t tell you anything about why it should be true,
what it means to actually “comb the hair”, or any of the underlying theory, leaving
you with just some vague notion in your head.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could think of these two approaches as opposite ends of a &amp;ldquo;rigor&amp;rdquo;-spectrum.
Somewhere in the middle, you might have the way that theoretical physics is often taught:
few things are formally defined, but concepts are at least made
explicit enough that you are able to use them in calculations, and equations
are usually derived (though less rigorously than in math).&lt;/p&gt;
&lt;p&gt;Where does the Napkin fit in on this axis? Is it more or less rigorous than
theoretical physics courses? I think that&amp;rsquo;s the wrong question to ask because
the one-dimensional model of approaches to teaching math is too simplistic.
The space of all the ways you could explain mathematical concepts is very large
and treating it as one-dimensional isn&amp;rsquo;t such a great approximation (though I think
that the &amp;ldquo;rigor&amp;rdquo;-axis might be the best one can do with only one dimension).&lt;/p&gt;
&lt;p&gt;So what&amp;rsquo;s a better model? I want to suggest thinking about mathematical explanations
using a two-dimensional approximation. On one axis, we have the rigor used when &lt;em&gt;stating&lt;/em&gt;
definitions or results, while the other axis is the rigor of the &lt;em&gt;derivations&lt;/em&gt; of these results.
Textbooks have both formal statements and formal derivations, casual hand-wavy explanations
have neither. Physics is somewhere in between, though to me the derivations often
seem more rigorous than the statements&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
Finally, the Napkin takes the opposite approach:
it states things with essentially the same rigor as textbooks but then places emphasis
on very informal derivations or explanations of why these statements are true.&lt;/p&gt;














&lt;figure  id=&#34;figure-figure-1-various-styles-for-teaching-mathematics-yes-it-would-look-better-as-a-tikz-figure-but-it-was-either-this-or-nothing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 1: Various styles for teaching mathematics. Yes, it would look better as a TikZ figure, but it was either this or nothing.&#34;
           src=&#34;https://ejenner.com/post/state-formally-reason-informally/rigor.svg&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: Various styles for teaching mathematics. Yes, it would look better as a TikZ figure, but it was either this or nothing.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;I find that this approach &amp;ndash; stating things formally but reasoning about them informally &amp;ndash;
works extremely well for me when I first learn about a subject. I&amp;rsquo;m happy to take someone&amp;rsquo;s
word that a statement is true, at least initially and if the statement seems like it &lt;em&gt;should&lt;/em&gt;
be true. But if I don&amp;rsquo;t know precisely what the statement is, I find it much harder to get
to grips with the subject.&lt;/p&gt;
&lt;p&gt;As a caveat, note that I said &amp;ldquo;when I first learn about a subject&amp;rdquo;. This approach doesn&amp;rsquo;t teach
how to write formal proofs, and it&amp;rsquo;s by design not as comprehensive as a textbook.
But sometimes an intuitive understanding is enough for my purposes, and even if I know I&amp;rsquo;ll
later want to learn a subject in more depth, I find it helpful to build this intuitive
understanding &lt;em&gt;before&lt;/em&gt; going through all the details.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This sounded a bit absurd to me when writing it down &amp;ndash; how can you rigorously derive something you haven&amp;rsquo;t even really stated? But I think there&amp;rsquo;s some truth to it, and what I mean is roughly this: the calculations done in physics courses are often essentially the same that you&amp;rsquo;d do for a formal proof. But the objects used in those calculations aren&amp;rsquo;t formally defined, it&amp;rsquo;s just taken for granted that everyone has a sense of what they are and how they behave (or maybe it&amp;rsquo;s explicitly stated how they behave, but not whether they are uniquely characterized by that behavior).&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Perspectives on spherical harmonics</title>
      <link>https://ejenner.com/post/spherical-harmonics/</link>
      <pubDate>Wed, 10 Mar 2021 17:07:00 +0100</pubDate>
      <guid>https://ejenner.com/post/spherical-harmonics/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Spherical%5Fharmonics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spherical harmonics&lt;/a&gt; appear in lots of different places and have different
interpretations that at first sight don&amp;rsquo;t seem to have anything to do
with one another. In this post, I&amp;rsquo;ll try to connect three very common ones
(namely as harmonic polynomials, as eigenfunctions of the Laplacian
and as irreps of \(\operatorname{SO}(3)\)).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re going to define spherical harmonics as homogeneous harmonic polynomials
\(\mathbb{R}^3 \to \mathbb{C}\). Let&amp;rsquo;s break this down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A polynomial of three variables is a finite sum of the form
\[\sum_{\alpha} a_\alpha x^{\alpha_x}y^{\alpha_y}z^{\alpha_z}\]
over multi-indices \(\alpha \in \mathbb{N}_0^3\). Some examples are
\(x^2y + 2z\) or \(xyz + y^2\).&lt;/li&gt;
&lt;li&gt;The coefficients \(a_\alpha\) can be complex numbers, but we will only plug
in real numbers for \(x\), \(y\) and \(z\). That&amp;rsquo;s why we interpret polynomials
as functions \(\mathbb{R}^3 \to \mathbb{C}\).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Homogeneous&lt;/em&gt; mean that \(\alpha_x + \alpha_y + \alpha_z\) is the same for
all the \(\alpha\) we sum over, so all the terms in the sum have the same
degree. For example, \(x^2 + 2yz + xz\) is homogeneous, while \(xy + z\) is not.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Harmonic&lt;/em&gt; means that the Laplacian of the polynomial vanishes: \(p\) is harmonic if
\(\Delta p = 0\). Here, \(\Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} +
\frac{\partial^2}{\partial z^2}\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will write \(\mathcal{H}_l\) for the space of all homogeneous harmonic polymonials
of degree \(l\) (meaning \(\alpha_x + \alpha_y + \alpha_z = l\) for all summands).&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;ve seen spherical harmonics before (and you presumably have, if you&amp;rsquo;re
reading this post), it&amp;rsquo;s probably been in the form of functions \(Y_l^m(\theta,
\varphi)\) defined on the sphere. So why are we talking about these polynomials
on \(\mathbb{R}^3\) instead?&lt;/p&gt;
&lt;p&gt;The answer is that every polynomial \(p \in \mathcal{H}_l\) can be written
in spherical coordinates as
\[p(x, y, z) = r^l Y(\theta, \varphi)\]
for some function \(Y: S^2 \to \mathbb{C}\). To see why, write \(x\), \(y\) and \(z\)
in spherical coordinates and plug them into the polynomial. They each have
a factor of \(r\) and then some factors depending on \(\theta\) and \(\varphi\).
So because \(p\) is homogeneous, each summand consists of a factor \(r^l\) times
something that depends only on \(\theta\) and \(\varphi\). So we can think of
homogeneous polynomials as polynomials defined on the sphere &amp;ndash; their continuation
to \(\mathbb{R}^3\) is automatically determined by their degree \(l\). Therefore,
we won&amp;rsquo;t really distinguish between homogeneous harmonic polynomials defined
on \(\mathbb{R}^3\) and their restrictions to \(S^2\), we will refer to both as
spherical harmonics.&lt;/p&gt;
&lt;p&gt;This should also explain the name: spherical harmonics are &lt;em&gt;harmonic&lt;/em&gt; polynomials
living on the &lt;em&gt;sphere&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The functions \(Y_l^m\) that you may have seen are just a particular choice of
basis for the vector space of spherical harmonics. If you multiply them by
\(r^l\), you get polynomials in \(\mathcal{H}_l\), and
\[\{r^l Y_l^m| -l \leq m \leq l\}\]
is a basis for \(\mathcal{H}_l\).&lt;/p&gt;
&lt;h2 id=&#34;eigenfunctions-of-the-laplacian&#34;&gt;Eigenfunctions of the Laplacian&lt;/h2&gt;
&lt;p&gt;One of the reasons that spherical harmonics are so ubiquitous is that they
are the eigenfunctions of the spherical Laplacian \(\Delta_{S^2}\). They key
to that is the following fact (which is just a brief calculation):
for a function \(Y: S^2 \to \mathbb{C}\),
\[\Delta (r^l Y) = r^{l - 2}\left(l(l + 1)Y + \Delta_{S^2}Y\right)\thinspace.\]
So \(r^l Y(\theta, \varphi)\) is harmonic if and only if
\[\Delta_{S^2}Y = -l(l + 1)Y\thinspace.\]
This already proves that spherical harmonics are eigenfunctions of the spherical
Laplacian.&lt;/p&gt;
&lt;p&gt;But we can say more than that: if we take any eigenfunction \(f: S^2 \to
\mathbb{C}\) of the spherical Laplacian and multiply by \(r^l\) (with \(l\) such
that \(-l(l + 1)\) gives the eigenvalue&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;), then \(r^l f(\theta, \varphi)\) must be
harmonic. So the eigenfunctions of the spherical Laplacian are in fact in 1-to-1
correspondence with harmonic homogeneous functions on \(\mathbb{R}^3\). It then turns
out &amp;ndash; and this part is far from obvious &amp;ndash; that all such functions are
polynomials&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;! So the
spherical harmonics aren&amp;rsquo;t just eigenfunctions of the spherical Laplacian, they
make up &lt;em&gt;all&lt;/em&gt; of its eigenfunctions.&lt;/p&gt;
&lt;h2 id=&#34;irreducible-representations-of--operatorname-so-3&#34;&gt;Irreducible representations of \(\operatorname{SO}(3)\)&lt;/h2&gt;
&lt;p&gt;Another famous role that spherical harmonics play is as the irreducible
representations of \(\operatorname{SO}(3)\) (more precisely: the (complex)
irreducible representations of \(\operatorname{SO}(3)\) are exactly the spaces
\(\mathcal{H}_l\)). This is connected to the fact that they are the
eigenfunctions of the spherical Laplacian.&lt;/p&gt;
&lt;p&gt;That the eigenspaces of the spherical Laplacian are representations of \(\operatorname{SO}(3)\)
follows directly from the fact that the Laplacian commutes with rotations: we have
a representation \(G \curvearrowright L^2(S^2, \mathbb{C})\) via
\[(r \cdot f)(x) := f(r^{-1}x)\]
for any rotation \(r\) and \(f \in L^2(S^2, \mathbb{C})\). For an eigenfunction
of the Laplacian, we get
\[\Delta_{S^2}(r \cdot f) = r \cdot \Delta_{S^2} f = r \cdot \lambda f = \lambda (r \cdot f)\thinspace,\]
so each eigenspace is invariant under the action of \(\operatorname{SO}(3)\).
Therefore, the representation on \(L^2(S^2)\) can be restricted to each eigenspace,
so each \(\mathcal{H}_l\) gives a representation of \(\operatorname{SO}(3)\).&lt;/p&gt;
&lt;p&gt;Showing that these representations are in fact irreducible is much more
difficult (there&amp;rsquo;s a proof &lt;a href=&#34;https://www.cis.upenn.edu/~cis610/sharmonics.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for example, if you really want to dive into
that). But if we just take that for granted, it&amp;rsquo;s again easy to show that
&lt;em&gt;every&lt;/em&gt; irreducible subrepresentation of \(L^2(S^2)\) is a space of spherical
harmonics: because the Laplacian
is an equivariant map on each such representation, Schur&amp;rsquo;s Lemma implies that
it must be either the zero map (which it isn&amp;rsquo;t) or multipication by a constant
\(\lambda \in \mathbb{C}\). Therefore, each irreducible representation is
contained in an eigenspace of the Laplacian. But these eigenspaces are themselves
irreducible, so the representation in question must already be equal to the
eigenspace.&lt;/p&gt;
&lt;p&gt;Finally, it&amp;rsquo;s possible to show that all irreducible representations of \(\operatorname{SO}(3)\)
are subrepresentations of \(L^2(S^2)\). This is again much more difficult and
is also a very special fact about \(\operatorname{SO}(3)\) (for example,
the Laplacian&amp;rsquo;s eigenspaces are still irreducible representations in higher dimensions,
but they are not the only ones anymore). But combining this with our results
from above, the spherical harmonics make up all the irreducible representations
of \(\operatorname{SO}(3)\).&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I&amp;rsquo;m skipping over some details here, see for example Claim 4.0.1 &lt;a href=&#34;http://www-users.math.umn.edu/~garrett/m/mfms/notes%5Fc/spheres%5FI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See Corollary 4.0.6 &lt;a href=&#34;http://www-users.math.umn.edu/~garrett/m/mfms/notes%5Fc/spheres%5FI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the same document&lt;/a&gt; for a proof&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Too much structure</title>
      <link>https://ejenner.com/post/too-much-structure/</link>
      <pubDate>Wed, 27 Jan 2021 08:53:00 +0100</pubDate>
      <guid>https://ejenner.com/post/too-much-structure/</guid>
      <description>&lt;p&gt;When proving simple statements in point set topology, there is often only
one obvious next step that can be done given the objects and statements you
already have&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. You don&amp;rsquo;t need to think about what you eventually want to prove
because there is only one step that will lead to a proof of &lt;em&gt;anything&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;an-example-continuous-images-of-compact-spaces-are-compact&#34;&gt;An example: continuous images of compact spaces are compact&lt;/h2&gt;
&lt;p&gt;As an example, let&amp;rsquo;s go through the proof that the image of a compact space
under a continuous map is again compact: we start with a compact space \(X\)
and a map \(f: X \to Y\). To make the notation less unwieldy, we&amp;rsquo;ll assume that \(f\) is surjective,
so we&amp;rsquo;ll show that \(Y\) is compact, but the proof works exactly the same without this assumption
(just replace every occurence of \(Y\) with \(f(X)\)).
Because we want to show that \(Y\) is compact,
i.e. that every open cover of \(Y\) has a finite subcover, we also start with
a given open cover \(Y = \bigcup_i U_i\).&lt;/p&gt;
&lt;p&gt;With only these objects available, there isn&amp;rsquo;t a lot we can do. For example,
if \(X\) were a normed vector space, we would have access to its zero vector
and then could construct \(f(0)\) from that. That wouldn&amp;rsquo;t lead anywhere but it&amp;rsquo;s a branch in the
tree of possible proofs that might distract us. Because \(X\) and \(Y\) have
so little structure, these kinds of options simply don&amp;rsquo;t exist.&lt;/p&gt;
&lt;p&gt;The only thing I can come up with is that we can look at the preimage of
each of the \(U_i\) under \(f\). This gives us a collection \(f^{-1}(U_i), i \in I\)
of subsets of \(X\). Such a collection in itself still doesn&amp;rsquo;t allow us to do anything
interesting, but because preimages preserve unions, we have
\[\bigcup_i f^{-1}(U_i) = f^{-1}\left(\bigcup_i U_i\right) = f^{-1}(Y) = X\]
so this collection is in fact a cover of \(X\). Because \(f\) is continuous, it is also
an &lt;em&gt;open&lt;/em&gt; cover.&lt;/p&gt;
&lt;p&gt;Again, there isn&amp;rsquo;t much we can do with this newly constructed open cover. We could
map it back to \(Y\) with \(f\) immediately but that just gives us back the open cover
of \(Y\) we started with.
The other thing we can do with an open cover of \(X\) is pick a finite subcover:
\(X\) is compact, and we can think of that procedurally as a way of turning any
open cover into a finite subcover.&lt;/p&gt;
&lt;p&gt;So now we have a new object: a finite open subcover \(X = U_{i_1} \cup \ldots \cup U_{i_k}\).
Inside \(X\), there isn&amp;rsquo;t anything else we can do with a (finite) cover, so the only
option is to now apply \(f\) again, which gives us sets \(f(f^{-1}(U_{i_1})), \ldots, f(f^{-1}(U_{i_k})) \subset Y\).
Because \(f\left(f^{-1}(U_i)\right) = U_i\), this is a finite subset of the open cover we
started with.&lt;/p&gt;
&lt;p&gt;An then we&amp;rsquo;re done because it is also a cover:
\[\bigcup_{j = 1}^k f\left(U_{i_j}\right) = f\left(\bigcup_{j = 1}^k U_{i_j}\right) = f(X) = Y\]&lt;/p&gt;
&lt;p&gt;The thing that I hope you took away from this walkthrough is how few choices
there were at each step. Apart from some steps that obviously didn&amp;rsquo;t add anything new,
there was always only one thing to do next.&lt;/p&gt;
&lt;p&gt;We didn&amp;rsquo;t even specifically aim to construct a finite subcover of \(\bigcup_i U_i\)
for most of the proof, we just &amp;ldquo;went with the flow&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This is a feeling that is much more rare in e.g. real analysis, even for proofs that
are similarly easy as the one above. With some experience, you might get enough
intuition to discard all the wrong options immediately but they&amp;rsquo;ll still be there.
You typically have to keep in mind what you want to prove and deliberately
steer your proof in that direction, otherwise the number of possible paths you
could take just explodes and you never get anywhere.&lt;/p&gt;
&lt;h2 id=&#34;the-importance-of--lack-of--structure&#34;&gt;The importance of (lack of) structure&lt;/h2&gt;
&lt;p&gt;The decisive difference between the point set topology example and real analysis is,
I think, how much structure the spaces and objects
we are working with have. By &amp;ldquo;structure&amp;rdquo;, I mean the same somewhat
elusive concept I&amp;rsquo;ve previously talked about &lt;a href=&#34;https://ejenner.com/post/perspectives-on-structure&#34;&gt;here&lt;/a&gt;. In short, a group is
a set with some additional structure and a field adds even more structure.
The way I use the word, a manifold also has more structure than a topological
space (even though it doesn&amp;rsquo;t require any new choices).&lt;/p&gt;
&lt;p&gt;One of the aspects of structure I talked about in the post I just linked is
that objects with less structure admit fewer definitions and theorems.
Applying theorems to the objects we&amp;rsquo;ve already constructed is how we
make progress in our proofs. So having fewer theorems to work with
leads to a proof tree with a lower branching factor: at each step of
the proof, there are only a few things we can do. In an extreme case,
we have a branching factor of one and can do the proof on autopilot,
as in the topology example above.&lt;/p&gt;
&lt;p&gt;If you are working on \(\mathbb{R}^n\) on the other hand, you can use all the topological
properties you had before, but you can also view \(\mathbb{R}^n\) as a vector space, you
can talk about lengths and angles and even about the Lebesgue measure of sets. This is possible
because \(\mathbb{R}^n\) has a lot of canonical structure,
so you suddenly have many more tools at your disposal.&lt;/p&gt;
&lt;p&gt;This explains why it can help to generalize a statement you are trying to prove:
afterwards, you have less structure to work with. Assuming the statement is still
true in its more general form, the tree of possible proofs has a much smaller
branching factor and becomes easier to explore.&lt;/p&gt;
&lt;h2 id=&#34;propositions-as-types&#34;&gt;Propositions as types&lt;/h2&gt;
&lt;p&gt;One last thing to mention is that all of this is closely connected to the &amp;ldquo;propositions as types&amp;rdquo;
interpretation: mathematical propositions can be interpreted as types, with proving a proposition
corresponding to constructing a term of that type. I already talked about constructing new objects
using the available objects and theorems and this is exactly the same idea but the language of
type theory formalizes this. If you want to see an example like the topology proof I gave but explicitly
using the propositions as types view, check out section II. in &lt;a href=&#34;https://www.lesswrong.com/posts/Xfw2d5horPunP2MSK/dependent-type-theory-and-zero-shot-reasoning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;. If you haven&amp;rsquo;t seen
the correspondence between propositions and types before and want to learn more,
&lt;a href=&#34;https://www.youtube.com/watch?v=IOiZatlZtGU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this talk&lt;/a&gt; is very fun to watch.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note the &amp;ldquo;simple&amp;rdquo; &amp;ndash; there are obviously really hard to prove statements in point set topology, as in any discipline&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Ways to think about structure in mathematics</title>
      <link>https://ejenner.com/post/perspectives-on-structure/</link>
      <pubDate>Tue, 29 Dec 2020 14:03:00 +0100</pubDate>
      <guid>https://ejenner.com/post/perspectives-on-structure/</guid>
      <description>&lt;p&gt;Most of the objects that appear in mathematics can be thought of as
sets with additional &amp;ldquo;structure&amp;rdquo;. For example, a group is a set \(G\) with
an operation \(G \times G \to G\) fulfilling certain axioms. This operation
is what makes a group feel more &amp;ldquo;structured&amp;rdquo; than a simple set of elements.
A topological space is a set equipped with a topology
and there is a myriad of other examples (graphs, ordered sets, vector spaces,
metric spaces and measure spaces to name a few).&lt;/p&gt;
&lt;p&gt;But &amp;ldquo;structure&amp;rdquo; in this sense is a somewhat elusive concept. We know it when we see it
but it&amp;rsquo;s hard to describe explicitly &amp;ndash; which is why I just gave some examples and hoped
you knew what I meant.
&lt;em&gt;(Sidenote: there is also a more formal notion of structure in mathematical logic but that&amp;rsquo;s not the topic of this post)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The goal of this post is not to give a formal definition of structure &amp;ndash; I&amp;rsquo;m not sure
how helpful that would even be &amp;ndash; but rather to describe different perspectives
that may be useful when thinking about it.
To guide us, we will consider one particular question: what does it mean
to say that object A has &amp;ldquo;more structure&amp;rdquo; than object B? For example, a vector space
has more structure than a group, which has more structure than a simple set.
We will start with more formal (but also more boring) perspectives and then work
our way towards more speculative and fuzzy ones.&lt;/p&gt;
&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll fix a set \(X\) and consider the different possible structures that can be imposed
on \(X\). Calligraphy letters like \(\mathcal{A}\) and \(\mathcal{B}\) refer to the set of all possible structures
of some type, for example the set of all groups on \(X\). Particular instances
are written as \(A \in \mathcal{A}\) (e.g. a particular group on \(X\)).&lt;/p&gt;
&lt;p&gt;We will write \(\mathcal{A} \prec \mathcal{B}\) for the informal notion that \(\mathcal{A}\)
has &lt;em&gt;more&lt;/em&gt; structure than \(\mathcal{B}\), for example \(\text{fields} \prec \text{groups}\).
An alternative way to think about this (which hopefully explains why the \(\prec\) sign points the way
it does) is that fields are a special case of groups (each field is also a group),
which means that the set of fields is in some sense a subset of the set of groups.
This leads us right into the first perspective on structure.&lt;/p&gt;
&lt;h2 id=&#34;structure-can-be-canonically-removed&#34;&gt;Structure can be canonically removed&lt;/h2&gt;
&lt;p&gt;If \(\mathcal{A} \prec \mathcal{B}\) (\(\mathcal{A}\) has more structure than \(\mathcal{B}\)),
then there is a canonical way to turn any instance
\(A \in \mathcal{A}\) into an instance \(B \in \mathcal{B}\).
As an example, a vector space can be canonically turned into a group by just using vector
addition as the group operation and ignoring scalar multiplication. Or any metric space
can be treated as just a topological space by using the topology induced by the metric
and ignoring the metric itself (category theory footnote&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;h2 id=&#34;structure-leads-to-smaller-symmetry-groups&#34;&gt;Structure leads to smaller symmetry groups&lt;/h2&gt;
&lt;p&gt;If \(\mathcal{A} \prec \mathcal{B}\), then the automorphism group&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; of \(A \in \mathcal{A}\)
is smaller than the group of the corresponding \(B \in \mathcal{B}\) (where &amp;ldquo;corresponding&amp;rdquo;
means that \(B\) is just \(A\) with parts of the structure removed as described in the previous
section).&lt;/p&gt;
&lt;p&gt;For example, we can treat the real numbers as a metric space or as a topological space.
For a metric space, the automorphism group consists of only isometries (i.e. maps that
don&amp;rsquo;t change distances between points), which for the real numbers are only translations.
If we treat them as a topological space though (which has a lot less structure), then the
automorphisms are all the homeomorphisms of the real number line, which form a much larger
group.&lt;/p&gt;
&lt;h2 id=&#34;more-structure-leads-to-fewer-structure-preserving-maps&#34;&gt;More structure leads to fewer structure-preserving maps&lt;/h2&gt;
&lt;p&gt;If we consider two sets \(X\) and \(Y\), there are \(|Y|^{|X|}\) maps from \(X\) to \(Y\). If we now introduce
a group structure, most of those maps are typically not homomorphisms, i.e. not structure-preserving.
If we then turn the groups into rings, even fewer maps will additionally be compatible with
the ring multiplication. So adding structure reduces the number of maps which preserve
all of that structure (which is pretty obvious when put like that).&lt;/p&gt;
&lt;p&gt;The previous perspective is a special case of this, where \(Y = X\) and we only consider automorphisms
rather than any structure-preserving maps, so it shouldn&amp;rsquo;t be surprising that we also got fewer automorphisms
if we had more structure. But I think it&amp;rsquo;s a very important special case that deserves to be treated seperately
because the interpretation via symmetries makes it much more intuitive than this general
version.&lt;/p&gt;
&lt;h2 id=&#34;structure-allows-more-definitions-and-theorems&#34;&gt;Structure allows more definitions and theorems&lt;/h2&gt;
&lt;p&gt;Now we start getting into slightly more hand-wavy territory. If \(\mathcal{A} \prec \mathcal{B}\), then
there are more concepts we can define for objects with structure \(\mathcal{A}\) than for
objects with structure \(\mathcal{B}\). We can also prove more and stronger theorems
about objects with structure \(\mathcal{A}\) then about objects with structure \(\mathcal{B}\).
This is related to the previous observation that any object with structure \(\mathcal{A}\)
can be canonically turned into one with structure \(\mathcal{B}\). The important observation
here is that this process &amp;ldquo;is compatible with definitions and theorems&amp;rdquo; (I told you it was getting hand-wavy).
What I mean by that is that if some property holds for \(B\), then it also holds for any object
\(A\) which can be turned into \(B\) by forgetting parts of its structure.&lt;/p&gt;
&lt;p&gt;Some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On a Riemannian manifold, we can do things like measure the angle at which two curves intersect,
which is simply not a concept that makes sense for a manifold without a metric&lt;/li&gt;
&lt;li&gt;Rings allow us to talk about divisibility, which does not have an analogon if only a group structure
is avaliable&lt;/li&gt;
&lt;li&gt;All vector spaces have a basis but the same is not true for all modules (which have less structure
than vector spaces)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While adding structure &amp;ldquo;preserves definitions and theorems&amp;rdquo;, it can sometimes make definitions
trivial or collapse certain distinct concepts into one. For example, divisibility becomes very boring
on fields because every element is divisible by every other element (except 0).&lt;/p&gt;
&lt;h2 id=&#34;algorithmic-complexity&#34;&gt;Algorithmic complexity&lt;/h2&gt;
&lt;p&gt;Now it&amp;rsquo;s getting really hand-wavy, so activate your lack-of-rigor-deflectors.&lt;/p&gt;
&lt;p&gt;Loosely speaking, algorithmic complexity (or Kolmogorov complexity) measures how long the shortest
possible description of some object is. This can be formally defined for bit sequences
but I will appeal to your intuition to also apply it to other things like mathematical
structures, without explicitly specifying how to encode those as bit sequences.&lt;/p&gt;
&lt;p&gt;One connection between structure and complexity is quite obvious: if \(\mathcal{A} \prec \mathcal{B}\),
then describing \(\mathcal{A}\) is more complex. For example, describing what a field is
takes slightly longer than just describing what a group is because there are more axioms
that need to be specified. Similarly, defining a Riemannian manifold is more complex
than just defining what a topological space is. Note that I switched from talking about
a description of \(\mathcal{A}\), e.g. the set of all groups, to talking about a definition of what a group is.
But in terms of descriptive complexity those are essentially the same since the shortest
description of the set of all groups on \(X\) consists of a definition of what a group
is and then saying &amp;ldquo;all groups on \(X\)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;There are some cases where this complexity perspective becomes a bit of a stretch. For example,
it&amp;rsquo;s not obvious that defining a metric space is more complex than defining a topological space
(unlike in the case of fields and groups, where the hierarchy is clear). I&amp;rsquo;d argue that it is in fact
more complex because you need concepts like the real numbers which are pretty complicated
compared to topological spaces. But there might be other examples where there really is a very
short description of something which nevertheless has a lot of structure in terms of the other
perspectives above. This is fine: our goal here is not to give a formal definition of structure
but rather to list some of the properties that are typically associated with it.&lt;/p&gt;
&lt;p&gt;There is another, more interesting way in which complexity comes into play when talking about
structure: how long is an average description of a particular element \(A \in \mathcal{A}\) (given
a description of \(\mathcal{A}\))? Some examples to build intuition about this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specifying a topological space can be extremely complex. Because there is such a large number of
possible topologies on a fixed set, most of them need to have very long desriptions.
Also note that those topological spaces with very simple descriptions are often those that have
a natural additional structure. For example, to define the Euclidean topology on \(\mathbb{R}^n\),
we usually first define its vector space structure, use that to define a metric and then use that
to define a topology&lt;/li&gt;
&lt;li&gt;Specifying a field on a finite set is very easy: there is at most one anyways (up to isomorphism)&lt;/li&gt;
&lt;li&gt;If the cardinality of \(X\) is prime, there is also only one group on \(X\). Otherwise, there might be more
but still far fewer than there are topologies&lt;sup&gt;[I think, citation needed]&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This seems to point towards more structure making it easier to specify a particular instance. But this is
not always the case. For example, a Riemannian smooth manifold has more structure than just a smooth
manifold (according to all the previous perspectives). But since every smooth manifold can be equipped
with a Riemannian metric but that metric is not uniquely determined by the manifold, describing
a Riemannian manifold usually takes longer than just describing a smooth manifold without a metric,
because the choice of metric needs to be specified.&lt;/p&gt;
&lt;p&gt;In general, adding structure means that there might be additional choices that need to be specified
(such as a Riemannian metric) but it can also impose restrictions (for example, many topological spaces
can&amp;rsquo;t be turned into metric spaces). These two factors pull the descriptive complexity of individual
instances in opposite directions.&lt;/p&gt;
&lt;h2 id=&#34;inherent-structure-of-objects&#34;&gt;Inherent structure of objects&lt;/h2&gt;
&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; a new perspective for thinking about structure. Instead, I will give an example for a possible &amp;ldquo;application&amp;rdquo; of the
complexity-based perspective. Hopefully that will illustrate how these perspectives can be useful
to have in your mental toolkit.&lt;/p&gt;
&lt;p&gt;There are interesting connections between what we discussed in the previous section and &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%5Fstructure%5Ffunction#The%5Falgorithmic%5Fsufficient%5Fstatistic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov sufficient statistics&lt;/a&gt;. Intuitively speaking,
the Kolmogorov sufficient statistic of a bit string is the part of that string that has &amp;ldquo;structure&amp;rdquo; in the
sense of not being algorithmically random. Any bit string can be efficiently described by first describing
its Kolmogorov sufficient statistic (which is a list of bit strings with the same &amp;ldquo;structure&amp;rdquo;) and then
specifying its algorithmically random component (by giving its index in that list).&lt;/p&gt;
&lt;p&gt;This is exactly analogous to describing e.g. a group on \(X\) by first defining what a group is (or rather
defining an enumeration of all groups on \(X\)) and then saying &amp;ldquo;the 14th object in that enumeration&amp;rdquo;.
The important property of the Kolmogorov sufficient statistic is that this description in two parts is
efficient (there is no shorter description using some other scheme, up to an additive constant).
As an example where this is not the case, we could also specify a group by first defining an enumeration of monoids and then
saying &amp;ldquo;the 247th object in that enumeration&amp;rdquo;. But because there are many more monoids than group,
this description would probably be inefficient in most cases: we save ourselves the specification
of a single axiom but we pay by needing to specify a much higher index.&lt;/p&gt;
&lt;p&gt;Perhaps this idea can be used to define the &amp;ldquo;true inherent structure&amp;rdquo; on an object as its Kolmogorov
sufficient statistic. But fleshing that out is a topic for another post.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In summary, here are all the perspectives we talked about:
If an object has more structure, &amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this structure can always be canonically removed&lt;/li&gt;
&lt;li&gt;it has fewer symmetries&lt;/li&gt;
&lt;li&gt;there are fewer maps between it and other objects that preserve all the structure&lt;/li&gt;
&lt;li&gt;more concepts can be defined and more theorems proven&lt;/li&gt;
&lt;li&gt;specifying the class of objects with that structure tends to be more complex&lt;/li&gt;
&lt;li&gt;specifying that particular object is often easier because the structure
restricts the space of options, but there are exceptions&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;In category theory these are forgetful functors but I&amp;rsquo;m just interested in intuition here, not formalism. In this example, there is also a natural way to turn any abelian group into a \(k\)-vector space, for a given field \(k\), by tensoring with \(k\). But that vector space won&amp;rsquo;t be over \(X\) anymore and in many other cases there is no canonical way to add structure at all.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The automorphism group is the set of all isomorphisms from an object to itself (which becomes a group via composition as the group operation)&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
