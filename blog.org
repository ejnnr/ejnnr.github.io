# -*- org-babel-inline-result-wrap: %s -*-
#+startup: overview
#+hugo_section: post
#+hugo_base_dir: ./
#+hugo_front_matter_key_replace: description>summary
#+options: author:nil tasks:done todo:nil
#+property: header-args :exports both :eval no-export
* DONE Discounting in a relativistic universe :Physics:
CLOSED: [2020-06-20 Sat 12:25]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: discounting-relativistic-universe
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
For people who want to discount the future, special relativity creates
some challenges. There are different ways to handle those but none
seem completely satisfactory which may be yet another argument against
discounting pure utilities.
#+end_description
I've heard that when starting a new sketchbook, you should begin by
drawing some silly doodles on the first page to break the paralysis
that a fresh book full of beautiful blank pages can induce.
So for my first blog post I chose the silliest
topic that came to mind, namely the intersection of ethics, economics
and special relativity.

Discounting is the idea that obtaining value \(V\) some time \(\Delta t\) into the future
is worth only \(f(\Delta t)V\) now where \(f(\Delta t) < 1\) is the discount factor.
What exactly "value" means depends on the context. For now we will talk about money
as an example but we will get back to this point later.

This definition of discounting raises an obvious problem: the distance in time to future events is not
invariant under Lorentz boosts, so by discounting like this, your value assignments
become dependent on your frame of reference. Now, as long as you never accelerate,
your frame of reference will stay the same and this isn't a practical problem
(though you may still have objections on aesthetic grounds). But as soon as you change your
state of motion, you'll run into problems.

Imagine that you're about to leave for your vacation in the Alpha Centauri system,
taking the new /Starline 90C/ moving at 90% the speed of light.
Suddenly, Omega comes along and offers you a deal: it will pay you $90 right now
but in return you will have to pay $100 once you arrive on Alpha Centauri in
src_elisp{(format "%0.2f" (/ 4.34 0.9))} {{{results(4.82)}}} years (as seen from your current
frame of reference on earth). This sounds like a great deal to you: you discount
at 3% per year, so the $100 you'll have to pay are only worth
src_elisp{(format "$%0.2f" (* (expt 0.97 4.82) 100))} {{{results($86.35)}}} to you now.

So you accept the deal, board your spaceship and begin accelerating towards
Alpha Centauri. But as you do, you feel your value assignments shifting
-- or rather you realize that you will be on Alpha Centauri in only
src_elisp{(format "%0.2f" (* 4.82 (sqrt (- 1 0.81))))} {{{results(2.10)}}} years
in your new reference frame because of time dilation.
This means that you suddenly value the $100 you will have to pay on arrival
at src_elisp{(format "$%0.2f" (* (expt 0.97 2.1) 100))} {{{results($93.80)}}},
just because you stepped into a spaceship and took off.

So clearly, improper discounting is an important financial hazard for
space tourists. But what should you do instead, if you want to keep
your normal discounting procedures while on earth? 

Now we need to get back to what we meant by "value". If value refers to money,
then discounting is closely related to the fact that you can invest money you
already have now, so getting money at a later point in time is less valuable.
The amount of time used for discounting calculations should then be the proper time
of the money, so it depends on whether you were going to leave most of your
money invested on earth (in which case you should discount with the 4.82 years
in earth's frame of reference) or whether you were going to invest it aboard
the /Starline 90C/ (in which case you should discount with the travel time
of 2.10 years).

But what if you want to discount pure utilities? In that case the question
is no longer one of economics but of ethics. We are looking for a discount
function \(f\) that satisfies the following criteria:
1. \(f\) assigns some discount factor to each point in your future light cone
   -- all of those are events that you might be able to influence and
   therefore need to take into account for utility calculations.
2. On the world line of your current frame of reference, these factors coincide
   with the old discounting factor \(f(t)\) -- "It all adds up
   to normality".
3. \(f\) is invariant under Lorentz boosts in the sense that if your
   velocity suddenly changed and you recalculated all discount factors,
   they would remain the same. Essentially, your ethical judgements
   don't change just because you take a flight to Alpha Centauri at
   relativistic speeds.

I think there is only one way of discountig that satisfies all of these
desiderata[fn::because every point in your future light cone lies on
the world line of some reference frame that you can reach by a Lorentz boost]:
use the spacetime interval instead of the time as measured in your current
reference frame.

The spacetime interval between two points is
\begin{equation}\label{eq:spacetime_interval}
\Delta s = \sqrt{\left(c\Delta t\right)^2 - \left(\Delta x\right)^2 -
\left(\Delta y\right)^2 - \left(\Delta z\right)^2}
\end{equation}
where \(\Delta t\) is their time difference (what we used for discounting before)
and \(\Delta x, \Delta y, \Delta z\) are the spatial distances. The nice
thing about \(\Delta s\) is that it is invariant under Lorentz transformations,
so if instead of discounting with \(f(\Delta t)\), you discount with \(f(\Delta s)\),
then your value assignments won't change when you change frames of reference.

What consequences does this have? For small spatial distances, not much changes.
The \(c\) in equation \eqref{eq:spacetime_interval} means that as long as you could reach
an event while travelling much slower than the speed of light, \(\Delta s \approx \Delta t\).
On the other hand, events that are close to the edges of your future light cone
have \(\Delta s\) close to 0, meaning they are discounted only very weakly.
So you'd care about things that happen in 4.3 years (earth frame) on Alpha Centauri
almost as much as about what happens on earth right now -- and much more than
about things that happen on on earth 4.3 years into the future.

If you think this is absurd, I completely agree. One way to get around this is
to give up desideratum 3 above. Maybe if your velocity changes very suddenly,
it's justified for your value judgements to also change very suddenly?
But there is of course another way to avoid these consequences: if \(f(\Delta s)\)
is independent of \(\Delta s\), then there is no weird bias towards things that
happen close to the edge of your light cone either. In other words: don't discount
pure utilities at all.

For me, the second option is much more appealing. There are already good
reasons against discounting utilities, this simply adds yet another one.
It's by no means an argument showing that you /can't/ consistently discount
events in the far future in a relativistic universe. But if you do, there will
be /some/ consequences that I find rather unintuitive. Either you care a lot
about what happens on faraway star systems in the far future, or how much
you value different things changes whenever you change your velocity.
Take your pick.
* DONE Trading off speed against the probability of success in the Karger-Stein Algorithm :Graphs:
CLOSED: [2020-12-06 Sun 18:00]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: karger-stein
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
The Karger-Stein algorithm is an improvement over Karger's beautiful contraction
algorithm for minimum graph cuts. In this post, I show how it finds the perfect
tradeoff between finding a mincut with high probability and finding it quickly.
In the course of doing so, we will also understand where the somewhat opaque
factor of sqrt(2) comes from.
#+end_description
/(Note: this is an analysis of one aspect of the Karger-Stein algorithm, it's not meant to be a beginner-friendly introduction)/

[[https://en.wikipedia.org/wiki/Karger%27s_algorithm][Karger's algorithm]] randomly contracts a graph and surprisingly, this can be used
to find a minimum cut with probability \(\mathcal{O}(n^{-2})\), where \(n\) is
the number of vertices of the graph.  This is a much, much higher probability
than sampling a graph cut uniformly at random would give! But it still means we
need to run the algorithm \(\mathcal{O}(n^2\log n)\) times to get a high success
probability. Karger's algorithm can be implemented in \(\mathcal{O}(n^2)\)
time[fn::There is also an implementation in \(\mathcal{O}(m)\), where \(m\) is the
number of edges, but for the Karger-Stein algorithm that won't make a difference
and we'll ignore it.], which gives an overall runtime of \(\mathcal{O}(n^4 \log n)\) -- not great,
there are much faster deterministic algorithms.

To understand how the Karger-Stein algorithm improves upon that, we need
the following key result that forms the foundation for Karger's algorithm:

*Theorem*[fn::David Karger: /Global min-cuts in RNC, and other ramifications of a simple min-cut algorithm/, SODA 1993]:
When Karger's algorithm contracts a graph from \(n\) to \(r\) vertices,
any given mincut survives with probability \(\geq {r \choose 2}/{n \choose 2} = \frac{r (r - 1)}{n (n - 1)}\).

In particular, for \(r = 2\), we get the \(\mathcal{O}(n^{-2})\) probability mentioned above.

But note the following: if we make only a few contractions, \(r \lesssim n\), then
mincuts are almost guaranteed to survive! This is the key insight that allows us
to improve the runtime of Karger's algorithm, leading to the improved /Karger-Stein algorithm/.

The idea is the following: first we contract the graph down to roughly \(\frac{n}{b}\) vertices,
where \(b\) is small enough that mincuts are very likely to survive. Then we branch: we again
contract the graph down by a factor of \(b\), but we do so \(a\) times independently
from one another. \(a\) needs to be chosen high enough that mincuts are very likely
to survive /in at least one of the branches/. We repeat this process until we have contracted
the graph down to just 2 vertices[fn::Usually we stop a bit before 2 vertices and just
compute the mincut from there using other methods but that doesn't matter here].
If we chose \(a\) and \(b\) right, at least one of the final leaves of our computational
tree will likely contain a mincut. So we return the best cut we've found among all
the leaves.

The Karger-Stein algorithm as it was originally described and as it is usually presented
uses \(a = 2\) and \(b = \sqrt{2}\). So we always split the computation into two branches
and reduce the number of vertices by a factor of \(\sqrt{2}\) before branching again.
But in this post, I would like to motivate where these numbers come from, as well
as show that they're not the only ones that work. So in the following, we're going to
analyze the "generalized Karger-Stein algorithm" with arbitrary \(a\) and \(b\).

** Success probability
As mentioned above, any minimum cut survives a contraction from \(n\)
to \(r\) vertices with probability \(\geq {r \choose 2}/{n \choose 2} = \frac{r (r - 1)}{n (n - 1)}\).
I said we first contract to "roughly" \(\frac{n}{b}\) vertices -- to be precise we contract
until \(\lceil \frac{n}{b} + 1\rceil\) vertices are left, this will give us a nice bound.
The probability that a mincut survives this contraction is[fn::This and the following analysis
is based on the paper by Karger and Stein: /A new approach to the minimum cut problem/, Journal of the ACM 1996.
The difference is just that I consider arbitrary \(a\) and \(b\), rather than just \(a = 2\) and
\(b = \sqrt{2}\).]:
\begin{equation}
\begin{split}
p &\geq \frac{\lceil \frac{n}{b} + 1 \rceil \lceil \frac{n}{b} \rceil}{n (n - 1)}\\
&\geq \frac{(\frac{n}{b} + 1) \cdot \frac{n}{b}}{n (n - 1)}\\
&\geq \frac{1}{b^2}
\end{split}
\end{equation}

We can now apply this bound recursively: After we have contracted to \(\lceil \frac{n}{b} + 1 \rceil\)
vertices, we can forget that this is a partially contracted graph, and just treat
this number as the "new \(n\)".

We will write \(p_k\) for the survival probability if there are \(k\) levels of recursion
left before we reach the leaves of the tree. So \(p_0\) will be the probability in the
leaves of the recursion tree. Depending on when precisely we stop the recursion
and what method we use to finish the contraction, \(p_0\) might take different values,
but all that matters for us is that it is some constant.

Using the bound we found above, we get the following recurrence:
\[p_{k + 1} \geq 1 - \left( 1 - \frac{p_k}{b^2} \right)^a\]
What's going on here? \(\frac{p_k}{b^2}\) is a lower bound on the probability
that any given mincut survives in one particular branch. So \(\left(1 - \frac{p_k}{b^2}\right)^a\)
is an upper bound on the probability that the mincut survives in none of the
\(a\) branches, and consequently \(1 - \left( 1 - \frac{p_k}{b^2} \right)^a\) is a lower
bound on the probability that it survives in at least one.

This recurrence doesn't have an obious solution we can just read off but with
some rewriting, we can get something that's good enough for our purposes.
Substituting \(z_k := \frac{b^2}{p_k} - 1\), we get
\begin{equation}
\begin{split}
z_{k + 1} &= \frac{b^2}{p_{k + 1}} - 1 \\
&\leq \frac{b^2}{1 - \left(1 - \frac{1}{z_k + 1}\right)^a} - 1\\
&= \frac{b^2 \left(z_k + 1\right)^a}{\left(z_k + 1\right)^a - z_k^a} - 1\\
&\leq \frac{b^2 \left(z_k + 1 \right)^a}{a z_k^{a - 1}} - 1\\
&\leq \frac{b^2}{a} z_k + \text{const}
\end{split}
\end{equation}
where we used \(z_k \geq 1\) in the last step. The constant term may depend
on \(a\) and \(b\) but not on \(z_k\).

If \(a \geq b^2\), then \(z_k \in \mathcal{O}(k)\) which means that \(p_k \in \Omega\left(\frac{1}{k}\right)\).
The depth of recursion for a graph with \(n\) vertices is \(\Theta(\log n)\),
so the overall success probability is \(\Omega\left(\frac{1}{\log n}\right)\).

What this means in words: if we create enough branches (at least \(b^2\)) compared
to how long we contract before branching again, then we get quite a high success
probability -- \(\Omega\left(\frac{1}{\log n}\right)\) means that \(\log^2 n\) runs are enough
to get an overall success probability that approaches 1 as \(n \to \infty\).

But what if \(a < b^2\), i.e. if we don't have enough branches at each stage?
Then the inequality derived above only yields
\(z_k \in \mathcal{O}\left(\left(\frac{b^2}{a}\right)^k\right)\)
so the success probability \(p_k\) can be exponentially low in \(k\).
This means we'd have to repeat the algorithm a potentially exponential
number of times, which would make it useless.

That still leaves the question: why does the Karger-Stein algorithm use \(a = b^2\)
in particular, when \(a > b^2\) would give a success probability at least as high?
For that we need to turn to the runtime complexity.

** Runtime
The runtime of the Karger-Stein algorithm can be described with the following
recurrence:
\[T(n) = aT\left(\frac{n}{b}\right) + \mathcal{O}(n^2)\]
The \(\mathcal{O}(n^2)\) term is for contracting down to roughly \(\frac{n}{b}\)
vertices. At that point, we solve \(a\) smaller version of the original problem,
each of size \(\frac{n}{b}\). That leads to the \(aT\left(\frac{n}{b}\right)\) term.

This kind of recurrence is exactly what the [[https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)][Master theorem]] is for. In this case, if we
choose \(a = b^2\), we get a runtime of \(\Theta(n^2 \log n)\) for a single run of
the Karger-Stein algorithm. We already saw that with \(a < b^2\), we get an exponentially
low success probability, so that choice isn't interesting anyway. Finally, if \(a > b^2\),
we get a high success probability, but the runtime becomes \(\Theta(n^c)\),
where \(c := \frac{\log a}{\log b} > 2\).

We can summarize all our results (and a few I didn't mention) in one table:
| Condition   | Time for single run            | Success probability                     | Total runtime                  | Comment                      |
|-------------+--------------------------------+-----------------------------------------+--------------------------------+------------------------------|
| \(a < b^2\) | \(\Theta(n^2)\)                | exponentially low in \(n\)              | exponential in \(n\)           | Too little branching         |
| \(a = b^2\) | \(\Theta(n^2 \log n)\)         | \(\Omega\left(\frac{1}{\log n}\right)\) | \(\mathcal{O}(n^2 \log^3 n)\)  | Just right                   |
| \(a > b^2\) | \(\Theta(n^c)\) with \(c > 2\) | \(\Omega(1)\)                           | \(\Theta(n^c)\) with \(c > 2\) | Unnecessarily much branching |
The "total runtime" column contains the runtime that is needed to achieve a high success probability
by repeating the Karger-Stein algorithm often enough (at least if \(n\) is large enough). This is the complexity
that we want to minimize in practice.

We can now see that combining the analysis of the success probability with the runtime analysis
explains why the Karger-Stein algorithm uses \(a = b^2\): in the other cases, we are either very
unlikely to succeed and therefore need too many runs, or we are taking unnecessarily long for
a single run.

But also note that any choice of a "branching factor" \(a\) works, as long as we then choose
\(b = \sqrt{a}\). So splitting the computation up into just two subproblems is a reasonable
and simple choice, but from a purely asymptotic perspective it is arbitrary.

* DONE Ways to think about structure in mathematics :Structure:Math:
CLOSED: [2020-12-29 Tue 14:03]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: perspectives-on-structure
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
"Structure" is a concept that keeps popping up when thinking about mathematics
but it's hard to pin down what it is exactly. I discuss several different perspectives
for thinking about it.
#+end_description
Most of the objects that appear in mathematics can be thought of as
sets with additional "structure". For example, a group is a set \(G\) with
an operation \(G \times G \to G\) fulfilling certain axioms. This operation
is what makes a group feel more "structured" than a simple set of elements.
A topological space is a set equipped with a topology
and there is a myriad of other examples (graphs, ordered sets, vector spaces,
metric spaces and measure spaces to name a few).

But "structure" in this sense is a somewhat elusive concept. We know it when we see it
but it's hard to describe explicitly -- which is why I just gave some examples and hoped
you knew what I meant.
/(Sidenote: there is also a more formal notion of structure in mathematical logic but that's not the topic of this post)/

The goal of this post is not to give a formal definition of structure -- I'm not sure
how helpful that would even be -- but rather to describe different perspectives
that may be useful when thinking about it.
To guide us, we will consider one particular question: what does it mean
to say that object A has "more structure" than object B? For example, a vector space
has more structure than a group, which has more structure than a simple set.
We will start with more formal (but also more boring) perspectives and then work
our way towards more speculative and fuzzy ones.

** Notation
We'll fix a set \(X\) and consider the different possible structures that can be imposed
on \(X\). Calligraphy letters like \(\mathcal{A}\) and \(\mathcal{B}\) refer to the set of all possible structures
of some type, for example the set of all groups on \(X\). Particular instances
are written as \(A \in \mathcal{A}\) (e.g. a particular group on \(X\)).

We will write \(\mathcal{A} \prec \mathcal{B}\) for the informal notion that \(\mathcal{A}\)
has /more/ structure than \(\mathcal{B}\), for example \(\text{fields} \prec \text{groups}\).
An alternative way to think about this (which hopefully explains why the \(\prec\) sign points the way
it does) is that fields are a special case of groups (each field is also a group),
which means that the set of fields is in some sense a subset of the set of groups.
This leads us right into the first perspective on structure.

** Structure can be canonically removed
If \(\mathcal{A} \prec \mathcal{B}\) (\(\mathcal{A}\) has more structure than \(\mathcal{B}\)),
then there is a canonical way to turn any instance
\(A \in \mathcal{A}\) into an instance \(B \in \mathcal{B}\).
As an example, a vector space can be canonically turned into a group by just using vector
addition as the group operation and ignoring scalar multiplication. Or any metric space
can be treated as just a topological space by using the topology induced by the metric
and ignoring the metric itself (category theory footnote[fn::In category theory these are forgetful functors but I'm just
interested in intuition here, not formalism. In this example, there is also a natural way to turn
any abelian group into a \(k\)-vector space, for a given field \(k\), by tensoring with \(k\). But that
vector space won't be over \(X\) anymore and in many other cases
there is no canonical way to add structure at all.]).

** Structure leads to smaller symmetry groups
If \(\mathcal{A} \prec \mathcal{B}\), then the automorphism group[fn::The automorphism group
is the set of all isomorphisms from an object to itself (which
becomes a group via composition as the group operation)] of \(A \in \mathcal{A}\)
is smaller than the group of the corresponding \(B \in \mathcal{B}\) (where "corresponding"
means that \(B\) is just \(A\) with parts of the structure removed as described in the previous
section). 

For example, we can treat the real numbers as a metric space or as a topological space.
For a metric space, the automorphism group consists of only isometries (i.e. maps that
don't change distances between points), which for the real numbers are only translations.
If we treat them as a topological space though (which has a lot less structure), then the
automorphisms are all the homeomorphisms of the real number line, which form a much larger
group.
** More structure leads to fewer structure-preserving maps
If we consider two sets \(X\) and \(Y\), there are \(|Y|^{|X|}\) maps from \(X\) to \(Y\). If we now introduce
a group structure, most of those maps are typically not homomorphisms, i.e. not structure-preserving.
If we then turn the groups into rings, even fewer maps will additionally be compatible with
the ring multiplication. So adding structure reduces the number of maps which preserve
all of that structure (which is pretty obvious when put like that).

The previous perspective is a special case of this, where \(Y = X\) and we only consider automorphisms
rather than any structure-preserving maps, so it shouldn't be surprising that we also got fewer automorphisms
if we had more structure. But I think it's a very important special case that deserves to be treated seperately
because the interpretation via symmetries makes it much more intuitive than this general
version.

** Structure allows more definitions and theorems
Now we start getting into slightly more hand-wavy territory. If \(\mathcal{A} \prec \mathcal{B}\), then
there are more concepts we can define for objects with structure \(\mathcal{A}\) than for
objects with structure \(\mathcal{B}\). We can also prove more and stronger theorems
about objects with structure \(\mathcal{A}\) then about objects with structure \(\mathcal{B}\).
This is related to the previous observation that any object with structure \(\mathcal{A}\)
can be canonically turned into one with structure \(\mathcal{B}\). The important observation
here is that this process "is compatible with definitions and theorems" (I told you it was getting hand-wavy).
What I mean by that is that if some property holds for \(B\), then it also holds for any object
\(A\) which can be turned into \(B\) by forgetting parts of its structure.

Some examples:
- On a Riemannian manifold, we can do things like measure the angle at which two curves intersect,
  which is simply not a concept that makes sense for a manifold without a metric
- Rings allow us to talk about divisibility, which does not have an analogon if only a group structure
  is avaliable
- All vector spaces have a basis but the same is not true for all modules (which have less structure
  than vector spaces)

While adding structure "preserves definitions and theorems", it can sometimes make definitions
trivial or collapse certain distinct concepts into one. For example, divisibility becomes very boring
on fields because every element is divisible by every other element (except 0).
** Algorithmic complexity
Now it's getting really hand-wavy, so activate your lack-of-rigor-deflectors.

Loosely speaking, algorithmic complexity (or Kolmogorov complexity) measures how long the shortest
possible description of some object is. This can be formally defined for bit sequences
but I will appeal to your intuition to also apply it to other things like mathematical
structures, without explicitly specifying how to encode those as bit sequences.

One connection between structure and complexity is quite obvious: if \(\mathcal{A} \prec \mathcal{B}\),
then describing \(\mathcal{A}\) is more complex. For example, describing what a field is
takes slightly longer than just describing what a group is because there are more axioms
that need to be specified. Similarly, defining a Riemannian manifold is more complex
than just defining what a topological space is. Note that I switched from talking about
a description of \(\mathcal{A}\), e.g. the set of all groups, to talking about a definition of what a group is.
But in terms of descriptive complexity those are essentially the same since the shortest
description of the set of all groups on \(X\) consists of a definition of what a group
is and then saying "all groups on \(X\)".

There are some cases where this complexity perspective becomes a bit of a stretch. For example,
it's not obvious that defining a metric space is more complex than defining a topological space
(unlike in the case of fields and groups, where the hierarchy is clear). I'd argue that it is in fact
more complex because you need concepts like the real numbers which are pretty complicated
compared to topological spaces. But there might be other examples where there really is a very
short description of something which nevertheless has a lot of structure in terms of the other
perspectives above. This is fine: our goal here is not to give a formal definition of structure
but rather to list some of the properties that are typically associated with it.

There is another, more interesting way in which complexity comes into play when talking about
structure: how long is an average description of a particular element \(A \in \mathcal{A}\) (given
a description of \(\mathcal{A}\))? Some examples to build intuition about this:
- Specifying a topological space can be extremely complex. Because there is such a large number of
  possible topologies on a fixed set, most of them need to have very long desriptions.
  Also note that those topological spaces with very simple descriptions are often those that have
  a natural additional structure. For example, to define the Euclidean topology on \(\mathbb{R}^n\),
  we usually first define its vector space structure, use that to define a metric and then use that
  to define a topology
- Specifying a field on a finite set is very easy: there is at most one anyways (up to isomorphism)
- If the cardinality of \(X\) is prime, there is also only one group on \(X\). Otherwise, there might be more
  but still far fewer than there are topologies^{[I think, citation needed]}
This seems to point towards more structure making it easier to specify a particular instance. But this is
not always the case. For example, a Riemannian smooth manifold has more structure than just a smooth
manifold (according to all the previous perspectives). But since every smooth manifold can be equipped
with a Riemannian metric but that metric is not uniquely determined by the manifold, describing
a Riemannian manifold usually takes longer than just describing a smooth manifold without a metric,
because the choice of metric needs to be specified.

In general, adding structure means that there might be additional choices that need to be specified
(such as a Riemannian metric) but it can also impose restrictions (for example, many topological spaces
can't be turned into metric spaces). These two factors pull the descriptive complexity of individual
instances in opposite directions.

** Inherent structure of objects
This is /not/ a new perspective for thinking about structure. Instead, I will give an example for a possible "application" of the
complexity-based perspective. Hopefully that will illustrate how these perspectives can be useful
to have in your mental toolkit.

There are interesting connections between what we discussed in the previous section and [[https://en.wikipedia.org/wiki/Kolmogorov_structure_function#The_algorithmic_sufficient_statistic][Kolmogorov sufficient statistics]]. Intuitively speaking,
the Kolmogorov sufficient statistic of a bit string is the part of that string that has "structure" in the
sense of not being algorithmically random. Any bit string can be efficiently described by first describing
its Kolmogorov sufficient statistic (which is a list of bit strings with the same "structure") and then
specifying its algorithmically random component (by giving its index in that list).

This is exactly analogous to describing e.g. a group on \(X\) by first defining what a group is (or rather
defining an enumeration of all groups on \(X\)) and then saying "the 14th object in that enumeration".
The important property of the Kolmogorov sufficient statistic is that this description in two parts is
efficient (there is no shorter description using some other scheme, up to an additive constant).
As an example where this is not the case, we could also specify a group by first defining an enumeration of monoids and then
saying "the 247th object in that enumeration". But because there are many more monoids than group,
this description would probably be inefficient in most cases: we save ourselves the specification
of a single axiom but we pay by needing to specify a much higher index.

Perhaps this idea can be used to define the "true inherent structure" on an object as its Kolmogorov
sufficient statistic. But fleshing that out is a topic for another post.

** Conclusion
In summary, here are all the perspectives we talked about:
If an object has more structure, ...
- this structure can always be canonically removed
- it has fewer symmetries
- there are fewer maps between it and other objects that preserve all the structure
- more concepts can be defined and more theorems proven
- specifying the class of objects with that structure tends to be more complex
- specifying that particular object is often easier because the structure
  restricts the space of options, but there are exceptions

* DONE VAEs from a generative perspective :Deep__learning:
CLOSED: [2021-01-06 Wed 14:45]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: vae-generative
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Variational autoencoders are usually introduced as a probabilistic extension of autoencoders
with regularization. An alternative view is that the encoder arises naturally as a tool
for efficiently training the decoder. This is the perspective I take in this post, deriving
VAEs without assuming an autoencoder architecture a priori.
#+end_description
\(
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\E}{\mathbb{E}} % expected value
\newcommand{\R}{\mathbb{R}}
\)
This is my attempt to tell a story[fn::Michael Nielsen calls this "discovery fiction", mentioned for example [[http://cognitivemedium.com/srs-mathematics][here]]]
about how you might invent variational autoencoders (VAEs).
There are already [[https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73][great introductory posts]] doing this and if you haven't seen VAEs before, I would
strongly recommend you start with one of those. These introductions often start with
autoencoders and then extend them to VAEs. In contrast, we will start by asking ourselves
how to generate new data that matches a training distribution and then motivate VAEs
from there. We won't assume an autoencoder-like architecture a priori, instead it will arise naturally
from this motivation.

Of course just this motivation of generating new samples given a training distribution won't uniquely lead to VAEs
-- after all, there are other good options for generative models. So at some points we will
need to make design decisions but hopefully they won't come out of the blue.

One pedagogical note before we start: if this derivation of VAEs seems unnecessarily
long and convoluted, that's because it is. The goal is not to arrive at the VAE framework
as quickly as possible, but rather to make each step seem natural and to avoid any unmotivated
"magical" jumps. It's probably best if you forget for a moment what you know about VAEs,
in particular that they consist of an encoder and a decoder. We will get there at the very end
but initially this preconception might just be confusing.

** Generative models
The goal in generative modeling is the following: we have some family of probability
distributions \(\mathcal{P}\). Given a set of training examples \(\mathcal{D}\) (assumed to be i.i.d.), we now want
to pick the distribution \(p \in \mathcal{P}\) from our family that maximizes the likelihood \(\prod_{x \in \mathcal{D}} p(x)\).
Equivalently, we can maximize the log-likelihood:
\[\argmax_{p \in \mathcal{P}} \sum_{x \in \mathcal{D}} \log p(x)\]
For now, we will consider the simper special case where we only have a single datapoint \(x\) and want
to maximize \(\log p(x)\) (we will get back to the general case at the end).

Optimizing over a family of probability distributions is very abstract. To turn this into a problem
we can actually solve numerically, we will use a parameterized family \(p_\theta(x)\) and optimize over the
parameter \(\theta \in \R^p\). \(p_\theta(x)\) should
be differentiable with respect to \(\theta\), then we can at least find a local optimum for our
problem using gradient ascent.

This still leaves the question which parameterized family we should use. This is the largest
crossroads we'll face in this post: there are many good options to choose from. The challenge we
face is to find a good trade-off between having a flexible family of distributions and
keeping the number of parameters manageable. For example, if \(x\) takes on discrete values,
we could in principle use the full categorical distribution over all possible values of \(x\).
This would be as flexible as possible but the number of parameters might be huge. If \(x\)
describes a \(28\times 28\) binary image, there are already \(2^{28 \cdot 28} = 2^{784} \approx 10^{236}\)
possible values that \(x\) can take, meaning we'd need about that many parameters.

The way we will deal with this problem is to use a continuous mixture of simple distributions.
We will introduce a new latent variable \(z \in \mathbb{R}^k\) on which we /define/ a very simple distribution \(p(z)\), for
example a unit normal, \(z \sim \mathcal{N}(0; I)\). Then we parameterize a distribution
\(p_\theta(x|z)\), which gives us
\[p_\theta(x) = \int p(z) p_\theta(x|z) dz\]
The important point is that for a fixed \(z\), \(p_\theta(\cdot|z)\) may be an extremely simple distribution.
In the example above, we could use an independent Bernoulli for each of the 784 pixels, which
requires only 784 parameters. But because we additionally have a dependency on \(z\), the marginal
distribution \(p_\theta\) can be much more complex (in particular, the pixels are typically not independent).
Of course the dependency on \(z\) will require some additional parameters but this could just be a reasonably
sized neural network, which gives us far fewer parameters than the \(2^{784}\) that a full categorical
distribution would require.

This already describes our model. Sampling from this model is easy: we sample \(z \sim p(z)\) and
then for this \(z\) sample \(x \sim p_\theta(x|z)\). By assumption, both of those distributions are
very simple (and we can also choose them to be easy to sample from).

But evaluating the likelihood \(p_\theta(x)\) of a datapoint is intractable for most models \(p_\theta(x|z)\)
and \(p(z)\) because it requires calculating a complicated integral. Even if we only care about generating
samples, this is a problem: to train the model, we want to maximize \(\log p_\theta(x)\), but we can't even
evaluate it (nor its gradient, for the same reason).

The cleverness of VAEs lies in using the right approximations to make this optimization problem
tractable, and that is what the remainder of this post is about.

** Variational inference
First, we expand the log-likelihood a bit. For any value of \(z\), we have
\[\log p(x) = \log p(z) p(x|z) - \log p(z|x)\]
(not writing our the dependency on \(\theta\) for now).
The first term is easy to evaluate. So if we could evaluate the second term, our problem
would be solved (sidenote on motivation[fn::If you intrinsically care about \(p(z|x)\), for example
because you hope the latent variables will have an interesting meaning, this and parts of the
remaining post are unnecessary, you'll get to VAEs more directly. But my point is that you
don't /need/ that motivation -- VAEs arise pretty naturally even if you only care about
finding a good model \(p_\theta(x)\) of the training data.]).

This is where variational inference comes into play ([[https://ermongroup.github.io/cs228-notes/inference/variational/][here]] is a tutorial if you want to dive a bit deeper
but that's not necessary for this post). The idea of variational inference is that
you have some distribution \(p\) that you care about, but which is intractable to work with.
So you define a family \(\mathcal{Q}\) of simpler distributions and then find
\[q^* := \argmin_{q \in \mathcal{Q}} D(q\Vert p)\]
where \(D(\cdot \Vert \cdot)\) is the Kullback-Leibler divergence (which measures "distances"
between probability distributions, though it is not a metric in the mathematical sense[fn::In particular,
the Kullback-Leibler divergence is not symmetric, which raises the question why we use \(D(q\Vert p)\)
and not \(D(p\Vert q)\). The reason is that the latter would itself lead to an intractable optimization
problem and so we wouldn't have made any progress.]). We
can then use \(q^*\) in place of \(p\) whenever we need to evaluate it.

This may sound like an enourmous amount of computational overhead: to just evaluate
our objective, we will have to solve an entire optimization problem each time! We will later
find a way to alleviate this issue but for now, let's just ignore it and understand
how we would solve the problem naively.

To apply this to our problem, we will approximate \(p(z|x)\) with a simpler distribution \(q_\lambda(z)\),
parameterized by a new parameter \(\lambda\). For example, \(q_\lambda\) could be a Gaussian
and \(\lambda\) would be its mean and covariance matrix. Note that while \(q_\lambda(z)\) does not
explicitly depend on \(x\), the optimal parameter
\(\lambda^*\) does depend on \(x\) because we minimize the Kullback-Leibler divergence between
\(q_\lambda\) and \(p(\cdot | x)\).

The variational inference problem is now minimizing
\[D(q_\lambda\Vert p(\cdot | x)) = \mathbb{E}_{z \sim q_\lambda}\left[\log q_\lambda(z) - \log p(z|x)\right]\]
This still contains the \(p(z|x)\) term that we can't evaluate. But we can get rid of that by writing
\[\mathbb{E}_{z \sim q_\lambda}\left[\log q_\lambda(z) - \log p(z|x)\right]
= \mathbb{E}_{z \sim q_\lambda}\left[\log q_\lambda(z) - \log p(x, z)\right] + \log p(x)\]
We have reintroduced \(\log p(x)\), which is intractable, but crucially,
it doesn't depend on \(\lambda\). So to solve the minimization problem above, we can
also minimize the expected value on the right. Usually, we instead maximize the
negative of that:
\[\argmax_\lambda \E_{z \sim q_\lambda}\left[\log p(x, z) - \log q_\lambda(z)\right]\]
The objective
\[\E_{z \sim q_\lambda}\left[\log p(x, z) - \log q_\lambda(z)\right]\]
is called the /evidence lower bound/ (ELBO) because it is a lower bound on the log evidence \(\log p(x)\):
\[\E_{z \sim q_\lambda}\left[\log p(x, z) - \log q_\lambda(z)\right] = \log p(x) - D(q_\lambda\Vert p(\cdot|x)) \leq \log p(x)\]
For now this fact isn't really interesting, but it will become relevant later.

Maximizing the ELBO is finally a tractable problem: we can write
\[\log p(x, z) = \log p(z) + \log p_\theta(x|z)\]
which is something we can easily evaluate. The expectation is over \(q_\lambda\) which
also doesn't pose a problem[fn::If you've read the previous footnote: this is the point where using the other KL divergence
would mean we're stuck because we have an expectation with respect to \(p(z|x)\)].

** Combining the optimization problems
Let's briefly recap our progress so far. We originally wanted to find
\[\argmax_\theta \log p_\theta(x)\]
which we rewrote as
\[\argmax_\theta \log p_\theta(x, z) - \log p_\theta(z|x)\]
for an arbitrarily chosen \(z\).
We then used variational inference to approximate the intractable term as
\[\log p_\theta(z|x) \approx \log q_{\lambda^*}(z)\]
where \(\lambda^*\) is the solution to the variational problem:
\[\lambda^*(\theta) = \argmax_\lambda \E_{z \sim q_\lambda}\left[\log p_\theta(x, z) - \log q_\lambda(z)\right]\]

So we could now in principle plug in this approximation and solve
\[\argmax_\theta \log p_\theta(x) \approx \log p_\theta(x, z) - \log q_{\lambda^*}(z)\]
but there are problems with this.
First, note that \(\lambda^*\) depends on \(\theta\). If we for example use gradient
ascent to optimize over \(\theta\), we would need to find the new
optimal \(\lambda\) after each gradient step.
Second, using an arbitrarily chosen \(z\) is kind of silly: we optimized
\(q_\lambda\) such that the entire distribution approximates \(p(z|x)\) well,
we should make use of this entire distribution.

So let's go back. We know that the solution to
\[\argmax_\theta \log p_\theta(x) = \argmax_\theta \log p_\theta(x, z) - \log p_\theta(z|x)\]
is the same for any \(z\). So we can also maximize
\[\E_{z \sim q}\left[\log p_\theta(x, z) - \log p_\theta(z|x)\right]\]
instead, for an arbitrary distribution \(q\).
Plugging in our approximation, we get
\[\E_{z \sim q}\left[\log p_\theta(x, z) - \log q_{\lambda^*}(z)\right]\]

The question now is which distribution \(q\) to use. But note that by using \(q = q_{\lambda^*}\),
we again get the ELBO, this time as the objective for our original optimization problem.
This is a good choice for two reasons:
1. The ELBO is a lower bound on the evidence, \(\text{ELBO} \leq \log p(x)\). If we used another
   distribution \(q\), we wouldn't have any guarentee that we're optimizing for the right thing
   if the approximation \(p(z|x) \approx q_{\lambda^*}(z)\) became bad enough. This way, we're
   at least optimizing a lower bound on what we really care about.
2. We saw above that we need to find the new \(\lambda^*(\theta)\) after each update to \(\theta\),
   which is very inefficient. But the ELBO is already our objective for \(\lambda\), so now we have
   the same optimization objective for both parameters and can optimize them jointly.

With this choice of \(q = q_{\lambda^*}\), the joint optimization problem becomes
\[\argmax_{\theta, \lambda} \E_{z \sim q_\lambda}\left[\log p_\theta(x, z) - \log q_\lambda(z)\right]\]
We can use the reparameterization trick / pathwise gradients to optimize this
efficiently with gradient ascent.

** Using an encoder
For a single datapoint \(x\), we now have an efficiently solvable problem. But now we get
back to the more interesting setting of an entire dataset \(\mathcal{D}\). We then want to
optimize the likelihood of the entire dataset:
\[\sum_{x \in \mathcal{D}} \log p_\theta(x)\]
The problem is that \(q_\lambda(z)\) is supposed to
approximate \(p(z|x)\), so the optimal \(\lambda\) is different for each datapoint \(x\).
Full variational inference would mean using a separate parameter
\(\lambda\) for each datapoint. The optimization would then be over \(\theta, \lambda_1, \ldots, \lambda_n\),
where \(\lambda_i\) is the parameter for the \(i\)-th datapoint. This again gets us into the realm
of a huge number of parameters and computational infeasibility.

So instead, we use /[[https://gordonjo.github.io/post/amortized_vi/][amortized variational inference]]/. This means that instead of optimizing parameters for
\(q_\lambda\) for each \(x\), we learn a function \(x \mapsto \lambda\). This function is trained to approximate the optimal solution
\(\lambda^*(x)\). The downside is that we're introducing yet another approximation, which can only
worsen how well we maximize the likelihood \(p(x)\). But the big advantage is that evaluating it is
much cheaper than solving an entire optimization problem.

In practice, this means we train a neural
network \(f_\varphi(x)\) to find the best \(\lambda\) (in terms of the objective above) for a given \(x\). We then
use \(q_{f_\varphi(x)}(z)\) in place of \(q_{\lambda^*}(z)\). To make the notation a bit nicer,
we write this as
\[q_\varphi(z|x) := q_{f_\varphi(x)}(z)\]

Then we finally get the VAE objective:
\[\argmax_{\theta, \varphi} \E_{z \sim q_\varphi}\left[\log p(z)p_\theta(x|z) - \log q_\varphi(z|x)\right]\]

** Connection to VAEs in practice
As you've probably guessed by now, \(p_\theta(x|z)\) is the decoder of a VAE
and \(q_\varphi(z|x)\) is the encoder. The ELBO can be rewritten as
\[\begin{aligned}\E_{z \sim q(\cdot | x)} \left[\log p(z)p(x|z) - \log q(z|x)\right]
&= \E_{z \sim q(\cdot | x)} \log p(x|z) - \E_{z \sim q(\cdot | x)} \log \frac{q(z|x)}{p(z)}\\
&= \E_{z \sim q(\cdot | x)}\log p(x | z) - D(q(\cdot|x)\Vert p)\end{aligned}\]
which gives us the interpretation as "reconstruction + regularization loss"
that you may have encountered elsewhere (to treat this as a loss that is minimized,
you would multiply everything by \(-1\)).

\(q_\varphi\) is typically chosen as a normal distribution, because that makes the KL divergence
in the ELBO easy to calculate if \(p(z)\) is chosen as a unit normal. The choice of \(p_\theta\)
depends on the type of data. As mentioned, for binary images we might use independent
Bernoulli distributions for each pixel. For continuous output, a normal distribution is a common
choice.

** Conclusion
We saw how to arrive at VAEs starting from a purely generative motivation, without
assuming an autoencoder architecture a priori. Interestingly, this gives a very different
impression than the "autoencoder perspective": what we really care about is the
decoder, whereas the encoder is just a useful trick to be able to train the decoder efficiently.

This doesn't mean that the autoencoder perspective is wrong of course. Having an encoder
can be intrinsically useful for some applications, and this is something which is missing
in this post. But I think the perspective we took here demonstrates that the VAE architecture
is far less arbitrary than it may seem when starting from autoencoders.

** Further reading
- The [[https://ermongroup.github.io/cs228-notes/extras/vae/][CS 228 lecture notes on VAEs]] take a somewhat similar approach to this post in terms of emphasizing
  the variational inference perspective. They Also contain details on some points that I basically ignored, for example on the reparameterization
  trick
- [[https://arxiv.org/abs/1606.05908][Carl Doersch's tutorial on VAEs]] contains much more detail and also has a different motivation for why
  we want to approximate \(p(z|x)\) (namely to use that to estimate the integral by sampling values of \(z\)
  that contribute the most)
- There is also a [[https://arxiv.org/abs/1906.02691][tutorial by Kingma and Welling]], the authors who introduced VAEs. You could also look
  at their [[https://arxiv.org/abs/1312.6114][original paper]] but that's a lot terser

* DONE Building Blocks of RL Part I: Value-based methods :Reinforcement__learning:
CLOSED: [2021-01-13 Wed 16:58]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: rl-building-blocks-1
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Reinforcement Learning consists of a few key building blocks that can be combined to create
many of the well-known algorithms. Framing RL in terms of these building blocks
can give a good overview and better understanding of these algorithms. This is part 1
of a series with such an overview, covering value-based methods (mainly in a tabular
setting).
#+end_description
Reinforcement Learning consists of atomic building blocks that can be combined to create
many of the well-known algorithms. This is not a secret but it can sometimes be
obscured when learning about different methods one after another, never getting
the big picture view. So this is my attempt at the kind of overview I would have like
when I first got into RL. How helpful it is to you probably depends a lot on how similar
your learning style is to mine.

This is part 1 of of a planned three-part series. [[/post/rl-building-blocks-2][Part 2]] will be about policy optimization and
[[/post/rl-building-blocks-3][part 3]] about model-based RL.

** Motivation
Why consider the building blocks of RL individually at all? There are at least two good
reasons:
1. It makes RL methods easier to memorize. This is for two reasons: first, memorization
   becomes easier when the material is split into small chunks. Second, many of the building
   blocks are shared by several methods, so we can avoid duplicate effort more effectively
   by explicitly considering these building blocks.
2. More importantly, it gives a better understanding of the landscape of RL methods. A very naive view of
   RL methods would just consider them as a very long list of possibilities. But in reality,
   they are more of a very high-dimensional table, with different options to choose from
   for different aspects of the algorithm.

** Target audience and what this is not
On its own, this is not an introduction to Reinforcement Learning; I assume that you already
know most of the definitions and algorithms and mainly describe how they fit into one common framework.
That said, it might be helpful to read this series in parallel to learning about the algorithms
it covers. Or you can use it as a review, or to deepen your big picture of RL. If you're already
very familiar with RL theory, you probably won't find anything new.

This is also not a guide on which method to choose for which problem. It might
/help/ with that but I don't focus on the various advantages and disadvantages.

Finally, this overview is far from exhaustive. My main goal is to present the framework
and give enough examples to provide intuition for how concrete algorithms fit in.
In particular, I focus on a tabular setting
(for Part 1) and cover Deep RL only briefly towards the end. All of the things I discuss
for a tabular setting are still relevant for Deep RL, so it should still be useful even
if you're not interested in tabular RL for its own sake. But if you're looking for an overview
of the parts that are specific to Deep RL, this is not it.

** Notation
- \(A_t\) is the action taken at time \(t\) while the agent is in state \(S_t\). Afterwards,
  the environment returns a reward \(R_{t + 1}\) and a new state \(S_{t + 1}\)
- The return \(G_t\) is the discounted sum of rewards from time \(t\) onwards
  \[G_t = \sum_{k = 1}^\infty \gamma^{k - 1} R_{t + k}\]
** Value functions
This post is about value-based methods, which means the model explicitly learns and represents
a value function and uses that value function to compute the policy (I will cover actor-critic methods
when we talk about policy optimization in part 2).

There are two types of value functions: state-value functions or V-functions assign a value to
every state \(s\). We write \(v_\pi(s)\) for such a value function. Q-functions assign a value
to every state-action pair \((s, a)\), i.e. to taking action \(a\) in state \(s\), and we write them
as \(q_\pi(s, a)\). Many algorithms work essentially the same for both kinds of functions but there will be
a few cases where we need to make a distinction.

Finding these value functions \(v_\pi\) or \(q_\pi\) for a given policy \(\pi\) is called /policy evaluation/.
Of course just evaluating a policy is not that useful by itself. After all, the goal of reinforcement
learning is to find a good policy. We do this using generalized policy iteration (GPI), which we will talk about more
later. For now you only need to know that GPI is a method (or rather collection of methods) for finding an optimal policy,
which needs to evaluate a policy as one of its substeps. So we will start by only discussing policy
evaluation, keeping in mind that this will later help us with finding good policies as well.

** General shape of the update
We will start in a tabular setting meaning there are only finitely many states and the value function
is a simple lookup table. All the value-based methods in this setting have the same general shape:
we have some observations \(S_t, A_t, R_{t + 1}, S_{t + 1}, A_{t + 1}, \ldots\), which we got from
running policy \(\pi\) on the environment (or on an environment model, more on that in part 3).
We keep an estimate \(V\) of the true value function \(v_\pi\), which is
updated for each observed state \(S_t\) as follows:
\[V(S_t) \gets V(S_t) + \alpha(\text{target} - V(S_t))\]
or analogously
\[Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha(\text{target} - Q(S_t, A_t))\]
for an estimate \(Q\) of \(q_\pi\).
\(\alpha\) is a learning rate which may or may not be constant. \(\text{target}\)
is the key piece that distinguishes all the algorithms we'll look at from one another.
It should be something that is, in expectation, a better value estimate than the old \(V(S_t)\).
In some cases it will depend on \(V\), those are called /bootstrapping/.

** Targets for policy evaluation
Now that we have described the general shape of the update, we can define all the most
popular methods in the tabular setting by just giving the target:
- Monte Carlo :: the target is simply the return \(G_t\). The way we described the general method
     in the previous section, we get every-visit MC. There is also first-visit MC, which updates the estimate
     for each state only once per episode (the first time it occurs).
- TD(0) :: the target is \(R_{t + 1} + \gamma V(S_t)\) or \(R_{t + 1} + \gamma Q(S_t, A_t)\)
     This is also the 1-step return \(G_{t:t+1}\). If we're learning the Q-function,
     this is called /Sarsa/.
- Expected Sarsa :: Like Sarsa, but with an expectation over the next action, rather than
     the actually sampled action: \(R_{t + 1} + \gamma \mathbb{E}_{a \sim \pi} Q(S_t, a)\)
     This only works for Q-functions, since for V-functions, we would need to know the environment dynamics
     to calculate the expected value.
- n-step TD :: The target is the \(n\)-step return \(G_{t:t+n}\).
     This generalizes MC and TD(0): with \(n = 1\), we get TD(0) and with \(n = \infty\), we get MC.
- n-step expected Sarsa :: Uses a variation of the n-step return as the target, where the value of the \(n\)-th
     state is estimated not by the value function but by an expected value over actions:
     \[G_{t:t+n} - \gamma^n Q(S_{t + n}, A_{t + n}) + \gamma^n \mathbb{E}_{a \sim \pi} Q(S_{t + n}, a)\]
     This generalizes expected Sarsa and again only works for Q-functions.
- TD(\(\lambda\)) :: uses \(\lambda\)-returns as the target (an exponentially weighted average for \(n\)-step returns for all values
     of \(n\)

For a complete algorithm, we also need to specify a learning rate. If we decay the learning rate at the
right pace, all these methods are guaranteed to converge to the true \(v_\pi\) (and this decay schedule
is the same for all methods). But of course a constant learning rate can also work well.

There is also dynamic programming, though it is a bit of a degenerate case: if we know the true
environment transition probabilities, there is no need to sample episodes. Instead, we can use
\[\text{target} = \mathbb{E}_{A_t \sim \pi, S_{t + 1}, R_{t + 1} \sim \text{env}} \left[R_{t + 1} + \gamma V(S_{t + 1})\right]\]

** GPI, control and Q-learning
All the methods from the previous section are policy evaluation methods:
if the policy \(\pi\) from which we sample trajectories is fixed, they converge
to \(v_\pi\) or \(q_\pi\), not to the optimal value functions.

As promised, we can use a policy evaluation inside a larger algorithm to
find optimal policies. This works as follows:
1. Start with a random policy \(\pi\) and value function \(V\) or \(Q\)
2. Iterate until convergence:
   1. Run one of the policy evaluation algorithms above for one or several steps
      to make the value estimate closer to the true \(v_\pi\) or \(q_\pi\)
   2. Improve the policy \(\pi\), for example by making it \(\varepsilon\)-greedy
      with respect to the current value estimate
This is called generalized policy iteration or GPI.

The second step in the loop, where we improve the policy, is easy for
Q-functions. The greedy policy is then simply given by
\[\pi'(s) = \operatorname*{argmax}_a Q(s, a)\]
and the \(\varepsilon\)-greedy policy just means following \(\pi'\) with probability
\(1 - \varepsilon\) and choosing randomly with probability \(\varepsilon\).

With V-functions on the other hand, we would need access to the environment dynamics
to compute the greedy policy. Because we usually don't have that,
we use Q-functions if we want to do value-based control. Nevertheless,
V-functions have other important uses (we'll see them again for Actor-Critic methods
in Part 2).

We now add one more target to our growing collection:
\[\text{target} = R_{t + 1} + \gamma \max_{a} Q(S_t, a)\]
This results in Q-learning, which in contrast to all the previous targets learns the optimal
policy directly. So it solves a different problem than policy evaluation and doesn't need
to be combined with GPI.

However, we can also fit Q-learning into the GPI framework: it is equivalent to using
Sarsa[fn::or expected Sarsa, they are the same for a deterministic policy]
and making the policy greedy after each Q-update. Combining this into a single
target that directly learns the optimal Q-function just simplifies things.

** Off-policy learning
/(Here and in the next section, I will write the equations only for V-functions
to make it more readable but they all work exactly the same for Q-functions)/

So far, we have used policy evaluation only to learn the policy \(\pi\) that was used to
sample actions. Off-policy learning means that actions are sampled by the behavior policy
\(b\) but we still want to learn the value function for some other specified policy \(\pi\).

Consider our general update rule:
\[V(S_t) \gets V(S_t) + \alpha(\text{target} - V(S_t))\]
The /expected update/ at each step is
\[\mathbb{E}_{A_t \sim \pi} \left[\alpha(\text{target} - V(S_t))\right]\]
i.e. the amount by which \(V(S_t)\) changes on average on one update.
We want to tweak the update rule in such a way that we get this expected update
even though we are using samples from \(b\) rather than \(\pi\).

There is a general method for estimating an expected value with respect to
one probability distribution \(p\) using samples from a different distribution \(q\).
It's called importance sampling and is simply the observation that
\[\mathbb{E}_{x \sim p} f(x) = \mathbb{E}_{x \sim q} \frac{p(x)}{q(x)} f(x)\]
So when we can only sample from \(q\), we multiply each outcome by the importance
sampling ratio \(\frac{p(x)}{q(x)}\) to adjust. This has nothing to do with reinforcement
learning, it's a much more general method.

So we can use importance sampling for off-policy learning. For a 1-step method such as
TD(0), our new update rule becomes
\[V(S_t) \gets V(S_t) + \alpha\frac{\pi(A_t|S_t)}{b(A_t|S_t)}(\text{target} - V(S_t))\]
Note that this is a strict generalization: if \(b = \pi\), which is the on-policy case we had before,
the importance sampling ratio is one. Also note that we didn't have to modify the target,
so this can be applied the same way to all 1-step methods.

Why did I say "for a 1-step method"? If the target (even implicitly) depends on more future
actions, i.e. \(A_{t + 1}, A_{t + 2}, \ldots\), then these need to be included in the importance
sampling ratio. So in general, we can define
\[\rho_{t:t+n} := \prod_{\tau = t}^{t + n} \frac{\pi(A_\tau|S_\tau)}{b(A_\tau|S_\tau)}\]
and then use the update rule
\[V(S_t) \gets V(S_t) + \alpha\rho_{t:t+n}{b(A_t|S_t)}(\text{target} - V(S_t))\]
where \(k\) is the number of future actions the target depends on. For example, Monte Carlo
has \(n = \infty\) (meaning until the end of the episode) and Sarsa has \(n = 1\).

This means that there is a slight dependency between the target and the importance
sampling ratio, namely the number \(n\) of future steps that are considered. But other than
that, importance sampling works the same for all of our targets.

Now you may have heard that some methods like expected Sarsa are "off-policy
methods" while others are on-policy. This seems to clash with our observation that
importance sampling has almost nothing to do with the update target, so what's going on?

First, we can /always/ use importance sampling and get a method that works in an off-policy
setting. So when we say that Sarsa is on-policy, that just means that we need importance
sampling to use it for off-policy learning.

Second, some methods are off-policy methods in the sense that they already work in
an off-policy setting without importance sampling. This is typically the case because
the target doesn't depend on the sampled action at all. For example in the target for
expected Sarsa, we already take an expectation over the action, so the target itself
is independent of the sampled action. Therefore, it doesn't matter which policy we use for sampling,
only which one we use for taking the expectation inside the target.

For such off-policy methods, the expected update is the correct one no matter which
behavior policy we use. If we use importance sampling, we still get the same expected
value, since the importance sampling ratio has an expected value of one. But for those
methods, there is no reason to use it, and since it increases the variance, it would even hurt.

As a final note, what I described is more specifically called /ordinary/ importance sampling.
There is also /weighted/ importance sampling which lowers the variance at the cost of introducing
some bias. Which of those you use is in principle an orthogonal choice to your update
target.

** What about function approximation?
So far, we only considered a tabular setting, meaning that the value function estimate
is a lookup table that assigns a value to each state. Our update equation reflects this:
the \(\gets\) in
\[V(S_t) \gets V(S_t) + \alpha(\text{target} - V(S_t))\]
only makes sense if we can assign any value to any state.

If the state space is too large or even infinite, this won't work. Instead, we need to
limit ourselves to some family of functions and want to pick one among those that approximates the
true \(v_\pi\) as well as possible. We can then write the value estimate as a function
\(\hat{v}(s, w)\) of the state \(s\) and a parameter \(w\). We can't set \(\hat{v}\) itself anymore, only \(w\).
I'm switching from \(V\) to \(\hat{v}\) only to avoid confusion between tabular and non-tabular
value functions, there's no other difference.

I won't cover many of the theoretical aspects that arise in this setting, such as convergence
guarantees, because that's a big topic in itself. But as long as we focus on just describing
the various methods, rather than on their theoretical properties, function approximation
doesn't require many changes to our framework.

The ideal update would still be
\[\hat{v}(s, w) \gets \hat{v}(s, w) + \alpha(\text{target} - \hat{v}(s, w))\]
but that doesn't work anymore because we can only choose \(w\) directly.
Instead, we introduce a new update rule that works on \(w\) instead of
on the value function itself:
\[w \gets w + \alpha(\text{target} - \hat{v}(s, w))\nabla_w \hat{v}(s, w)\]
It contains the gradient of the value function, which we can think of
as being needed for converting between the thing we want to change
(\(\hat{v}\)) and the thing we can directly change (\(w\)). But other than
that, the update is very similar. In particular, we have the same choices
to make: a learning rate (and how it changes) and the update target.
If we want to use importance sampling, we simply multiply the update
by \(\rho_{t:t+n}\) just as before.
So we only need to change the update rule and can then plug in all the same targets as before.
For example, if we use the Q-learning target with this update rule,
we get the basic algorithm underlying DQN.

That isn't to say that there aren't any other choices that need to be made
when using function approximation. To name just a few important ones:
- We need to choose the parameterized family of functions that we use
  for \(\hat{v}\). In Deep RL, this is a neural network, which means that
  there are many, many options to choose from.
- The update rule for \(w\) above isn't the only one we can use. Chapter 11
  of Sutton's and Barto's [[http://incompleteideas.net/book/the-book.html][book on RL]] contains details on the various choices
  and the issues associated with them but that's beyond the scope of this post.
  Besides what's listed there, we can also choose more complex optimizers.
  Our update rule can be interpreted as SGD on the squared value error,
  so you could instead use SGD with momentum, Adam, or whatever your
  favorite optimizer is.
- We haven't really talked about where the samples that we're using to update
  are coming from. The theoretically simplest case is to always
  sample new actions after the policy is updated. But in practice, you might
  for example want to use a replay buffer instead.

These are the kinds of things that get us from an update rule plus a target
to a full practical method such as DQN. We could try to incorporate as many
of them as possible into our framework but I'm not sure how useful that
would be. In any case, they are arguably not really building blocks of
reinforcement learning in particular; most of them are more generally
about designing and optimizing deep neural networks.

So when we go from tabular RL to function approximation, we get many new choices
on top of the ones we already need to make in a tabular setting. But the building
blocks we've seen for tabular methods, such as update targets or importance
sampling, persist essentially unchanged.

** Summary: building blocks for value-based methods
To summarize, these are the main building blocks for (tabular) value-based methods:
- The target for the update: this is something that should be a better estimate
  of the true value function \(v_\pi\) than the current estimate. Examples include TD(0)
  (including Sarsa), Monte Carlo, expected Sarsa, Q-learning and n-step TD targets
- Importance sampling: there isn't too much choice here. If the target depends on the taken action(s)
  and the behavior policy differs from the target policy, you need importance sampling. Otherwise
  there's no reason to use it. But as mentioned, you can at least choose between ordinary
  and weighted importance sampling.
- The learning rate: could just be a fixed learning rate but might also decay over time
- How to improve the policy: remember that GPI consists of a policy evaluation step where we
  try to find \(q_\pi\), and a policy improvement step where we use our estimate of the value
  function to update the policy. This improved policy might for example be \(\varepsilon\)-greedy
  with respect to our estimate but there are other options

All of these are in principle independent choices: some combinations might work together
better than others but choosing one option in each of these dimensions does give a valid
RL algorithm.

In a function approximation setting, and in Deep RL in particular, we also need to make
many "engineering choices". These are certainly important and can determine whether
an algorithm works really well or doesn't even converge. But what I have hopefully convinced
you of is that all the core building blocks from tabular RL appear in essentially the same
way in deep RL and really are fundamental "building blocks".

Next up: [[/post/rl-building-blocks-2][Part 2]], where we will apply a similar breakdown to policy optimization methods.
* DONE Too much structure :Structure:Math:
CLOSED: [2021-01-27 Wed 08:53]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: too-much-structure
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Proving things for object that have a lot of structure can be harder
than for object with less structure, simply because the tree of possible
proofs is much wider. This is probably why trying to prove a more general
case is sometimes a helpful strategy.
#+end_description
When proving simple statements in point set topology, there is often only
one obvious next step that can be done given the objects and statements you
already have[fn::Note the "simple" -- there are obviously really hard to prove statements
in point set topology, as in any discipline]. You don't need to think about what you eventually want to prove
because there is only one step that will lead to a proof of /anything/.

** An example: continuous images of compact spaces are compact
As an example, let's go through the proof that the image of a compact space
under a continuous map is again compact: we start with a compact space \(X\)
and a map \(f: X \to Y\). To make the notation less unwieldy, we'll assume that \(f\) is surjective,
so we'll show that \(Y\) is compact, but the proof works exactly the same without this assumption
(just replace every occurence of \(Y\) with \(f(X)\)).
Because we want to show that \(Y\) is compact,
i.e. that every open cover of \(Y\) has a finite subcover, we also start with
a given open cover \(Y = \bigcup_i U_i\).

With only these objects available, there isn't a lot we can do. For example,
if \(X\) were a normed vector space, we would have access to its zero vector
and then could construct \(f(0)\) from that. That wouldn't lead anywhere but it's a branch in the
tree of possible proofs that might distract us. Because \(X\) and \(Y\) have
so little structure, these kinds of options simply don't exist.

The only thing I can come up with is that we can look at the preimage of
each of the \(U_i\) under \(f\). This gives us a collection \(f^{-1}(U_i), i \in I\)
of subsets of \(X\). Such a collection in itself still doesn't allow us to do anything
interesting, but because preimages preserve unions, we have
\[\bigcup_i f^{-1}(U_i) = f^{-1}\left(\bigcup_i U_i\right) = f^{-1}(Y) = X\]
so this collection is in fact a cover of \(X\). Because \(f\) is continuous, it is also
an /open/ cover.

Again, there isn't much we can do with this newly constructed open cover. We could
map it back to \(Y\) with \(f\) immediately but that just gives us back the open cover
of \(Y\) we started with.
The other thing we can do with an open cover of \(X\) is pick a finite subcover:
\(X\) is compact, and we can think of that procedurally as a way of turning any
open cover into a finite subcover.

So now we have a new object: a finite open subcover \(X = U_{i_1} \cup \ldots \cup U_{i_k}\).
Inside \(X\), there isn't anything else we can do with a (finite) cover, so the only
option is to now apply \(f\) again, which gives us sets \(f(f^{-1}(U_{i_1})), \ldots, f(f^{-1}(U_{i_k})) \subset Y\).
Because \(f\left(f^{-1}(U_i)\right) = U_i\), this is a finite subset of the open cover we
started with.

An then we're done because it is also a cover:
\[\bigcup_{j = 1}^k f\left(U_{i_j}\right) = f\left(\bigcup_{j = 1}^k U_{i_j}\right) = f(X) = Y\]

The thing that I hope you took away from this walkthrough is how few choices
there were at each step. Apart from some steps that obviously didn't add anything new,
there was always only one thing to do next.

We didn't even specifically aim to construct a finite subcover of \(\bigcup_i U_i\)
for most of the proof, we just "went with the flow".

This is a feeling that is much more rare in e.g. real analysis, even for proofs that
are similarly easy as the one above. With some experience, you might get enough
intuition to discard all the wrong options immediately but they'll still be there.
You typically have to keep in mind what you want to prove and deliberately
steer your proof in that direction, otherwise the number of possible paths you
could take just explodes and you never get anywhere.

** The importance of (lack of) structure
The decisive difference between the point set topology example and real analysis is,
I think, how much structure the spaces and objects
we are working with have. By "structure", I mean the same somewhat
elusive concept I've previously talked about [[/post/perspectives-on-structure][here]]. In short, a group is
a set with some additional structure and a field adds even more structure.
The way I use the word, a manifold also has more structure than a topological
space (even though it doesn't require any new choices).

One of the aspects of structure I talked about in the post I just linked is
that objects with less structure admit fewer definitions and theorems.
Applying theorems to the objects we've already constructed is how we
make progress in our proofs. So having fewer theorems to work with
leads to a proof tree with a lower branching factor: at each step of
the proof, there are only a few things we can do. In an extreme case,
we have a branching factor of one and can do the proof on autopilot,
as in the topology example above.

If you are working on \(\mathbb{R}^n\) on the other hand, you can use all the topological
properties you had before, but you can also view \(\mathbb{R}^n\) as a vector space, you
can talk about lengths and angles and even about the Lebesgue measure of sets. This is possible
because \(\mathbb{R}^n\) has a lot of canonical structure,
so you suddenly have many more tools at your disposal.

This explains why it can help to generalize a statement you are trying to prove:
afterwards, you have less structure to work with. Assuming the statement is still
true in its more general form, the tree of possible proofs has a much smaller
branching factor and becomes easier to explore.

** Propositions as types
One last thing to mention is that all of this is closely connected to the "propositions as types"
interpretation: mathematical propositions can be interpreted as types, with proving a proposition
corresponding to constructing a term of that type. I already talked about constructing new objects
using the available objects and theorems and this is exactly the same idea but the language of
type theory formalizes this. If you want to see an example like the topology proof I gave but explicitly
using the propositions as types view, check out section II. in [[https://www.lesswrong.com/posts/Xfw2d5horPunP2MSK/dependent-type-theory-and-zero-shot-reasoning][this post]]. If you haven't seen
the correspondence between propositions and types before and want to learn more,
[[https://www.youtube.com/watch?v=IOiZatlZtGU][this talk]] is very fun to watch.
* DONE Asymmetry between position and momentum in physics :Physics:
CLOSED: [2021-01-19 Tue 10:52]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: position-momentum-asymmetry
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
In both classical mechanics and QM, there are transformations between position-based
and momentum-based representations that preserve the dynamical laws. So from
a mathematical perspective, position and momentum seem to play equivalent roles
in physics. But they don't play equivalent roles in our cognition, which is part of
the physical universe -- seemingly a paradox.
#+end_description
/Epistemic status: thinking out loud, not an expert on physics/

In physics, there appears to be a deep duality between position and momentum,
in the sense that they are largely equivalent perspectives on viewing the same system.
In classical mechanics, \(x \mapsto p\) and \(p \mapsto -x\) is a canonical transformation,
which means that treating momentum as position and position as negative momentum
results in unchanged dynamics.
In quantum mechanics, the roles of position and momentum can be similarly switched
with a change of basis.

So mathematically speaking, it would appear that there is nothing special about either
position or momentum, both yield similar and equally good descriptions. And yet, human
cognition treats position and momentum very differently, they don't /feel/ like dual descriptions
of reality. To us, there is a big difference between a car that is close to us and moving with a
high relative velocity (distant in momentum space) and one that is far away and more or less
stationary with respect to us.

But human cognition runs on brains, which run on physics, which seems to treat
position and momentum equivalently. So how can this be? How does the
cognitive asymmetry arise from what seems to be symmetry on the fundamental physical
level?

The motivation for this post is mostly to point out the question. I'm not sure myself what the answer is
but I'll at least give my guesses below.

** False assumptions
I've said that human cognition "runs on brains", which "run on physics" and the argument
loses a lot of its punch if this assumption is false. Cognition not running
on physics could mean something like a fundamental Cartesian distinction between body and
mind. That doesn't answer why humans perceive two things differently that appear to be
equivalent in physics but at least that fact doesn't seem as paradoxical anymore.

There's also the possibility that cognition does run on physics but uses physics we don't
know of yet, and in which there is a fundamental difference between position and momentum
that our cognition exploits.

I think that neither of these cases is very likely. If we didn't find any other explanations
for the cognitive difference between position and momentum, then this difference
might be strong evidence for a Cartesian view or for new physics playing a role
in our cognition. But I think there are other, more promising explanations, based
on the observation that while the fundamental physical laws treat position and momentum
the same, the Hamiltonian that happens to govern our universe does not. That's what I'll get to
next.

** Hamiltonian part I: Locality
This explanation is specific for quantum mechanics. So if it turns out to be the
reason for the asymmetry between position and momentum, this would mean
that this feature of our cognition is inherently quantum mechanical and would not
appear in a classical universe.

The Schrdinger equation, which determines the time evolution of a system,
can be written in terms of position as follows:
\[i\hbar \frac{\partial}{\partial t}\psi(x, t) = \left(-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} + V(x)\right)\psi(x, t)\]
This time evolution is local in the following sense: to calculate \(\frac{\partial}{\partial t} \psi(x, t)\),
we only need to know the wave function \(\psi\) in an arbitrarily small neighborhood of \(x\) (so that
we can calculate its second spatial derivative).

We can also write the Schrdinger equation in terms of momentum:
\[i\hbar \frac{\partial}{\partial t}\psi(p, t) = \left(\frac{p^2}{2m} + V\left(i\hbar\frac{\partial}{\partial p}\right)\right)\psi(p, t)\]
What does it mean to plug a derivative into the potential \(V\)? We'll assume that \(V\) is analytic,
which means that it can locally be written as a power series. Then \(V\left(i\hbar \frac{\partial}{\partial p}\right)\)
is defined by plugging in \(i \hbar \frac{\partial}{\partial p}\) into that power series.

If \(V\) happens to be a polynomial, this is just a sum of normal differential operators and the time evolution
is local in exactly the same sense as for position. But in general, \(V\) can be an infinite power series,
and we will take arbitrarily high derivatives of \(\psi\). This means that locality can be violated -- this power
series of derivatives may depend on points that are far away in momentum space[fn::I saw this point made
in [[https://www.lesswrong.com/posts/XDkeuJTFjM9Y2x6v6/which-basis-is-more-fundamental?commentId=A5Pux22d5QKj58fXi][this comment]]]. The most famous example for a power series of differential operators being non-local
is probably the fact that \(\exp\left(a \frac{\partial}{\partial x}\right) f(x) = f(x - a)\) (see e.g. [[https://math.stackexchange.com/questions/1341495/exponential-of-powers-of-the-derivative-operator/1495596][this stackexchange post]]). \(f(x - a)\) depends
on the value of \(f\) outside a small enough neighborhood (if \(a \neq 0\)), so in such cases, the time
evolution in terms of position is /not/ local in the sense described above.

This raises the question: where does the asymmetry between these two formulations of the Schrdinger
equation come from? The answer is that it's all the Hamiltonian's fault. The Schrdinger equation can
be written in basis independent form as
\[i \hbar \frac{\partial}{\partial t} \psi = \hat{H}\psi\]
where \(\hat{H}\) is the Hamiltonian operator. This Hamiltonian usually has the form
\[\hat{H} = \frac{\hat{p}^2}{2m} + V(\hat{x})\]
So the asymmetry on the level of the Hamiltonian is that the momentum operator appears as
a second power, whereas the position operator is plugged into the potential, which may be
an infinite power series.

In the position basis, \(\hat{p}\) turns into a derivative whereas in the momentum basis, \(\hat{x}\)
becomes a derivative. This leads our observation that time evolution is local in the position
formulation in a sense that does not hold for momentum.

** Hamiltonian part II: "Weak" locality
In the previous section, we considered only a single particle (though the same asymmetry
applies to multiple particles -- having only a single particle is the weaker assumption). If we have multiple
interacting particles, we get a different sense of locality that doesn't require QM anymore.

At the beginning I mentioned the difference in our cognition between a distant stationary
car and a nearby car that's moving fast. It's very reasonable that we think about these
situations differently: if a car is very far away, it can't interact with us, i.e. hit us.
The same is not true for momentum: if a car is moving very fast, it can still hit us,
even though it is far away in momentum space.

We might call the fact that spatially distant objects tend to interact less "weak locality".
"Weak" because they can still interact, just typically not as much. So position satisfies
weak locality while momentum apparently doesn't.

The reason for that can again be found in the Hamiltonian. For multiple particles \(i = 1, \ldots, n\),
the Hamiltonian usually has the form
\[H = \sum_{i = 1}^n H_i(x_i, p_i) + \sum_{i \neq j} V(|x_i - x_j|)\]
Here, \(x_i, p_i\) are the position and momentum of particle \(i\). \(H_i\) is the Hamiltonian for
a single particle, which only depends on the position and momentum of that particle. This includes
the kinetic energy and any potentials that are not caused by particle interactions.

The second sum in the Hamiltonian describes the interactions between particles. The way I wrote
it, it can model any pairwise interaction that depends only on the distance between particles.
It so happens that for the forces that actually occur in our universe, the interaction potential \(V\)
diminishes as the distance between the interacting particles increases. This is what leads to
the weak locality in position space. Since the interaction does not depend on the momenta
of the particles, there is no analogous weak locality for momentum.

As in the previous section, the asymmetry again boils down to the Hamiltonian being
asymmetric in position and momentum. This fits rather well with my own intuition.
For example, in a harmonic oscillator, both position and momentum appear as a second
power in the Hamiltonian, and they really do seem much "more equivalent" there
than in other systems.

But it raises the question why the Hamiltonian has such a form. Classical mechanics
or QM themselves don't have an answer; after all, symmetric Hamiltonians such
as the harmonic oscillator work completely fine in principle, it's just that our universe
isn't a harmonic oscillator. I'm not sure whether QFT can shed light onto this question,
otherwise maybe theories of quantum gravity can. This would likely mean a more
fundamental difference between position and momentum, which in turn leads to
the asymmetry in the Hamiltonian. 

Another approach is to say that most possible Hamiltonians aren't symmetric in position and momentum,
so it's not surprising at all that ours isn't. This doesn't feel quite as satisfying and whether
you buy into that argument at all depends on how you think about the "probability" of
physical laws being a certain way. In a similar vein, one could appeal to the anthropic
principle: we can only observe Hamiltonians that permit observers to exist in the
universe they describe. A harmonic oscillator is presumably too simple for that and maybe
the same is true for any Hamiltonian which treats position and momentum exactly
equivalently.
* DONE Building Blocks of RL Part II: Policy Optimization :Reinforcement__learning:
CLOSED: [2021-02-03 Wed 07:39]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: rl-building-blocks-2
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Reinforcement Learning consists of a few key building blocks that can be combined to create
many of the well-known algorithms. Framing RL in terms of these building blocks
can give a good overview and better understanding of these algorithms. This is part 2
of a series with such an overview, covering some policy optimization methods.
#+end_description
/This is part 2 of a three-part series. [[/post/rl-building-blocks-1][Part 1]] covered value-based methods and
also gave some introduction and defined some notation. [[/post/rl-building-blocks-3][Part 3]] will cover model-based RL./

So far, we have looked at the building blocks necessary to learn value functions
for a given policy, called policy evaluation. We have also seen that with GPI, we can use policy evaluation
for control, i.e. to find optimal value functions. The policy was always derived
from the value function, by picking actions (\(\varepsilon\)-)greedily.

In this post, we take a more direct approach to control: what we really
want to learn is a good policy, so why not optimize the policy directly, without
the detour of learning a value function?
** Policy optimization and policy gradient methods
Policy optimization in general means that we have a parameterized
family of policies \(\pi_\theta(a|s)\) and want to maximize the expected
return with respect to the parameters \(\theta\):
\[\operatorname*{argmax}_{\theta} J(\theta)\]
where \(J\) is the expected return:
\[J(\theta) := \mathbb{E}_{\tau \sim \pi_\theta, \mu} R(\tau)\]
Here \(\tau\) is a trajectory which is sampled using the initial state distribution
\(\mu\) and policy \(\pi_\theta\). \(R(\tau)\) is the return of that trajectory.

In principle, there are many ways we could solve this optimization problem.
For example, we could perform a grid search over parameters \(\theta\) and evaluate
the expected return for each parameter by sampling lots of episodes. But this
wouldn't scale well (\(\theta\) might very well be a vector with millions of dimensions
if we use Deep RL). In practice, most methods instead use stochastic gradient
ascent or variations thereof and that is all we will cover in this post.

One sidenote before we dive in: why do we use a parameterized policy at
all? For value-based methods, we started in a tabular setting, where we
could directly assign values to each state. The difference is that even
in a tabular setting, the policy is not an arbitrary function
-- it has to be normalized over actions. So we can't just update a single
probability \(\pi(a|s)\) without also adjusting others.

** Some theory: the policy gradient theorem
If you're only interested in a description of some policy optimization methods, you can
skip this and the next section. But it sheds some light onto why these methods are designed
the way they are and why they work.

We want to optimize the expected return \(J(\theta)\). To see what that entails, we can
write it out explicitly as
\[J(\theta) := \sum_{a \in \mathcal{A}} q^{\pi_\theta}(s_0, a) \pi_\theta(a|s_0)\]
where \(s_0\) is the initial state of the MDP.
To optimize this function using gradient ascent, we need to find \(\nabla_\theta J(\theta)\).
But this seems very difficult at first because while the influence of \(\theta\) on \(\pi_\theta\)
is easy to find, it also affects the state distribution and thereby \(q^{\pi_\theta}\).

Fortunately, the policy gradient theorem comes to the rescue. It states
that
\[\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} q^{\pi_\theta}(s, a) \nabla_\theta \pi_\theta(a|s) \]
where \(\mu^\pi\) is the on-policy state distribution of \(\pi\).
Essentially, we can just apply the gradient to the policy itself and don't need to know
how the state distribution depends on the policy. Section 13.2 of
[[http://incompleteideas.net/book/the-book.html][Sutton and Barto's textbook]] contains more details and a proof.

Another very useful fact is that we can subtract a baseline from the state value:
\[\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \left(q^{\pi_\theta}(s, a) - b(s)\right) \nabla_\theta \pi_\theta(a|s) \]
(the only difference to the previous equation is the \(- b(s)\) term).
This fact is also sometimes called the policy gradient theorem. \(b(s)\) may be any function
or random variable, as long as it doesn't depend on \(a\).

** More theory: score function estimators

Typically, we will still be unable to evaluate the gradient
\[\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} \mu^{\pi_\theta}(s) \sum_{a \in \mathcal{A}} q^{\pi_\theta}(s, a) \nabla_\theta \pi_\theta(a|s) \]
analytically. Some types of gradients of an expected value
can be estimated by sampling:
\[\nabla_x \mathbb{E}_{x \sim p} f(x) = \mathbb{E}_{x \sim p} \nabla_x f(x)\]
so if we can sample from \(p\) and can calculate \(\nabla f(x)\), we can
estimate this gradient. But our case is different: ignoring the expectation
over states (which doesn't pose a problem), we want to evaluate a gradient
of the form
\[\nabla_\theta \mathbb{E}_{a \sim \pi_\theta(a)} f(a)\]
The variable \(\theta\), with respect to which we differentiate, appears in the
distribution, so we can't just approximate this gradient by sampling as
we did in the other case.

Gradients of this form (called /stochastic gradients/) appear often in machine
learning, not just in RL. One method to calculate them is the reparameterization
trick, which you might know from variational autoencoders, but that requires
assumptions that often aren't met in RL. What we will use instead is
the REINFORCE method or /score function estimation/.
We can use the fact that \(\nabla g(x) = g(x) \nabla \log g(x)\) for any \(g\) and write
\[\begin{split}
\nabla_\theta \mathbb{E}_{a \sim \pi_\theta} f(a) &= \int_a f(a)\nabla \pi_\theta(a) da \\
&= \int_a \pi_\theta(a) f(a) \nabla \log \pi_\theta(a) da \\
&= \mathbb{E}_{a \sim \pi_\theta} \left[f(a)\nabla \log \pi_\theta(a)\right]
\end{split}\]
The right hand side has the form we can deal with: an expectation over
some term, with a probability distribution we can sample from.
As long as we can evaluate \(\nabla \log \pi_\theta\), we can now estimate
the gradient we need.

Recall that \(f(s, a) = q^{\pi_\theta}(s, a) - b(s)\) where \(b\) is an arbitrary
baseline. But more generally, we can use any function \(f(s, a)\) which has
\(q^{\pi_\theta}(s, a) - b(s)\) as its expected value, and we will get an unbiased
estimator for the gradient \(\nabla J(\theta)\). Keep this in mind and the various
update targets we will soon see should make sense.

** The general formula
Similar to value-based methods, we can generate many algorithms
for policy optimization using a single update equation.

Using the estimator for the gradient \(\nabla J\), we can learn
the parameter \(\theta\) of the policy with stochastic gradient ascent.
We will use samples \(s_1, a_1, r_2, s_2, a_2, \ldots\) and then
update according to
\[\theta \gets \theta + \alpha \sum_t \Psi_t \nabla \log p_\theta(a_t|s_t)\]
where \(\Psi_t\) is some estimate of \(q^{\pi_\theta}(s_t, a_t)\) and \(\alpha\) is a learning rate.

Later, we will generalize this to
\[\theta \gets \theta + \alpha \sum_t \Psi_t g_t\]
where the gradient \(\nabla \log \pi_\theta\) is replaced by a more
general vector \(g_t\) that determines the direction of the update.

In the next section, we cover possible choices for \(\Psi_t\), and in the
section after that we will look at choices for \(g_t\).

** Targets
Recall from the section on score function estimators that \(\Psi_t\)
should be an estimate of \(q^{\pi_\theta}(s_t, a_t)\).
This means that we can use many of the targets we've already seen in part 1.
In addition, we can subtract a /baseline/ \(b(s_t)\) without changing
the expected value of the update. In principle, \(b(s_t\)) can be any function
of the state, but to reduce variance as much as possible, we usually use a learned
state-value function, leading to so-called Actor-Critic methods. The baseline
can be learned using any of the methods for learning \(v_\pi\) from part 1 (or other policy evaluation
methods).

Here then are typical targets we can use:
- Monte Carlo :: \(\Psi_t = G_0\) or \(\Psi_t = G_t\) i.e. total or future return
- MC with baseline :: \(\Psi_t = G_0 - V(s_t)\) or \(\Psi_t = G_t - V(s_t)\)
- n-step TD with baseline :: \(\Psi_t = G_{t:t+n} - V(s_t)\) (of course the baseline is optional)
- Generalized Advantage Estimation :: \(\Psi_t = G_t^{\lambda} - V(s_t)\) where
     \[G_t^{\lambda} := (1 - \lambda) \sum_{n = 1}^\infty \lambda^{n - 1} G_{t:t + n}\]
     is the \(\lambda\)-return. This is the TD(\(\lambda\)) target with a baseline (which again
     is in principle optional but helps to reduce variance)

This is where the "building blocks" perspective really starts paying off: value-based
methods and policy optimization are very different approaches in terms of their
large-scale design, but on a smaller level, they are composed of some of the same
parts.
** Updating methods: VPG, NPG, TRPO
As promised, we can not only choose the target \(\Psi_t\) but also
have some freedom when it comes to the vector \(g_t\) in whose
direction we update the parameter \(\theta\).

The simplest option is Vanilla policy gradient (VPG), which uses
\(g_t = \nabla \log \pi_\theta(a_t|s_t)\). This is what we've already seen,
it corresponds to stochastic gradient ascent on \(J(\theta)\).

But this simple method has its drawbacks: gradient descent leads
to small changes in the parameter \(\theta\), but it doesn't make
any guarantees about the changes in the policy \(\pi\) itself. If the
policy is very sensitive to the parameter around some value \(\theta_0\),
then taking a gradient step from there might change the policy a lot
and actually make it worse. To avoid that, we'll need to use a small
learning rate, which slows down convergence.

The solution is to use the Natural Policy Gradient (NPG[fn::Sham Kakade, 2001.
[[https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf][(pdf link)]]]) instead of
the usual gradient. Instead of limiting the size of the step in parameter
space, it directly limits the change of the policy at each step (well, not
really[fn::Vanilla gradient descent can be interpreted as follows: we want to
find \(\Delta \theta\) such that \(J(\theta + \Delta \theta)\) is maximized,
but under the constraint that the update isn't too large, \(\Vert\Delta \theta\Vert_2 \leq c\).
As \(c\) gets smaller, the optimal update \(\Delta \theta\) aligns more and more
with the gradient \(\nabla J(\theta)\).
NPG does the same using the Kullback-Leibler divergence between \(\pi_\theta\) and
\(\pi_{\theta + \Delta \theta}\) instead of the \(L^2\) distance between the parameters.
So /infinitesimally/ (i.e. as the learning rate approaches zero), it limits the change in the policy,
but it doesn't actually give any guarantees in the finite regime.]
but that's the intuition).
Natural gradients are a general method for finding optimal probability distributions,
not specific to Reinforcement Learning,
but NPG is probably their most well-known application.

Computationally, the natural gradient is just the normal gradient multiplied by
the inverse Fisher matrix \(F^{-1}\) of the policy. If you want to know
more, [[http://www.scholarpedia.org/article/Policy_gradient_methods#Natural_Policy_Gradients][the Scholarpedia article]] has some details.

For both of these methods, we use a constant learning rate \(\alpha\)
(or one that is adapted using a fixed schedule, \(\alpha = \alpha(t)\)).
The update vector \(g_t\) is given by:
- VPG :: \(g_t = \nabla \log \pi_\theta(a_t|s_t)\)
- NPG :: \(g_t = F^{-1} \nabla \log \pi_\theta(a_t|s_t)\)
     where \(F\) is the Fisher matrix of the policy

A third option is Trust-Region Policy Optimization (TRPO)[fn::John Schulman et al., 2015. [[https://arxiv.org/abs/1502.05477][(arxiv link)]]].
The motivation is similar to that of NPG: limit how much the policy changes (in terms of the
KL divergence). But it takes that idea further and actually guarantees
an upper bound on how much the policy will change.

We can fit TRPO into our framework by using the same update vector as NPG
with a learning rate that adapts at each step:
- TRPO :: \(g_t = F^{-1} \nabla \log \pi_\theta(a_t|s_t)\) and the adaptive learning rate
     \(\alpha = \beta^j \sqrt{\frac{2\delta}{\tilde{g} F^{-1} \tilde{g}}}\)
     where \(\tilde{g} := \Psi_t g_t\)

     \(\beta \in (0, 1)\) and \(\delta\) are hyperparameters and \(j \in \mathbb{N}_0\)
     is chosen minimally such that a constraint on the KL divergence between old and
     new policy is satisfied

Each of these updating methods can be combined with any of the targets,
yielding a 2D grid of algorithms. In practice, some combinations are of course
preferred, for example TRPO is typically used together with GAE. But these
two aren't connected in a fundamental way, it's simply a choice that works
well.

** Conclusion
We've seen that just like for value-based methods, we can get many different
policy optimization methods by plugging in different terms into a single
update equation. Moreover, the targets we can plug in aren't new: we've
used them for value-based methods too.

On the other hand, we've of course only scratched the surface when it comes
to policy optimization methods. For example, we didn't look at deterministic policy
gradients or at PPO. And admittedly these methods don't fit into the framework
presented here as neatly as the ones we did consider. Furthermore, as I already discussed
in Part 1, there are many details that determine whether a method will actually
work in practice that we didn't consider at all.

So I don't want to create the false impression that all of policy optimization can
be reduced to picking a target and an update vector. My goal is rather to convince
you that thinking of RL methods as a combination of several composable building
blocks is a better mental model than thinking about each method individually.
The methods presented here simply fit this mental model especially well: you can combine
any of the targets with any of the updating methods, so the building blocks are
in some sense independent pieces.

Next up: [[/post/rl-building-blocks-3][Part 3]] which discusses model-based RL and concludes this series.
** Resources
- [[https://spinningup.openai.com/en/latest/index.html][Spinning Up]] by OpenAI has explanations and implementations for several policy optimization
  algorithms. If you'd like a more practical perspective, this is a good place to start
- Lilian Weng has a long [[https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html][list of many policy gradient methods]] that might serve as an overview
  or as a quick reference for how a given method works
- More on score function estimators: http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/
  and http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/
* DONE Boring numbers, complexity and Chaitin's incompleteness theorem
CLOSED: [2021-02-10 Wed 16:27]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: boring-numbers
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
There is a "complexity barrier": a number such that we can't prove
the Kolmogorov complexity of any specific string to be larger than
that. The proof of this astonishing fact is closely related to some
famous paradoxa and we'll use this connection to get a better intuition
for why the complexity barrier exists.
#+end_description
Informally, Chaitin's incompleteness theorem states that there is a constant \(L\),
such that we can't prove the Kolmogorov complexity of any specific bit string to
be larger than \(L\).
We /can/ of course prove that there are infinitely many bit strings with higher complexity
than \(L\) -- but we can't name a single one!

John Baez calls this constant \(L\) the [[https://math.ucr.edu/home/baez/surprises.html][complexity barrier]]. And surprisingly,
he argues that it is probably very low (on the order of a few thousand bits for reasonable
encoding schemes)!

At least to me, this is a pretty amazing fact. Consider all the material available on the
internet for instance: everything ever written online, all the videos and images,
and binaries for every computer program out there. "Obviously" we can't just write
a Python program of a few kilobytes that outputs all of this, ... right?
Well, I'm pretty sure we can't, but somewhat incredibly, there's no proof of that!

Take a moment to be properly astonished by this result because the aim of this post
is to make it as obvious as possible. We'll get there soon enough, but first let's
look at some fun paradoxa.

** There are no boring numbers ...

The follwing "paradox" is quite famous:
#+begin_quote
Assume there was an uninteresting natural number. Then the smallest such number
would be interesting -- because it's the smallest uninteresting number,
that's quite an interesting property!
This is a contradiction, so there can be no uninteresting natural numbers.
#+end_quote

We can formalize it as follows: we have some boolean "boringness" property, call it \(P\), defined over
natural numbers. So \(P(n)\) just means "\(n\) is boring".
Being the lowest boring number is itself interesting:
\[P\left(\min_{P(n)} n\right)\quad \text{is false}\]
This is self-contradictory if there are any \(n\) such that \(P(n)\),
so no natural numbers can have property \(P\).

** ... and every number can be described in 13 words or less

There's a arguably more interesting variation of this paradox: let \(P_k(n)\) mean
"\(n\) cannot be described in fewer than \(k\) words". Consider then the description
"the smallest natural number which cannot be described in fewer than 14 words".
In our notation, this would be
\[\min_{P_{14}(n)} n\]
However, this description is only 13 words long, so
\[P_{14}\left(\min_{P_{14}(n)} n\right)\quad \text{is false}\]
which is a contradiction if such a number exists. Therefore, every number can be described
in at most 13 words.

This seems suspicious: while there are many 13-word phrases, there's still only
a finite number of them[fn::At least in languages where you can't just create
an arbitrary number of new words by composing existing ones. If you can, then
one word is enough to "describe" any number]. So there aren't enough short
descriptions to go around for each natural number to get one.

** There are arbitrarily complex strings ...

Of course the problem is that "cannot be described in fewer than \(k\) words" is
not a well-defined property because there is no unambiguous mapping from English
descriptions to numbers.

But what if we replace English by a formal language to circumvent this issue? For example,
a Turing machine without any input either halts and outputs a number (in the form
of a bit string), or it runs forever. If we fix an encoding for Turing machines,
any Turing machine has a length, so we can define
\(P_k(n)\) as "\(n\) is not the output of any halting Turing machine with length less than \(k\)". Or more briefly:
"the [[https://en.wikipedia.org/wiki/Kolmogorov_complexity][Kolmogorov complexity]] of \(n\) is at least \(k\)".

So let's repeat our argument with Turing machines instead of natural language.
We need to define a program that outputs \(\min_{P_k(n)} n\) while being itself
shorter than \(k\). Given a subroutine that checks whether \(P_k(n)\) for arbitrary \(n\),
we could simply iterate over \(n = 1, 2, \ldots\) until we find one such that
\(P_k(n)\) and then output that. If such an \(n\) existed, this program would
output \(\min_{P_k(n)} n\), so by the same argument as before, there can't
be any \(n\) with complexity higher than \(k\) ...

... which can't be right, because just as with natural language descriptions,
there are only finitely many programs of length \(\leq k\), but infinitely many
bit strings.

This time, the issue is the phrase "Given a subroutine ...": \(P_k\) is
undecidable, so this subroutine unfortunately doesn't exist[fn::It might seem like
we've in fact proven that \(P_k\) is undecidable, because if it was decidable,
we'd get a contradiction. But actuallly, we've only shown that \(P_k\) can't
be computed by any program of length less than \(k\). A longer program doesn't
immediately lead to a contradiction.]. Or fortunately, if you value the consistency of mathematics.

In essense, the problem is this: in English, the smallest number with a simple property
can be described in few words because you only need to describe the property and a few
additional words. But the same is not true for Kolmogorov complexity /if the property isn't decidable/.

For any decidable property, the argument works: the smallest number with that property
will have low Kolmogorov complexity (where "low" means "not much larger than the
complexity of the property"). Let's see what we can get by applying that insight.

** ... but not provably complex ones!
This post is supposed to be about Chaitin's incompleteness theorem, so we'd better
connect all of this talk about paradoxa and complexity to that.
The only missing ingredient is to consider /provably/ large
complexities, rather than just large complexities as before.

This means \(P_k(n)\) will now be "\(n\) encodes a proof that some specific bit
string has Kolmogorov complexity higher than \(k\)". We need some system of
logic to make "proof" well-defined and an encoding scheme of proofs as natural
numbers but we'll ignore that since our goal is just to gain intuition.

This new \(P_k\) is clearly decidable: we just need to check whether \(n\) is a valid encoding
of a proof and whether that proof shows that some specific number has Kolmogorov
complexity higher than \(k\).

This program has length \(\log k\) (to encode \(k\))
plus some constant. As we saw, this means that \(\min_{P_k(n)} n\) also has Kolmogorov
complexity at most \(\log k\) (up to a constant): we can iterate over \(n\) and return
the first one for which \(P_k(n)\) holds.

So far there's no contradiction: \(n\) proves that the Kolmogorov complexity of
/some specific number/, call if \(M(n)\), is larger than \(k\). And we've only seen
that the Kolmogorov complexity of \(\min_{P_k(n)} n\) is low. But of course
\(M\) is itself computable and in fact by a pretty short program
(which just looks at what \(n\) proves). So \(M_k := M\left(\min_{P_k(n)} n\right)\)
also has a small complexity, more precisely:
\[K(M_k) \leq \log k + \text{const}\]

But by construction, \(K(M_k) > k\) (that's what \(\min_{P_k(n)} n\) encodes a proof of). So we get
\[k < K(M_k) < \log k + \text{const}\]
which obviously can't hold if \(k\) is sufficiently large. So for \(k\) greater than
some constant \(L\), we run into a paradox ... if any \(n\) encoding such a proof exists.
If there is no \(n\) encoding a proof that some specific bit string has complexity higher than
\(k\), then there is no smallest such \(n\), and we can't define \(M_k\). This proves
Chaitin's Incompleteness Theorem:
#+begin_quote
There is some constant \(L\) such that for any given bit string, we can't prove
it has Kolmogorov complexity higher than \(L\).
#+end_quote

** Further reading
- The "fact" that every number can be described in at most 13 words is known as the [[https://en.wikipedia.org/wiki/Berry_paradox][Berry paradox]]
- You might like [[https://math.ucr.edu/home/baez/surprises.html][John Baez's post]] that I already linked above. In addition to a discussion
  of Chaitin's incompleteness theorem, he talks about how another famous paradox -- the surprise examination
  paradox -- motivates a proof of Gdel's second incompleteness theorem! Or you could read the
  [[http://www.ams.org/notices/201011/rtx101101454p.pdf][paper by Kritchman and Raz]] that introduced that proof.
- Chaitin himself briefly points out the similarity of his impossibility result to Berry's paradoxon in
  /Gdel's Theorem and Information/ (1982)

* DONE L1 regularization: sparsity through singularities :Machine__learning:
CLOSED: [2021-02-17 Wed 09:33]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: sparsity-singularities
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
L1 regularization is famous for leading to sparse optima, in contrast to
L2 regularization. There are several ways of understanding this but I'll
argue that it's really all about one fact: the L1 norm has a singularity
at the origin, while the L2 norm does not. And this is not just true
for L1 and L2 regularization: singularities are always necessary to get sparse weights.
#+end_description

Additive \(L_1\) or \(L_2\) penalties are two common regularization methods
and their most famous difference is probably that \(L_1\) regularization
leads to sparse weights (i.e. some weights being exactly 0) whereas \(L_2\)
regularization doesn't. There are many pictures and intuitive explanations
for this out there but while those are great to build some understanding,
I think they conceal the arguably deeper reason why \(L_1\) regularization
leads to sparse weights. But before we discuss that, we need to understand
why \(L_2\) regularization does /not/ help to get sparse weights.

** \(L_2\) regularization doesn't lead to any sparsity
Let \(w\) be a vector of parameters and \(\mathcal{L}(w)\) be any continuously
differentiable loss function[fn::This is a restriction, for example a model
containign ReLUs will typically only be differentiable almost everywhere,
and as we'll see, individual non-differentiable points will play a big role.
It might be possible to argue that the types of non-differentiable points
created by ReLUs don't change the conclusions of following discussion but
we'll just assume a differentiable loss so we can focus on the conceptual insights.].
For \(L_2\) regularization, we want to find
\[\operatorname*{argmax}_w \mathcal{L}(w) + \beta\Vert w\Vert_2^2\]
This means that the gradient has to be zero:
\[\nabla \mathcal{L}(w) + 2\beta w = 0\]
or in components:
\[\left.\frac{\partial\mathcal{L}}{\partial w_i}\right\rvert_{w_i = 0} + 2\beta w_i = 0\]

So we can get \(w_i = 0\) as the optimal solution only if \(\frac{\partial \mathcal{L}}{\partial w_i}\big\rvert_{w_i = 0} = 0\),
i.e. if \(w_i = 0\) is already optimal without regularization! So \(L_2\) regularization
doesn't help to get sparsity /at all/. The same is true for \(L_p\) regularization for
any \(p > 1\), because
\[\left.\frac{\partial}{\partial w_i} \Vert w\Vert_p^p \right\rvert_{w_i = 0} = 0\]

** \(L_1\) regularization: non-differentiability to the rescue
\(L_1\) regularization just uses the 1-norm instead of the Euclidean
norm:
\[\operatorname*{argmax}_w \mathcal{L}(w) + \beta\Vert w\Vert_1\]
How does that change things? Well, the 1-norm of a vector is not
differentiable at 0. More precisely:
\[\frac{\partial}{\partial w_i} \Vert w\Vert_1 = \begin{cases}
+1, \quad w_i > 0\\
-1,\quad w_i < 0\\
\text{undefined for } w_i = 0
\end{cases}
\]
So when can \(w_i = 0\) be a local minimum of the regularized loss? We can't just
set the derivative to zero as before, because the derivative doesn't exist.

To understand what we can do instead, let's first recall why setting the derivative
to zero works for differentiable functions. If \(f(x)\) has a local minimum
at 0, then this means that \(f(x) \geq f(0)\) for all sufficiently small \(x\).
Since we assumed \(f\) to be differentiable at \(0\), \(f(x)\) is well approximated[fn::meaning that the
error \(f(x) - (f(0) + f'(0)x)\) doesn't just approach 0 for \(x \to 0\)
(that would be continuity), it approaches 0 fast enough that even
\[\frac{f(x) - (f(0) + f'(0)x)}{x} \to 0\]
This is enough to make the rest of argument work out.]
by
\[f(x) \approx f(0) + f'(0)x\]
for small \(x\). So if \(f'(0) > 0\), then \(f(x) < f(0)\) for small negative \(x\),
and if \(f'(0) < 0\) we get the same for positive \(x\). So the derivative at
the minimum has to be zero, because otherwise taking a small step in the right
direction would decrease the value.

We can apply the same idea to the loss with \(L_1\) regularization: what happens
if we take a small step \(h\) away from \(w_i = 0\)? The loss is differentiable
and thus changes approximately linearly:
\[\mathcal{L}\Big\rvert_{w_i = h} \approx \mathcal{L}\Big\rvert_{w_i = 0} + h\frac{\partial\mathcal{L}}{\partial w_i}\]
But for the regularization term, the change is always just \(|h|\), instead
of a linear term:
\[\Vert w\Vert_1 \bigg\rvert_{w_i = h} = \Vert w \Vert_1\bigg\rvert_{w_i = 0} + |h|\]

So if we write \(\tilde{\mathcal{L}}\) for the regularized loss, then we get
\[\tilde{\mathcal{L}}\Big\rvert_{w_i = h} \approx \tilde{\mathcal{L}}\Big\rvert_{w_i = 0} + h\frac{\partial\mathcal{L}}{\partial w_i} + \beta |h|\]
As long as \(\left|\frac{\partial\mathcal{L}}{\partial w_i}\right| < \beta\),
this is larger than the regularized loss at \(w_i = 0\), because
the \(+ \beta |h|\) term will dominate. That is why \(L_1\) regularization
leads to sparser weights: it pulls all those weights to zero whose
partial derivative at 0 has absolute value less than \(\beta\)[fn::Of course this
is not guaranteed for complex loss functions: there might be another local
optimum somewhere else. This is just the condition for 0 to be one of
the local optima at which we might end up.].

** Interlude: Priors
If the loss function \(\mathcal{L}\) models some log-likelihood, then
regularization can be interpreted as performing maximum a posteriori (MAP) estimation
rather than maximum likelihood estimation (MLE). This means we start with some
prior over possible values of the parameter \(w\), update this distribution
using the evidence from the loss function, and then pick the parameters
which are the most probable according to the posterior distribution.

\(L_2\) regularization corresponds to a Gaussian prior and \(L_1\) regularization
to a [[https:en.wikipedia.org/wiki/Laplace_distribution][Laplace prior]] (in both cases centered around 0). So it's natural to
try to explain the sparsity behavior of these regularization methods
in terms of the underlying priors.

Here's what a Gaussian (red) and Laplace (blue) distribution look like, both with
unit variance and properly normalized:
#+caption: Gaussian and Laplace distribution with unit variance (created using [[https://www.desmos.com/]])
[[file:./fig/sparsity-regularization/gaussian_laplace.png]]

One difference is that the Laplace distribution has a higher density
at (and around) 0. I've seen this used as an explanation for sparsity several times:
the Laplace distribution seems more "concentrated" around 0, i.e. assigns a higher
prior to 0, which is why we get sparse solutions.

But that is very misleading (and depending on what is meant by "concentrated" just wrong). Consider the following
figure:
#+caption: Narrower Gaussian
[[file:./fig/sparsity-regularization/narrow_gaussian_laplace.png]]

These are still a normalized Gaussian and a Laplace distribution, the only difference is that
I've chosen a much smaller variance for the Gaussian. This corresponds to
choosing a higher coefficient \(\beta\) for the \(L_2\) penalty. I'd argue that in this case the Gaussian is much more
"concentrated around 0", at least its density is much higher. But even with arbitrarily
high \(\beta\), \(L_2\) regularization won't lead to sparse solutions: you can make
the prior as narrow as you like, and you'll get weights that are closer and closer to
zero but never precisely.

The real difference is the singularity (i.e. non-differentiability) of the Laplace distribution
at 0. Since the logarithm is a diffeomorphism, singularities of the prior correspond
1-to-1 with singularities of the log prior, i.e. the regularization term.

** Singularities are necessary for sparsity
We've seen that the difference between \(L_1\) and \(L_2\) regularization can be explained
by the fact that the \(L_1\) norm has a singularity while the \(L_2\) norm doesn't, or equivalently
that the Laplace prior has one while the Gaussian prior doesn't.

But we can say more than that: a singularity is in fact /unavoidable/ if we want to make sparse weights likely.
If we choose any continuously differentiable prior \(p\) (or any continuously differentiable
additive regularization term), then the overall objective \(\tilde{\mathcal{L}}\) is
continuously differentiable and therefore, the gradient has to be zero at a local minimum.
So for \(w_i = 0\) to be a local minimum, we'd need
\[\frac{\partial \mathcal{L}}{\partial w_i}\bigg\rvert_{w_i = 0} + \frac{\partial \log p}{\partial w_i}\bigg\rvert_{w_i = 0} = 0\]
which puts an /equality/ constrain on \(\frac{\partial \mathcal{L}}{\partial w_i}\): we only
get sparse weights if the gradient has precisely the right value. Typically, this will
almost surely not happen (in the mathematical sense, i.e. with probability 0), so non-singular
regularization won't lead to sparse weights.

In contrast, we saw that
the singularity of the \(L_1\) norm (or the Laplace prior) creates an /inequality/ constraint
for the partial derivative: it leads to \(w_i = 0\) as long as the derivative lies in a certain
range of values. This is what makes sparse weights likely.

* DONE Building Blocks of RL Part III: Model-based RL :Reinforcement__learning:
CLOSED: [2021-02-24 Wed 10:41]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: rl-building-blocks-3
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Reinforcement Learning consists of a few key building blocks that can be combined to create
many of the well-known algorithms. Framing RL in terms of these building blocks
can give a good overview and better understanding of these algorithms. This is
the conclusion of a series with such an overview, covering model-based RL.
#+end_description
In [[/post/rl-building-blocks-1][Part 1]] and [[/post/rl-building-blocks-2][Part 2]], we've seen different methods for learning
good policies. One thing that all of them had in common was that
they only used trajectories sampled from the environment to do so.
This is what's called /model-free/ RL. In this final post, we will
generalize to /model-based/ RL, where we make use of a learned
model of the environment to improve the training process.

** The 10,000-mile satellite's-eye view on RL
From very far away, we can treat all the methods we've previously seen as
functions that map a trajectory and a current parameter to an update for that
parameter. The parameter could describe a value function or a policy.
Training an agent means repeatedly sampling a trajectory, calculating
that update, and updating the parameter.

It will be useful to think about this from the lense of types: an
update method is a function that takes in an object of type "trajectory"
and one of type "parameters" and returns an update of type
\(\Delta\text{parameters}\)[fn::Since parameters in deep learning are usually
vectors, there isn't any formal difference between a parameter and a change
of a parameter. But we'll still distinguish them because there is a big
/conceptual/ difference and because it's more general that way.]:
\[\text{trajectory} \times \text{parameters} \to \Delta\text{parameters}\]
Often, we can decompose this function. For example, a 1-step method
such as Sarsa calculates update based on individual \((s, a, r, s, a)\)
tuples, which we'll call "experience". So Sarsa really defines a function
with signature
\[\text{experience} \times \text{parameters} \to \Delta\text{parameters}\]
We then get the function signature from above by splitting up a trajectory
into its underlying experience tuples, applying the Sarsa update to each
one, and summing the results. This is a simple example but it illustrates the main idea of this post:
to compose small functions in different ways in order to get complete
RL algorithms.

As a slightly more complex example, consider Actor-Critic methods.
We use a policy optimization method (the /actor/) with a signature such as
\[A: \text{trajectory} \times \text{actor-param} \times \text{V-function} \to \Delta\text{actor-param}\]
At the same time, we use some method for learning value functions,
which has the signature
\[C: \text{trajectory} \times \text{critic-param} \to \Delta\text{critic-param}\]
And finally the model for the critic, which can be written as
\[V: \text{critic-param} \to \text{V-function}\]
We can combine these functions using some pretty simple boilerplate code,
to get a function with the
\(\text{trajectory} \times \text{params} \to \Delta\text{params}\)
signature that we want, where \(\text{params} := \text{actor-param} \times \text{critic-param}\)
is the type of the complete collection of parameters.
In Python, this might look as follows:
#+begin_src python :exports src
def train(trajectory, params):
    actor_param, critic_param = params
    v_function = V(critic_param)
    actor_update = A(trajectory, actor_param, v_function)
    critic_update = C(trajectory, critic_param)
    return (actor_update, critic_update)
#+end_src
You should read this more as pseudo-code: the point is not that we would
actually implement an agent exactly like this, but just to show how these
individual functions come together to define an update for the entire
agent.

The code above is completely agnostic to the choice of \(A, C\) and \(V\),
which is an important point throughout this post: we only care about the function
signatures of the methods we use as building blocks, not about how they
work internally.

** A complete training loop
Our ultimate goal is not to compute updates but to find a good policy. For that
we need two more components. First, a function
\[\text{parameters} \to \text{policy}\]
In the case of policy optimization methods, this is simply the parameterization
of the policy, i.e. the model of the actor. If we use value-based methods,
this can instead be decomposed into the value model
\[\text{parameters} \to \text{Q-function}\]
and a function that determines the policy based on the Q-estimate, e.g. an \(\varepsilon\)-greedy
policy,
\[\text{Q-function} \to \text{policy}\]

The second thing we need is a function that samples trajectories -- this is
the role of the environment:
\[\text{policy} \to \text{trajectory}\]
Then we can combine all of these into one function with the signature[fn::Usually,
the type \(A \to B\) refers to /pure/ functions that map objects with type \(A\)
to type \(B\). I use it slightly differently to also include stochastic functions,
which have a random output of type \(B\). For example, the sampled trajectory
is not a deterministic function of the policy, it will instead be different each
time the sampling function is applied.]
\[\text{parameters} \to \text{policy}\]
which takes initial parameter values and then trains a policy (until convergence
or some stopping criterion).

** Modeling the environment
The only role played by the environment in our training algorithm is
to provide a function
\[\text{policy} \to \text{trajectory}\]
for sampling trajectories. But remember our motto: we only care about
function signatures, not the internals of the functions themselves.
So if we can define a function like that in some other way, we can plug
it into our training algorithm without changing anything else.

Before we discuss how to define such a function, let's first take a step
back and consider how this signature is actually implemented by the
environment. An environment is defined by its transition probabilities
\[p(s', r|s, a)\]
i.e. the probability that the next state will be \(s'\) and the reward \(r\)
if action \(a\) is taken in state \(s\). So this defines a function
\[\text{state} \times \text{action} \to D(\text{state} \times \text{reward})\]
where \(D(\cdot)\) denotes distributions with values of a given type.
We can call this the /distributional/ function defined by the environment.

We can sample from distributions, meaning we have a function \(D(x) \to x\)
for any type \(x\). By composing this with the environment function, we get
the signature
\[\text{state} \times \text{action} \to \text{state} \times \text{reward}\]
which we'll call the /sample/ function induced by the distributional function
from above.

The policy type is not an atomic type, it can itself be written as
\[\text{state} \to D(\text{action})\]
and by composing with the sampling map, defines a function \(\text{state} \to \text{action}\).
That makes it clear how we can get the desired signature \(\text{policy} \to \text{trajectory}\)
from the sample function. We start with some initial state (perhaps also
sampled from a distribution), then apply the policy to get an action, then
apply the environment function to get a reward and a new state. Repeat until
the episode ends.

So an environment defines three different functions:
- Distributional function: \(\text{state} \times \text{action} \to D(\text{state} \times \text{reward})\)
- Sample function: \(\text{state} \times \text{action} \to \text{state} \times \text{reward}\)
- Trajectory function: \(\text{policy} \to \text{trajectory}\)
where each one is induced by the one above it in a natural way.

This gives us three different levels on which our environment model
could work: we could model any of these three functions and will at
the end get out a trajectory model \(\text{policy} \to \text{trajectory}\)
to plug into our training algorithm. That said, if we have a distributional
or sample model, we can do somewhat more clever things
with that, which we'll talk about in a moment.

** Where do models come from?
We've seen how we can plug in models into the training loop to replace
the role of the environment. This will always give us a valid method (in the
sense that it doesn't have type errors) but that method will only be useful
to the extent to which the model matches the environment.

So how do we get a good model? In some cases it might be feasible to
hard-code one. More generally, we can treat this as a supervised learning
problem. In that case, we use the real environment to gather trajectories
and then use those trajectories to train our model. We can then either
use only the model to train the agent, or use both real and simulated
trajectories for that.

This means we need to extend the training loop, which now also
has to train the model. This is where we make choices such as how
many trajectories to simulate per real trajectory that trains the model
etc.

** What else can we do with models?
As hinted before, using a model to replace the environment
(by simulating trajectories) is only one of many applications.

First, let's consider sample models (i.e. models that can
generate \((s', r)\) tuples for any state-action input). As we saw
at the beginning of this post, many RL methods don't need entire
trajectories to learn, they instead work on smaller sequences of
experience tuples. For example, 1-step methods compute
updates based on single \((s, a, r, s, a)\) tuples. One immediate consequence
is that we don't need to simulate an entire trajectory before updating
the policy or value function, we can do so after however many
experience tuples we want.

A more interesting advantage of such methods is that we can
use /prioritized sweeping/. We saw that a sample model can create
trajectories by using its output state as the next input.
But no one is forcing us to use that state
as the input! Instead of sampling cohesive trajectories,
we can at each step sample experience tuples for those states or state-action
pairs where we're most uncertain (e.g. as measured by the size
of the last update for those states).

Distributional models allow us to do even more. In particular, we
can apply dynamic programming techniques (value or policy iteration),
which we briefly mentioned in Part 1. You may also recall that for
control, we usually need to learn Q-functions, because a V-function
on its own doesn't tell us which actions are good. But if we have
a distributional model of the environment, then we can turn
a V-function into a Q-function by taking the expectation over next
states and rewards for any given action. So this is one of the rare
cases where learning a V-function is enough to find a good policy.
And of course since a distributional
model induces a sample model, the ideas we saw above still work.

Finally, we can use models during decision time. That means we don't
(just) use a model to /learn/ the policy -- instead, the policy itself
makes use of the model. Concretely, we can do a search over possible
actions, use the model to predict what would happen in each case, and
then use that to improve our estimate of how good an action is.
Monte Carlo Tree Search is one instance of that and works with any
sample model.

** Conclusion
We've seen three different types of environment models: distributional
models, sample models and trajectory models. Each of those can
create trajectories for a given policy and thus replace the role of the environment
in any RL algorithm. But distributional and sample models allow
us to do additional things that are not possible otherwise.

This post also marks the end of the /Building Blocks/ series. The framework
of composing functions based on their signatures hopefully sheds
some light onto how all of these building blocks fit together.
It also demonstrates that the perspective of atomic building blocks
can be applied on multiple levels: the previous posts showed how
building blocks such as certain update targets can be combined
to define different functions for mapping experience or trajectories
to updates for value functions or policies. This post showed
that these functions are themselves building blocks for complete
training loops and can be composed in different ways with models
and sampling procedures.
* DONE Deep Implicit layers :Deep__learning:
CLOSED: [2021-03-03 Wed 14:48]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: implicit-layers
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Several new architectures for neural networks, such as Neural ODEs and
deep equlibirum models can be understood as replacing classical layers
that explicitly specify how to compute the output with implicit layers.
These layers describe which conditions the output should specify but
leave the actual computation up to some solver that can be chosen arbitrarily.
This post contains a brief introduction to the main ideas behind implicit layers.
#+end_description
** Implicit Layers
Layers in neural networks are almost exclusively explicitly specified. That
just means that the output \(y\) is described as a (usually rather simple)
function of the input \(x\) and some parameters \(\theta\), i.e.
\[y = f(x; \theta)\thinspace.\]

The idea behind /implicit/ layers is the following:
#+begin_quote
Instead of specifying how to compute the layers output from the input,
we specify the conditions that we want the layers output to satisfy.
#+end_quote
(that quote is from the [[http://implicit-layers-tutorial.org/][Implicit layers tutorial]] given by Zico Kolter, David Duvenaud
and Matt Johnson at NeurIPS 2020, on which this post is based.
I definitely recommend you check it out if you're interested in all the details that I'll skip over)

"Conditions that we want the output to satisfy" is a bit vague, what does
this look like concretely? Well, it's vague on purpose because implicit
layers are a very general framework. But one simple way to
specify a condition is with
\[g(x, y; \theta) = 0\thinspace.\]
Whereas a forward pass in a neural network classically means applying
the function \(f\) at each layer, we now need to solve this equation for \(y\)
-- that solution will be the output of our layer.

This may sound a bit insane. Isn't solving an equation like that much
more expensive than just applying an explicit function? Why would you
want to do this during each forward pass? There's some truth to that
of course but it's not as bad as it may sound. First, it turns out
that relatively few or even just one implicit layer are often enough, so
while each layer is more expensive to compute, you need fewer of them.
And secondly, it's sometimes possible to ensure that the equation describing
the layer can be solved reasonably easily. After all, we have control over the
class of equations that we need to solve by choosing the network architecture.

So it's not as bad as it could be, but still, what to we gain? One very general
answer is that implicit layers can be very expressive. Even using a very simple
function \(g\), for example, the implicit function \(x \mapsto y\) defined by
solving the equation may be quite complex (this ties into the
fact that one or a few implicit layers are often enough to do the job).
On an even more abstract level, implicit layers decouple /what/ properties
we want the output to have from /how/ to compute the output. The learned parameters
only need to describe some conditions that the output should satisfy and we can then
use any method we want to actually find such an output.

One detail I've quietly swept under the rug is whether a solution \(y\) even
exists and whether it is unique. And for the most part, I'm going
to continue ignoring this issue because this is meant to be a relatively
informal introduction. I'll just mention that in some cases, you actually
get existence and uniqueness guarantees, and in others, you can still hope
that it works out empirically.

** But what about my gradients?
So you've specified a model architecture (the function \(g(x, y; \theta)\))
and you have some method for doing a forward pass (i.e. finding a \(y\) such
that \(g(x, y; \theta) = 0\) for a given input \(x\)). Now you want to train
your model with gradient descent. But you don't have any explicit function
to take the gradient of, so how does that work?

You could backpropagate through your solver: after all, you computed the output
\(y\) /somehow/, in principle you could backpropagate through that calculation.
But that's inefficient, so let's try to find a better way.

To simplify the notation, we'll ignore the parameters \(\theta\). You can think
of them as being a part of the input \(x\) -- for our purposes there's really not
much difference between the input to the layer and the parameters, since we
need gradients with respect to both.

So for a given input \(x\), we now want to find the Jacobian \(\frac{\partial y^*}{\partial x}\),
where \(y^*\) is the output such that \(g(x, y^*) = 0\). We can think of \(y^*\)
as a function of \(x\): for each input \(x\), we have some solution \(y^*(x)\).
To find the Jacobian of \(y^*\), we can use /implicit differentiation/.
We know that \(g(x, y^*(x)) = 0\) for all \(x\), so if we read the LHS as a function
of \(x\), it's just the constant zero function. The derivative of the zero function
is of course also 0, so \(\frac{d}{dx} g(x, y^*(x)) = 0\). On the other hand, we
can apply the chain rule,
\[\frac{d}{dx}g(x, y^*(x)) = \frac{\partial g}{\partial x} + \frac{\partial g}{\partial y}\frac{d y^*}{dx}\thinspace.\]
Since this expression has to be zero, we can rearrange and get
\[\frac{\partial g}{\partial y}\frac{d y^*}{dx} = -\frac{\partial g}{\partial x}\thinspace,\]
which we can further rewrite as[fn::This is assuming that the Jacobian \(\frac{\partial g}{\partial y}\)
is invertible. This is exactly the condition under which the [[https://en.wikipedia.org/wiki/Implicit_function_theorem][implicit function theorem]] holds.
In that case, \(y^*\) is indeed differentiable (so we can apply the chain rule as we did).
And if we drop the assumption that \(g(x, y) = 0\) is uniquely solvable, then the implicit function
theorem at least guarantees that a unique solution \(y*(x)\) exists locally around a point
\((x_0, y_0)\). So this theorem is sort of a theoretical backbone, which guarantees that
what we do actually works. But if we assume that a solution function \(y^*\) exists and is
differentiable, then computing it's Jacobian doesn't require any heavy machinery: as you can
see, we just apply the chain rule once and then rearrange a bit.]
\[\frac{d y^*}{dx} = -\left(\frac{\partial g}{\partial y}\right)^{-1}\frac{\partial g}{\partial x}\thinspace.\]
So now we have the Jacobian of \(y^*\) in terms of the Jacobian of \(g\), which
means we can calculate gradients without backpropagating through the solver.

** [[https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/][Matrix inversion considered harmful]]
There's one remaining issue: while we now know how to calculate the Jacobian \(\frac{d y^*}{dx}\),
doing so requires us to invert a matrix, which is expensive. Luckily, we don't actually
need to explicitly compute the Jacobian for gradient descent. What we ultimately care about
is the gradient of the loss, which is a scalar function. For example, if the implicit layer
described by \(g\) is the last one before the loss function \(L\), we want
\[\frac{dL}{dx} = \frac{dL}{dy}\frac{dy^*}{dx}\thinspace,\]
where \(\frac{dL}{dy}\) is the gradient of \(L\) as a row vector. More generally, we only
need to be able to compute products of the form
\[w^T \frac{dy^*}{dx} = -w^T\left(\frac{\partial g}{\partial y}\right)^{-1}\frac{\partial g}{\partial x}\thinspace,\]
not the Jacobian itself. We can do this by first solving
\[u^T\frac{\partial g}{\partial y} = -w^T\]
for \(u\), the so called /adjoint variable/. Crucially, this is possible without
explicitly computing and storing the inverse (for example using [[https://en.wikipedia.org/wiki/Iterative_method#Linear_systems][iterative methods]]). Then we can calculate
\[w^T \frac{dy^*}{dx} = u^T\frac{\partial g}{\partial x}\thinspace.\]
** What's next?
We've seen some of the basic ideas and themes surrounding implicit layers.
Instead of explicitly describing how to compute the output, they specify
a condition that the output should satisfy. Using implicit differentiation,
we can still effectively backpropagate through these layers, independent
of the solver we use in the forward pass.

But we haven't talked at all about concrete instantiations of implicit layers.
What would a network using these actually look like? And can we use contraints
other than those of the form \(g(x, y; \theta) = 0\)? All of that and more
is discussed in the [[http://implicit-layers-tutorial.org/][implicit layers tutorial]] that I already mentioned above.
It starts with Deep equilbrium models (which essentially use layers defined
by \(g(x, y) = 0\) as in this post, just framed differently) but then applies
the same ideas to constraints described by ODEs, leading to Neural ODEs.
* DONE Perspectives on spherical harmonics :Math:
CLOSED: [2021-03-10 Wed 17:07]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: spherical-harmonics
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Spherical harmonics are ubiquitous in math and physics, in part because
they naturally appear as solutions to several problems; in particular they
are the eigenfunctions of the spherical Laplacian and the irreducible
representations of SO(3). But why should the solutions to these problems
be the same? And why are they called spherical harmonics?
#+end_description
[[https://en.wikipedia.org/wiki/Spherical_harmonics][Spherical harmonics]] appear in lots of different places and have different
interpretations that at first sight don't seem to have anything to do
with one another. In this post, I'll try to connect three very common ones
(namely as harmonic polynomials, as eigenfunctions of the Laplacian
and as irreps of \(\operatorname{SO}(3)\)).

We're going to define spherical harmonics as homogeneous harmonic polynomials
\(\mathbb{R}^3 \to \mathbb{C}\). Let's break this down:
- A polynomial of three variables is a finite sum of the form
  \[\sum_{\alpha} a_\alpha x^{\alpha_x}y^{\alpha_y}z^{\alpha_z}\]
  over multi-indices \(\alpha \in \mathbb{N}_0^3\). Some examples are
  \(x^2y + 2z\) or \(xyz + y^2\).
- The coefficients \(a_\alpha\) can be complex numbers, but we will only plug
  in real numbers for \(x\), \(y\) and \(z\). That's why we interpret polynomials
  as functions \(\mathbb{R}^3 \to \mathbb{C}\).
- /Homogeneous/ mean that \(\alpha_x + \alpha_y + \alpha_z\) is the same for
  all the \(\alpha\) we sum over, so all the terms in the sum have the same
  degree. For example, \(x^2 + 2yz + xz\) is homogeneous, while \(xy + z\) is not.
- /Harmonic/ means that the Laplacian of the polynomial vanishes: \(p\) is harmonic if
  \(\Delta p = 0\). Here, \(\Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} +
    \frac{\partial^2}{\partial z^2}\).
We will write \(\mathcal{H}_l\) for the space of all homogeneous harmonic polymonials
of degree \(l\) (meaning \(\alpha_x + \alpha_y + \alpha_z = l\) for all summands).

If you've seen spherical harmonics before (and you presumably have, if you're
reading this post), it's probably been in the form of functions \(Y_l^m(\theta,
\varphi)\) defined on the sphere. So why are we talking about these polynomials
on \(\mathbb{R}^3\) instead?

The answer is that every polynomial \(p \in \mathcal{H}_l\) can be written
in spherical coordinates as
\[p(x, y, z) = r^l Y(\theta, \varphi)\]
for some function \(Y: S^2 \to \mathbb{C}\). To see why, write \(x\), \(y\) and \(z\)
in spherical coordinates and plug them into the polynomial. They each have
a factor of \(r\) and then some factors depending on \(\theta\) and \(\varphi\).
So because \(p\) is homogeneous, each summand consists of a factor \(r^l\) times
something that depends only on \(\theta\) and \(\varphi\). So we can think of
homogeneous polynomials as polynomials defined on the sphere -- their continuation
to \(\mathbb{R}^3\) is automatically determined by their degree \(l\). Therefore,
we won't really distinguish between homogeneous harmonic polynomials defined
on \(\mathbb{R}^3\) and their restrictions to \(S^2\), we will refer to both as
spherical harmonics.

This should also explain the name: spherical harmonics are /harmonic/ polynomials
living on the /sphere/.

The functions \(Y_l^m\) that you may have seen are just a particular choice of
basis for the vector space of spherical harmonics. If you multiply them by
\(r^l\), you get polynomials in \(\mathcal{H}_l\), and
\[\{r^l Y_l^m| -l \leq m \leq l\}\]
is a basis for \(\mathcal{H}_l\).

** Eigenfunctions of the Laplacian
One of the reasons that spherical harmonics are so ubiquitous is that they
are the eigenfunctions of the spherical Laplacian \(\Delta_{S^2}\). They key
to that is the following fact (which is just a brief calculation):
for a function \(Y: S^2 \to \mathbb{C}\),
\[\Delta (r^l Y) = r^{l - 2}\left(l(l + 1)Y + \Delta_{S^2}Y\right)\thinspace.\]
So \(r^l Y(\theta, \varphi)\) is harmonic if and only if
\[\Delta_{S^2}Y = -l(l + 1)Y\thinspace.\]
This already proves that spherical harmonics are eigenfunctions of the spherical
Laplacian.

But we can say more than that: if we take any eigenfunction \(f: S^2 \to
\mathbb{C}\) of the spherical Laplacian and multiply by \(r^l\) (with \(l\) such
that \(-l(l + 1)\) gives the eigenvalue[fn::I'm skipping over some details here,
see for example Claim 4.0.1 [[http://www-users.math.umn.edu/~garrett/m/mfms/notes_c/spheres_I.pdf][here]]]), then \(r^l f(\theta, \varphi)\) must be
harmonic. So the eigenfunctions of the spherical Laplacian are in fact in 1-to-1
correspondence with harmonic homogeneous functions on \(\mathbb{R}^3\). It then turns
out -- and this part is far from obvious -- that all such functions are
polynomials[fn::See Corollary 4.0.6 [[http://www-users.math.umn.edu/~garrett/m/mfms/notes_c/spheres_I.pdf][in the same document]] for a proof]! So the
spherical harmonics aren't just eigenfunctions of the spherical Laplacian, they
make up /all/ of its eigenfunctions.

** Irreducible representations of \(\operatorname{SO}(3)\)
Another famous role that spherical harmonics play is as the irreducible
representations of \(\operatorname{SO}(3)\) (more precisely: the (complex)
irreducible representations of \(\operatorname{SO}(3)\) are exactly the spaces
\(\mathcal{H}_l\)). This is connected to the fact that they are the
eigenfunctions of the spherical Laplacian.

That the eigenspaces of the spherical Laplacian are representations of \(\operatorname{SO}(3)\)
follows directly from the fact that the Laplacian commutes with rotations: we have
a representation \(G \curvearrowright L^2(S^2, \mathbb{C})\) via
\[(r \cdot f)(x) := f(r^{-1}x)\]
for any rotation \(r\) and \(f \in L^2(S^2, \mathbb{C})\). For an eigenfunction
of the Laplacian, we get
\[\Delta_{S^2}(r \cdot f) = r \cdot \Delta_{S^2} f = r \cdot \lambda f = \lambda (r \cdot f)\thinspace,\]
so each eigenspace is invariant under the action of \(\operatorname{SO}(3)\).
Therefore, the representation on \(L^2(S^2)\) can be restricted to each eigenspace,
so each \(\mathcal{H}_l\) gives a representation of \(\operatorname{SO}(3)\).

Showing that these representations are in fact irreducible is much more
difficult (there's a proof [[https://www.cis.upenn.edu/~cis610/sharmonics.pdf][here]] for example, if you really want to dive into
that). But if we just take that for granted, it's again easy to show that
/every/ irreducible subrepresentation of \(L^2(S^2)\) is a space of spherical
harmonics: because the Laplacian
is an equivariant map on each such representation, Schur's Lemma implies that
it must be either the zero map (which it isn't) or multipication by a constant
\(\lambda \in \mathbb{C}\). Therefore, each irreducible representation is
contained in an eigenspace of the Laplacian. But these eigenspaces are themselves
irreducible, so the representation in question must already be equal to the
eigenspace.

Finally, it's possible to show that all irreducible representations of \(\operatorname{SO}(3)\)
are subrepresentations of \(L^2(S^2)\). This is again much more difficult and
is also a very special fact about \(\operatorname{SO}(3)\) (for example,
the Laplacian's eigenspaces are still irreducible representations in higher dimensions,
but they are not the only ones anymore). But combining this with our results
from above, the spherical harmonics make up all the irreducible representations
of \(\operatorname{SO}(3)\).
* DONE Emacs as an amazing LaTeX editor :Productivity:
CLOSED: [2021-03-17 Wed 14:14]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: latex-emacs
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Emacs has some really amazing features for writing LaTeX; this post gives
an overview of some of them, either to convince you to give Emacs a try,
or to make you aware that these features exist if you're already using
Emacs but didn't know about them.
#+end_description
The purpose of this post is to point you towards some great features
and packages if you're already using Emacs to edit LaTeX, and to
make you jealous if you're using some other editor[fn::Though mostly the first
part -- I haven't actually tried many others and they may be just
as amazing.].

This isn't a tutorial for Emacs or even a tutorial on how to write LaTeX inside
Emacs. Rather, it's supposed to give an idea of what's possible, either
as inspiration or to convince you to give Emacs a try. One problem is
that setting all of this up can be a huge time sink, so you might want
to use a framework such as [[https://github.com/hlissner/doom-emacs][Doom]], where you just need to enable the LaTeX
module and get almost everything I describe here.

** The basics
Of course you get all the basics you would expect from a LaTeX editor.
Synctex is supported (meaning you can jump from a certain line in the
LaTeX code to the corresponding place inside your PDF viewer and the other
way around), you can compile files from inside emacs, you can jump to
compilation errors if there are any, there is auto-completion and so on.

** Visuals :ATTACH:
:PROPERTIES:
:ID:       f4936af5-3fa9-40d5-8008-73c770693b17
:END:
LaTeX can produce beautiful documents, but the source code isn't very
readable when writing mathematical expressions:
#+begin_src latex
  \alpha \mapsto \int_{\R}e^{-\alpha x^2}\,dx
#+end_src
Emacs and AUCTeX (which is the de-facto standard package for using LaTeX inside
Emacs) have several features that improve this situation:
- =preview-latex= replaces equations (and other parts of the LaTeX document)
  with images by compiling them. This means they look exactly the same inside
  the editor as they will in the compiled document. When the cursor is on
  an equation, this image preview is automatically replaced by the underlying text so you can
  still easily edit equations. However, this method of course has a noticeable
  delay because it requires a call to the LaTeX compiler.
- LaTeX superscripts and subscripts are displayed as super-/subscripts inside the editor.
  This is a purely visual feature, editing them doesn't require "entering" or "exiting"
  the subscript or anything like that.
- =prettify-symbols-mode= allows you to replace any string with any unicode symbol.
   AUCTeX comes with a fairly comprehensive predefined list, which replaces LaTeX commands
   such as greek letters, arrows and others with symbolic representations. But you
   can also add your own. For example, the example above uses =\R=, which my custom style
   file defines as =\mathbb{R}=, and it's possible to add replacement rules for such custom
   commands (as long as there is a fitting Unicode symbol). This makes the line above
   look like this in my editor:
  [[attachment:_20210317_140709screenshot.png]]
   When the cursor moves over one of those Unicode symbols, it is expanded to the underlying
   text. And the nice thing about this is that it's essentially instantaneous because nothing
   needs to be compiled.
- /Folding/ is something similar but more general (though unlike =prettify-symbols-mode= it's specific to LaTeX).
  It doesn't just allow replacing fixed strings but also more complicated
  expressions. By default, this is used for example to display =\label{some_label}= as =[l]=
  (which as always expands when the cursor moves over it). The reasoning here is that
  some elements such as labels are just distractions when reading LaTeX source code.
  But you can also use this to further improve how math is displayed, see
  [[https://tecosaur.github.io/emacs-config/config.html#editor-visuals][this config]] for some ideas (and in general for more ideas on how to
  beautify LaTeX inside Emacs).

** Editing
AUCTeX has a couple of nice features that make typing LaTeX a bit easier.
For example, you can let it automatically insert braces ={}= when typing =_= or =^=
inside a math environment, you can let it insert =\(\)= when typing a dollar sign,
and even =\enquote{}= when typing ="= [fn::Using =TeX-electric-sub-and-superscript=,
=TeX-electric-math=, and =LaTeX-csquotes-open-quote= / =LaTeX-csquotes-close-quote= ].

But things get even better with the [[https://github.com/iyefrat/evil-tex][=evil-tex=]] package. As the name suggests,
this is only relevant if you're using =evil-mode= (vim keybindings inside emacs),
but if so, it's definitely worth trying. Just a few examples of what this allows
you to do:
- Say you've typed
  #+begin_src latex
  \(ax^{2} + b\)
  #+end_src
  and suddenly realize that this is supposed to go into an exponent. With your cursor
  anywhere on this math environment, type =ysim^= ("surround everything inside the math
  environment as an exponent") and you'll get
  #+begin_src latex
  \(^{ax^{2} + b}\)
  #+end_src
  with the cursor at the =^=. Now you just need to enter the base.
- Your equations is now
  #+begin_src latex
  \(e^{ax^{2} + b}\)
  #+end_src
  and you decide that this merits its own displayed rather than
  inline equation. So you type =csmee= ("change the surrounding math environment
  to =equation=") and get
  #+begin_src latex
  \begin{equation}
  e^{ax^{2} + b}
  \end{equation}
  #+end_src
- After a bit more editing, you have (for some reason)
  #+begin_src latex
  \begin{equation}
  \beta(e^{ax^{2} + b} + \frac{1}{x})
  \end{equation}
  #+end_src
  Of course this looks ugly in the compiled document, you need to use =\left(= and =\right)=.
  With =evil-tex=, you can just type =mtd= ("toggle delimiter") with the cursor anywhere
  inside the parantheses, and it will add =\left= and =\right= for you. Type =mtd= again to
  go back to just the parantheses.

** Emacs calc's embedded mode
=calc= is the built-in calculator for Emacs; though saying "calculator"
is a bit misleading because it can do symbolic differentiation, unit conversion,
linear algebra and more. If your press =C-x * e= with your cursor on any LaTeX equation,
you will start =calc= in "embedded mode". This means that =calc= will parse the LaTeX
code and then let you do any calculations you want involving the expression.
The result will automatically be converted back to LaTeX and written into the
buffer.

For example, say you have
#+begin_src latex
\[\sin\left( x^2 + \sqrt{x} \right)\]
#+end_src
and want to know the derivative. You can enter embedded mode and type =ad= to differentiate,
then type =x= when prompted for the variable with respect to which to differentiate.
And just like that, you will have
#+begin_src latex
\[\left( 2 x + \frac{0.5}{\sqrt{x}} \right) \cos\left( x^2 + \sqrt{x} \right)\]
#+end_src
inside your buffer. =calc= can even parse and output things like =\begin{pmatrix}...\end{pmatrix}=,
so you can multiply matrices as well.

** And more
I've only covered some of my personal favorite features when it comes to
writing LaTeX inside Emacs, there's much more. For example, =LaTeX-math-mode= allows
you to very quickly enter mathematical symbols and RefTeX as well as other packages
make handling references, labels and citations very efficient.
And of course there are a gazillion other packages that can make writing LaTeX
easier -- this is Emacs after all.

The downside is of course that there is a pretty steep learning curve.
But for people who need to write LaTeX documents all the time, I'd argue
it's worth it.

* DONE State formally, reason informally :Math:
CLOSED: [2021-03-24 Wed 11:00]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: state-formally-reason-informally
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
There's a style of teaching mathematics that I really like: stating definitions
and theorems as formally as in any textbook, but focusing on informal arguments
for why they should be true.
#+end_description

Evan Chen's [[https://web.evanchen.cc/napkin.html][Infinitely Large Napkin]] is my go-to resource when I want to learn
about some new area of mathematics (or at least it used to be; I'm increasingly
often running into the issue that it doesn't have a chapter about what I want
to learn -- at barely over 900 pages it's just way too short). I recently asked
myself what it is about the Napkin that I like so much, and this post is part
of my answer.

Note that I'm describing the way /I/ like to learn math, not the objectively
best way. If you happen to like learning that way too, maybe this post can give
you a more explicit idea of what "that way" is. Otherwise, it will at least
give you some insight into the brain of someone who learns differently
than you do, and maybe that will help if you want to teach math to other
people.

One way of teaching mathematical concepts is the approach typically employed
by textbooks: give some definitions, probably a few examples, state a few theorems,
prove them. In some cases, you might also get an intuitive explanation of what
these definitions are all about or why a theorem is interesting or how the proof
roughly works. But often, these explanations are sparse and confined to the beginning
of a section, so as not to dilute the mathematical purity of the remaining text.

Another style of explanation is the one that consists almost entirely of hand-waving. Evan Chen
describes it as follows:
#+begin_quote
Someone tells you about the hairy ball theorem in the form you cant comb the
hair on a spherical cat, then doesnt tell you anything about why it should be true,
what it means to actually comb the hair, or any of the underlying theory, leaving
you with just some vague notion in your head.
#+end_quote

We could think of these two approaches as opposite ends of a "rigor"-spectrum.
Somewhere in the middle, you might have the way that theoretical physics is often taught:
few things are formally defined, but concepts are at least made
explicit enough that you are able to use them in calculations, and equations
are usually derived (though less rigorously than in math).

Where does the Napkin fit in on this axis? Is it more or less rigorous than
theoretical physics courses? I think that's the wrong question to ask because
the one-dimensional model of approaches to teaching math is too simplistic.
The space of all the ways you could explain mathematical concepts is very large
and treating it as one-dimensional isn't such a great approximation (though I think
that the "rigor"-axis might be the best one can do with only one dimension).

So what's a better model? I want to suggest thinking about mathematical explanations
using a two-dimensional approximation. On one axis, we have the rigor used when /stating/
definitions or results, while the other axis is the rigor of the /derivations/ of these results.
Textbooks have both formal statements and formal derivations, casual hand-wavy explanations
have neither. Physics is somewhere in between, though to me the derivations often
seem more rigorous than the statements[fn::This sounded a bit absurd to me when writing it
down -- how can you rigorously derive something you haven't even really stated?
But I think there's some truth to it, and what I mean is roughly this: the calculations done in physics courses are often
essentially the same that you'd do for a formal proof. But the objects used in those
calculations aren't formally defined, it's just taken for granted that everyone has a sense
of what they are and how they behave (or maybe it's explicitly stated how they behave,
but not whether they are uniquely characterized by that behavior).].
Finally, the Napkin takes the opposite approach:
it states things with essentially the same rigor as textbooks but then places emphasis
on very informal derivations or explanations of why these statements are true.

#+caption: Various styles for teaching mathematics. Yes, it would look better as a TikZ
#+caption: figure, but it was either this or nothing.
[[file:./fig/state-formally-reason-informally/rigor.svg]]

I find that this approach -- stating things formally but reasoning about them informally --
works extremely well for me when I first learn about a subject. I'm happy to take someone's
word that a statement is true, at least initially and if the statement seems like it /should/
be true. But if I don't know precisely what the statement is, I find it much harder to get
to grips with the subject.

As a caveat, note that I said "when I first learn about a subject". This approach doesn't teach
how to write formal proofs, and it's by design not as comprehensive as a textbook.
But sometimes an intuitive understanding is enough for my purposes, and even if I know I'll
later want to learn a subject in more depth, I find it helpful to build this intuitive
understanding /before/ going through all the details.

* DONE Collection of quick computer tips :Productivity:
CLOSED: [2021-03-31 Wed 14:49]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: computer-tips
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Many of us spend a lot of time working with our computer, so it's worth
spending some time to make that experience as pleasent and productive
as possible. This is a collection of tips that are relatively quick
to implement and still very valuable in the long run in my opinion.
Mainly geared towards developers and others who work with the shell
a lot.
#+end_description
This is a list of tips for improving your experience working
with your computer. I focus on things that are quick to implement
(say, 5 minutes to half an hour). Spending 5 minutes on something becomes
worthwhile as soon as it saves you [[https://xkcd.com/1205/][5 seconds per month over 5 years]],
and I think all the tips here easily clear that bar.
I'm not mentioning some things that are extremely
valuable but take more time to do, stuff like "learn vim keybindings".
That's not because those aren't important -- they might even be more important
than these quick hacks. But they don't fit well into this format because
I would need to give a lot more context: who they are useful for, why it's
worth investing time in them etc.

I've grouped these tips and tried to sort them by descending usefulness
inside each category, but that's of course quite subjective.
As a final caveat, I don't explain in detail how to set all of this up.
If you've played around with configuration files before, the pointers I give are hopefully
enough. If you haven't, this might not be the easiest place to start.

** General
- Make use of your Caps Lock key :: The Caps Lock key takes up extremely
     valuable keyboard real estate, even though most people never use it.
     I suggest mapping it to Control for non-vim users. If you use vim keybindings,
     you've probably already remapped it to Escape; in that case I would suggest
     using it as an Escape key when pressed and as Control when held down while
     pressing another key. How that works depends on your OS (I'm using [[https://gitlab.com/interception/linux/plugins/caps2esc][caps2esc]] on Archlinux).
- Fuzzy finder for opening files :: Opening files using a file browser or by first opening
     an application and then using the "Open file" dialog is really slow. Instead, you can
     use a launcher  that you can invoke with a keyboard
     shortcut. You then type in part of the path or filename and once you confirm your selection,
     the file is opened. On Linux, you can install for example [[https://albertlauncher.github.io/][Alfred]] or [[https://ulauncher.io/][ULauncher]] (which also have additional
     functionality rather than just opening files). Or you can use [[https://github.com/davatorium/rofi][rofi]], which is extremely
     flexible but will require a bit more setup.
- Adjust your typematic delay and rate :: If you hold a key down, this simulates
     pressing that key a bunch of times at a high frequency. The typematic rate
     is this frequency and the typematic delay is the time delay before this effect kicks
     in. You can adjust these values to your liking, how that works depends on your OS/Desktop environment.
     On Linux with X, you can use =xset r rate <delay in ms> <rate in Hz>= (this is temporary, so put this
     in a script that will be executed on startup).
- Redshift :: Use [[http://jonls.dk/redshift/][redshift]] or [[https://justgetflux.com/][f.lux]] to automatically adjust the color temperature of your screen according
     to the time of day. This will gradually make your screen look warmer during the evening.
- Dotfiles :: Put your dotfiles in a git repository. [[http://dotfiles.github.io/][This page]] contains a few ideas as inspiration on
     how to best set this up. Personally, I use [[https://github.com/anishathalye/dotbot][Dotbot]], which means I can put all my configuration
     files into one directory and they will be symlinked to the right places. You can also do something
     similar for your =/etc= files using [[http://etckeeper.branchable.com/][etckeeper]].
- sshfs :: This lets you mount a remote directory inside your local file system,
     after which you can edit, create, move and delete files there using whichever
     tools you like to use for that on your local machine. It's in the package repositories
     of most Linux distributions and using it is as simple as
     #+begin_src
       $ sshfs [user@]hostname:[directory] mountpoint
     #+end_src
** Shell
- Use vim keybindings everywhere :: There are extensions for most browsers
     that let you browse web pages with vim keybindings.
     You can also use them in =zsh= (add =bindkey -v= to you =~/.zshrc= or use [[https://github.com/jeffreytse/zsh-vi-mode][this extension]]
     for some improved features). In =fish=, you can use =fish_vi_key_bindings= inside your config,
     and for =bash= it's =set -o vi=. You can even enable them for all readline programs,
     such as the Python REPL, by adding
     #+begin_src
     set editing-mode vi
     set keymap vi
     #+end_src
     to your =~/.inputrc=.
- Ctrl-R history search :: Pressing Ctrl-R inside your terminal will let you search
     through your history of commands and paste the one you select to your prompt,
     after which you can edit it.
- Aliases :: Pay attention to commands you're using frequently and create aliases for them.
     Using [[https://linux.byexamples.com/archives/332/what-is-your-10-common-linux-commands/][this command]], you can also show your most common commands, maybe that gives some
     ideas (though what we really care about is closer to "most common long prefixes of commands";
     might be worth it to write a script for that instead).
- fzf :: [[https://github.com/junegunn/fzf][fzf]] is a general purpose fuzzy finder and you can use it for all kinds of
     things. But more importantly, for this list, it comes with three pre-defined
     shell-keybindings: Ctrl-R is replaced with an improved history search, and Alt-C lets
     you fuzzy search the directories on your machine and will =cd= to the chosen one.
     Finally, Ctrl-T lets you search files inside the current directory (recursively)
     and pastes the selected path into the prompt, which is much faster for typing long
     paths than the usual TAB completion.
- Autosuggestions :: Autosuggestions in your shell mean
     that the shell always tries to guess which command you're entering, typically
     based on the TAB completions as well as your history. It shows this guess and
     you can hit a keybinding to complete it. Make sure to remap this
     to something sensible (for example, zsh uses the right arrow key, which is way too far
     away, I use Ctrl-Space insted). =fish= has these built in, and there is an [[https://github.com/zsh-users/zsh-autosuggestions][extension]]
     for =zsh=. As far as I know, =bash= doesn't have autosuggestions at the moment.
- autojump :: [[https://github.com/wting/autojump][autojump]] watches the directories you visit in your shell and maintains
     a database of which ones you use the most. Then you can type =$ j <part of directory path>=
     and =autojump= will try to guess which directory you want to go to and =cd= there.
     A couple of letters from the directory name are usually enough for that.
     This is a good alternative to =fzf='s Alt-C search.
- Syntax highlighting :: This just means different parts of your command are colored
     differently, much more pleasant to work with. Again, =fish= has this built in, for
     =zsh= you can use another [[https://github.com/zsh-users/zsh-syntax-highlighting][extension]], and I don't think there's an easy way to do this
     in bash.
- !! :: =!!= expands to the previous command. So for example, =$ sudo !!= reruns your previous
     command as a super user.
- Output coloring :: Many shell commands can color their output but have this disabled by default. In particular:
  - ~alias ls='ls --color=auto'~ to always have colored =ls= output
  - [[https://github.com/sharkdp/bat][bat]] is a =cat= replacement with syntax highlighting among other things.
    It can also be used to [[https://github.com/sharkdp/bat#man][color man pages]]
  - The [[https://wiki.archlinux.org/index.php/Color_output_in_console][ArchWiki]] contains many more cases (most of them not specific to Arch)
- Color theme :: Most terminal emulators allow choosing your own color theme,
     and there are configurations online for most pairs of terminal emulator and
     popular color theme. So you can for example use the same theme you use inside
     your editor for your terminal as well.

** TODO LaTeX
- Set up your own style file with common macros etc.
- Set up Zotero with auto-export or something similarly good
- Detexify
- Mathpix
- Copy mathjax code? (browser extension)
- If you happen to use Emacs for that, see my other post
- Latexmk or similar (especially -pvc mode)

** TODO Emacs
** Zathura
Zathura is a lightweight PDF viewer with vim keybindings. I'm a big fan; in case
you are as well, here are two tips to make it even better:
- When you're on e.g. page 10 of a PDF but the page number in the footer is 1
  (because of title pages etc.), type =:offset 9= to tell Zathura about this mismatch.
  Whenever you type =<page number> G= later, Zathura will subtract this offset and
  you will be on the page with =<page number>= in the footer. Extremely useful if
  the PDF has no hyperlinked table of contents. Zathura remembers this setting for each file.
- Recoloring allows you to give PDFs a custom foreground and background color. For example,
  you could display PDFs as light text on a dark background. Or use the following in your =zathurarc=,
  which displays PDFs in a Solarized light color scheme:
  #+begin_src
  set recolor
  set recolor-darkcolor "#586e75"
  set recolor-lightcolor "#fdf6e3"
  set recolor-keephue
  #+end_src
  The last line means that the colors of images should be preserved (though they'll be
  less vibrant). This works for /all/ pdfs, even scanned images! Ctrl-R will toggle recolorization
  on and off, in case you want to switch back to the original.
  For the ideal visual experience, you can also set the color of all the GUI elements,
  see https://github.com/lennonwoo/zathura-solarized for a Solarized version.
* DONE Troubles with the Bias-Variance tradeoff :Machine__learning:
CLOSED: [2021-04-07 Wed 18:19]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: bias-variance-tradeoff
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
The bias-variance tradeoff is a key idea in machine learning. But I'll
argue that we know surprisingly little about it: when does it hold?
How does it relate to the Double Descent phenomenon? And what do we
even formally mean when we talk about it?
#+end_description
/(Last updated: 2021-04-22)/

Arguably one of the most important concepts in machine learning, taught in
any introductory course, is under- and overfitting. The story goes like this:
if your model is too simplistic, you won't be able to fit the data
well and get a large error, you /underfit/. On the other hand, if your model is too complex,
it will fit any noise that is present in the data, i.e. you /overfit/.
Such a model won't generalize to the test set, so you also get a large error.
Somewhere in between those two is a sweet spot with minimal test error.

The choice of words, under- and overfitting, already implies that we believe
a tradeoff exists: they are two ends of a scale, and we need to find the point
in the middle where we're neither under- nor overfitting too much.

Under- and overfitting can be formalized using the notions of bias and variance
(there'll be a short recap in the next section). Underfitting means that we have
a high test error because of high bias, while overfitting means that high variance
causes a high error. Phrased in those terms, the tradeoff between under- and overfitting
becomes the /bias-variance tradeoff/: methods with low bias tend to have high variance
and vice versa.

This idea is ubiquitous in machine learning. So when I recently wanted to look
up some details, I expected to find troves of information. Tons of empirical evidence,
a formal definition of what exactly we mean by "tradeoff", and hopefully even
theorems showing that this tradeoff exists. As you can tell from the setup,
that's not what happened. So in this post, I'm going to describe what I found
(and what I did not find). My goal is to clear up some potential misconceptions,
and hopefully convince you that the bias-variance tradeoff is less simple and more
interesting than you thought.

** Primer: Bias and variance
:PROPERTIES:
:ID:       6cd9fca4-ec33-483e-8645-ff732655f4a5
:END:
Under- and overfitting can be explained in terms of bias and variance.
I'm going to discuss everything in a supervised learning setting. So the setup
is the following:
- There is a true (unknown) function \(f(x)\), which we want to approximate
- We have a dataset \(D = \{(x_1, y_1), \ldots, (x_n, y_n)\}\) of datapoints
  that we use to learn an approximation, where \(y_i = f(x_i)\)
- We have some training process that takes in this dataset \(D\) and produces
  a function \(\hat{f}(x; D)\) that approximates \(f(x)\)
- Finally, we imagine there is some distribution over datasets \(D\). This
  distribution is unknown an it's a somewhat elusive concept, but think of it
  like this: we created our dataset with some process, e.g. by taking lots of
  photos and then having people label them. The distribution over datasets
  describes how likely this process is to produce any particular dataset.

Now we can define precisely what we mean by bias and variance:
- The bias is the difference between the expected value of \(\hat{f}(x; D)\)
  and the true value \(f(x)\), i.e.
  \[\mathbb{E}_D[\hat{f}(x; D)] - f(x)\]
- By "variance" we mean the variance of \(\hat{f}(x; D)\) with respect to \(D\), i.e.
  \[\mathbb{E}_D \left(\mathbb{E}_D[\hat{f}(x; D)] - \hat{f}(x; D)\right)^2\]

Ideally, we want both bias and variance to be small. The reason is the /bias-variance decomposition/
of the expected squared error[fn::I'm ignoring label noise here. More generally,
if the labels are sampled from \(y = f(x) + \varepsilon\) with \(\mathbb{E}[\varepsilon] = 0\), there would be an additional
\(\varepsilon^2\)-term, the /irreducible error/.]:
\[\mathbb{E}_D (\hat{f}(x; D) - f(x))^2 = \text{bias}^2 + \text{variance}\]
Assuming we ultimately want to minimize this expected squared error, it's clear
that all else being equal, we prefer methods with low bias and variance.

Ok, now what about the tradeoff between bias and variance? The idea is that methods
with low bias tend to have high variance, and those with low variance
tend to have high bias. So we can't get both low bias and low variance, instead we
need to find a tradeoff between the two, such that the expected squared error is minimized.
This is illustrated by the following figure (from [[http://scott.fortmann-roe.com/docs/BiasVariance.html][this essay]], which I recommend if you want
to gain more intuition about bias, variance, and their tradeoff):
[[file:fig/bias-variance-tradeoff/tradeoff.png]]

If we use an overly simplistic method, we have a high bias because
our model just can't get close to the true function \(f\), no matter what we feed
in as training data. With a more complex model, we can fit the true function better,
but the trained model \(\hat{f}\) also depends a lot more on the training data \(D\),
so the variance increases.

This explanation sounds sort of intuitive, but I find it a bit unsatisfying. Why exactly
do complex models vary more depending on the training data? When does this hold? Can I have
a theorem, please? And what is this "model complexity" anyways?

** A non-answer
Unfortunately, the bias-variance /decomposition/ and the bias-variance /tradeoff/ are often conflated somewhat,
so let's get one thing out of the way: the bias-variance decomposition is not
an explanation for the tradeoff (even though some handy-wavy explanations
suggest this with varying degrees of explicitness). The only way this could
work is if the total error was constant; this would indeed imply a tradeoff between
bias and variance. But it clearly isn't constant (neither in the figure
above, nor in practice). If the total error /was/ constant,
then we wouldn't care about the tradeoff between bias and variance: it wouldn't
matter which model we chose.

Instead, the bias-variance decomposition just motivates why we care about bias
and variance at all. So it explains why the bias-variance tradeoff is important, but it can't
explain why it's a thing in the first place.

** Evidence for a tradeoff
Why do we believe in a tradeoff between bias and variance? Well, I mainly do because
others have told me about it and it seems pretty intuitive. But those aren't very good
reasons, so what to we have in terms of hard evidence?

As an example, let's look at what /Elements of Statistical Learning/ by Hastie et al.
has to say. It considers two cases in Section 7.3: k-NN regression and linear regression.
For k-NN, it gives a theoretical expression for bias and variance, and at least the variance
does indeed increase with increasing model complexity (i.e. decreasing \(k\)).
A caveat is that the formula assumes the input points \(x_i\) are fixed and
only the targets \(y_i\) vary -- a very strong and usually unrealistic assumption.
Similarly, there are theoretical expressions in the case of ridge regression,
and based on those, variance should typically decrease and bias increase with
stronger regularization. In the same section, there are small toy experiments
that demonstrate a tradeoff empirically for k-NN and for linear models.

This seems to be fairly representative of the kind of evidence we currently
have in favor of a tradeoff. If you are curious you should look at Section 3.4 in
[[https://arxiv.org/pdf/1912.08286.pdf][this thesis]], which is more comprehensive than I'm going to be in a blog post.
But in brief, there is empirical evidence for methods such as decision trees,
fitting polynomials, or kernel regression. Then there are some general theoretical
results that are weak evidence for a tradeoff if you squint a bit. And that's pretty
much it.

If you are one of the two or three people working with neural networks,
this doesn't really inspire confidence. After all, the fact that something holds for linear
regression and polynomials isn't very strong evidence that it's also true
for a 25 million parameter ResNet. Which brings us to the question: /does/ the bias-variance
tradeoff exist outside the simple methods named above?

** Double Descent
Over the last few years, the bias-variance tradeoff has ben supplemented by
a more complicated narrative, dubbed "Double descent" by [[https://arxiv.org/abs/1812.11118][Belkin et al.]] in 2018.
The following figure illustrates this concept:
[[file:./fig/bias-variance-tradeoff/double_descent.png]]

On the left, we have the classical U-shaped curve for the squared error already shown above.
But the claim is that if you increase the model complexity past the interpolation
threshold, where the model can perfectly fit the training data, then the error
decreases again.

There is quite a bit of empirical evidence for this double descent curve,
especially in the context of [[https://arxiv.org/abs/1912.02292][neural networks]][fn::Though just to make things
even weirder: this only seems to occur when increasing the /width/ of networks.
Increasing the depth often isn't tested in papers on the topic, and when
it is, there's no double descent curve.]. The reason seems to be
that the variance curve [[https://arxiv.org/abs/2002.11328][is]] [[https://arxiv.org/abs/1810.08591][unimodal]], i.e. it first increases with model
complexity, as we'd classicaly suspect, but then decreases again. Of course
that just passes the buck: why does the variance decrease again after the
interpolation threshold?

I think that this is an extremely interesting question, but in this post
I'm not going to speculate on it. Instead, I'm still interested in the bias-variance
tradeoff in the classical regime: as long as the model complexity is below the interpolation
threshold, does a tradeoff always exist? If so, why? If not, when does it exist?
Understanding this question better should also help understanding the behavior
beyond the interpolation threshold: if we really understood why the tradeoff exists
at all, we would also know how it could potentially be violated.

** General theorems are hard
In an ideal world, we could give a formal statement of the bias-variance tradeoff
and then prove that it occurs under some fairly general circumstances.
Maybe that is indeed possible, but it's easy to rule out the most general kinds
of theorems through a few counterexamples.

The strongest form of bias-variance tradeoff would be the claim
"any decrease in variance leads to an increase in bias" (and vice versa).
This is clearly false. For example, let's say we start with a traing procedure that chooses
\(\hat{f}(x; D) = \operatorname{sha1}(D)\), i.e. which uses a hash of (some serialization of)
the training data for its prediction (there's nothing special about this example,
just think "a really ridiculous training procedure").
This procedure has high variance, but also high bias. Switching
to any reasonable training procedure will decrease both considerably.

But let's consider a more reasonable statement of the bias-variance-tradeoff:
assume we have a class of models, and the training procedure picks the model from that
class that has the lowest training error. We now want to know how the choice of
model class influences bias and variance. Furthermore, we only consider a nested
set of model classes. So we have an ordering of model classes from simple to complex,
where successively more complex classes contain the simpler ones.

Note that this is a rather typical example of what we mean when
we talk about "model complexity" and the bias-variance tradeoff in practical
contexts. For example, a wider neural network can always instantiate any function
that a more narrow one can, by having some weights be zero.

In this setting, the bias-variance tradeoff can be formulated as two separate claims:
the bias decreases with increasing complexity, while the variance increases.
Unfortunately, neither one is true in general.

A case where bias increases is easy to construct: Let's say the inputs \(x\) and
the targets \(y\) are both real numbers, and there is some noise, i.e. \(y = f(x) + \varepsilon\),
with \(\mathbb{E} \varepsilon = 0\). One very simple model class is \(\{f\}\), and
this class of course leads to a bias of zero. Then there is a more complex model
class \(\{f, f + 1\}\), where we have added a model that always predicts one more
than \(f\). Because of the noise, we might get unlucky with the training data and
pick this second model. So this more complex model class has non-zero bias.

A slight modification leads to an example where variance decreases with model
complexity: we take \(\{f - 1, f + 1\}\) as the simple model class and \(\{f - 1, f, f + 1\}\)
as a larger class. Assuming that we have a very large amount of training data,
we will almost always pick \(f\) in the second case, so the variance will be very
low. In contrast, in the first case, we pick each of \(f - 1\) and \(f + 1\) half the
time, so we have a constant variance, no matter how much training data we have.

Note that this decrease in variance
happens in the classical regime, in the sense that in either case, there is
at most one model that fits the data perfectly. So we haven't crossed the
interpolation threshold yet, and still the variance went down when we increased
model complexity.

Obviously, these examples are somewhat silly, and certainly not representative
of real-world scenarios. They do not constitute a good argument that there is no
bias-variance tradeoff in practice, but they do put some limits on what kinds of
bias-variance tradeoffs we can /prove/. So we'll have to put in a bit of work into
finding the right formalization of what we mean by "bias-variance tradeoff", and
in particular when this tradeoff is supposed to hold. As far as I'm aware, such
a formalization does not exist yet.

** Model complexity
Formalizing the bias-variance tradeoff probably also requires formalizing the
notion of "model complexity" to some extent. This is a somewhat elusive
concept, with many different aspects of the training procedure falling under
its umbrella. The prototypical example of model complexity is the size of the
class of models we use. For example, increasing the width of a neural network,
or using a larger basis of features for linear regression, both increase
the size of the model class under consideration.

But the size of the model class is not the entire story. For example,
regularization terms in the loss function don't change the model class but instead affect which model
is selected from that class. The same is true for the number of trainings steps,
or more generally the optimization procedure.

[[https://arxiv.org/abs/1912.02292][Nakkiran et al.]] define the /effective model complexity/
of a training procedure as the maximum number of training points for which the
expected training error remains below some threshold \(\varepsilon\).
That definition captures a lot of my intuition about what we mean by "complexity"
in the context of the bias-variance tradeoff. On the other hand, it seems slightly
ad-hoc and I'm still hoping for an even better notion of model complexity.

One thing that comes to mind when talking about "complexity" is of course Kolmogorov
complexity. But note that
we don't want the (expected) Kolmogorov complexity of the learned model.
For example, a randomly initialized neural network has high Kolmogorov complexity,
but its model complexity should be zero (it's all the way at
"high bias, low variance[fn::To forestall potential confusion: the variance /with respect to the training data/
is low. Of course there is high variance with respect to the random initialization itself,
but that's not what we're interested in.]"). What we might be able
to use instead is the algorithmic mutual information between the learned
model and the dataset. Another approach would be the (information-theoretic)
mutual information of dataset and model with respect to the distribution over datasets.
Both of these would probably be hard to estimate in practice (and I haven't
thought very long about whether they make sense theoretically and intuitively).
But they illustrate the type of more fundamental definition that I'm hoping exists.

Whether a good definition of model complexity would lead to a formalization
of the bias-variance tradeoff is not clear. For example, just
saying "bias/variance decreases/increases with increasing effective model
complexity" doesn't work (the examples from the previous sections are still
counterexamples). But at least I suspect that having the right notion
of model comlexity is /necessary/ for finding a general formal statement
about the bias-variance tradeoff, even if we then need additional conditions,
under which it holds (for example that it is only true in the classical regime,
i.e. when the model isn't overparameterized).

** Conclusion
After putting the bias-variance tradeoff under a lot of scrutiny, I also
want to emphasize the other side of the coin: it's clear that the bias-variance tradeoff
is real, that it appears in many different settings, and that it's extremely important
to keep in mind when doing machine learning.

But I do think that we don't understand it nearly as well as we could, and that
the way the tradeoff is often presented doesn't do a good job of hightlighting
the parts we don't understand. I highly recommend [[https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update][this essay]], which makes a similar
point more explicitly.

* DONE Scripting for personal productivity :Productivity:
CLOSED: [2021-04-14 Wed 19:42]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: automation-productivity
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
If you can program, you can use that to support your habits and automate
some routines. This post gives a few examples.
#+end_description
This is just a short PSA: if you can code, you can write small scripts
to support your habits and productivity routines. I'm not talking about
automating long, tedious tasks. Rather, I mean automating tasks that
take about five or ten seconds but that you do every day, or at least
very often.

The point is not actually that you'll save five seconds. Instead, such scripts
can give you the right nudges at the right time, make your life slightly less
annoying, or automate stuff so you can't accidentially forget doing it.
The philosophy is that [[https://www.lesswrong.com/posts/reitXJgJXFzKpdKyd/beware-trivial-inconveniences][even very small inconveniences matter]], and if you can
spend a few minutes to make every day that follows even a tiny bit more convenient,
it's probably worth doing.

This is best illustrated by some examples, so that's what the rest of this
post consists of. But I'm sure this is only scratching the surface, so take
these as inspiration and not as a comprehensive list of possibilities.

** Checklists
One of my most important routines is a "daily checklist" that I go through
at the end of each day, where I reflect a bit but mainly plan the next day.
I used to just have a list of all the steps in a text file and went through
those, but as the checklist grew, this became ever so slightly annoying,
and I was liable to skip steps sometimes. So I wrote a small script (inside
Emacs), which takes me through the checklist. I press a keyboard shortcut
to start the checklist and I'm prompted with the first item, then I press the
same shortcut again and I'm prompted with the next item and so on. For me,
this alone is already an improvement over a list in a textfile, because
I'm less likely to press the shortcut without actually doing the current item
than I was to just skip to the next one when reading a checklist. But even
better, for many items my script can give me additional nudges to make
the checklist less annoying. For example, I have several items where I look
at my weekly goals, my scheduled TODO items for the next day etc. Instead
of opening the right files by hand, the script does that automatically once
the corresponding prompt comes up. It doesn't sound like much, but it adds
up and makes going through the checklist much less annoying, which means
I'm more likely to do it diligently. Using a script for this checklist
has other advantages, which I'll talk about below.

Of course, this is not specific about a daily checklist you go through
every evening, it applies to any checklist you use regularly and which
has a reasonably large nummber of items.

** Automatically close distracting programs
Cal Newport recommends ending the work day with a [[https://www.calnewport.com/blog/2009/06/08/drastically-reduce-stress-with-a-work-shutdown-ritual/][shutdown ritual]] and
truly relaxing afterwards. This doesn't really work if you still have
programs such as Slack open that distract you with work-related notifications,
so my daily checklist script closes distracting programs automatically
at the end of the checklist.

One thing I haven't yet looked into but that would be nice is to also close
individual browser tabs automatically. For example, you could close
distracting websites whenever you suspend your PC, or
whenever they were idle for a specific time, or when you start
a Pomodoro, etc. A website blocker can of course serve a similar function,
but unless you /always/ block those website, the slight nudge from closing
tabs automatically could be useful on top of a blocker.

** Welcome screen
When I start my computer in the morning, I'm greeted by a fullscreen wallpaper,
with a nice quote and my top priority for the day. For example, it might look
like this[fn::[[https://www.kisscc0.com/photo/public-domain-creative-commons-license-nature-1a92bt/][Wallpaper source]]]:
[[file:fig/automation-productivity/welcome.png]]
The top priority is another thing where my daily checklist script comes
in handy: it prompts me to enter one for the next day and stores it; the welcome
screen can then later read it from disk and display it.

** Use APIs for the apps you use
This is of course very specific to the apps you use, but I'll give an example.
I use [[https://complice.co/][Complice]] to plan my day and [[https://www.beeminder.com/][Beeminder]] for accountability for my goals.
Yet another thing my daily checklist script does is to fetch all goals from
Beeminder for which some progress is due the next day, using the Beeminder API.
It then adds corresponding TODOs to Complice, using the Complice API. So before
I add TODOs manually, the list on Complice is already pre-populated with
what I need to do to stay on track with my Beeminder goals.

Of course this is only possible when the services you want to automate stuff
for expose an API, but it's worth checking whether that's the case. If it is,
it's often surprisingly easy to use, as long as you only want to do a few simple
things. What I just described as an example can be implemented as a bash script
with just a few lines, using =curl= to access the API and [[https://stedolan.github.io/jq/][=jq=]] to parse JSON.
** Final notes
In case you're interested, [[https://github.com/ejnnr/dotfiles/blob/main/bin/welcome_screen.sh][here]] is the script that displays the welcome screen and [[https://github.com/ejnnr/dotfiles/blob/main/bin/complice.sh][here]] is the one for the Complice/Beeminder API.
That repository also contains the other things I mentioned in this post.

I'm interested to apply this idea of automating parts of my routines
even more. If you have ideas in that direction, I would appreaciate [[mailto:erik@ejenner.com][a message]].
Also feel free to shoot me an email if you are trying to set up any of the things
I've described here and want more details on how to do that.
* DONE Distributions Part I: the Delta distribution :Math:
CLOSED: [2021-07-06 Tue 14:15]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: distributions-intro
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
#+begin_description
Did you always want to know kind of object this weird Dirac delta "function"
actually is? Well, it's a Schwartz distribution. If that doesn't help much,
then keep reading.
#+end_description
Schwartz distributions are a generalization of functions from \(\mathbb{R}^n\) to \(\mathbb{R}\):
strictly speaking, they aren't such functions themselves, but you can do a lot of the same
stuff with them that you can do with normal functions, such as taking derivatives, computing convolutions,
and even Fourier transforms (at least in certain cases).
And in some ways, they even make life easier compared to functions. For example, every distribution
is infinitely differentiable! But of course, we do have to give up some things: distributions
can't be evaluated at a single point and it's in general impossible to multiply two distributions.

In this series, we'll try to understand all of these properties of distributions and more.
I will focus on intuition but still [[/post/state-formally-reason-informally][give formal definitions]] of all the concepts we look at.
As a secondary purpose, studying distributions will also be an excellent opportunity to
practice finding good definitions. We will introduce many different operations on distributions
and in each case, we will try to understand how one could come up with the definition
in a natural way.

** Motivation
In electrostatics, /charge densities/ are used to model the amount
of electric charge in different places. Such a charge density is a function
\(\rho: \mathbb{R}^3 \to \mathbb{R}\) that assigns an amount of charge per volume
to every point \(x \in \mathbb{R}^3\). From an experimental standpoint,
these densities are only useful abstractions; what we can measure is at best
the /total charge in some volume/. This charge \(Q\) is given by the integral
of the density over the volume:
\[Q(V) = \int_V \rho(x) dx\]
for any subset \(V \subseteq \mathbb{R}^3\). You can even think of this as
the /definition/ of the density \(\rho\): the only thing we care about is that
when we measure the charge \(Q(V)\) in any volume \(V\), we get \(\int_V \rho(x) dx\).

Now assume we observe the following: \(Q(V) = 1\) for any volume \(V\) that contains
the origin but \(Q(V) = 0\) if \(V\) does not contain the origin. Intuitively, we
conclude that there is a point charge with value 1 in the origin and no charge anywhere else.
But how can we model this using a density \(\rho\)? If \(\rho\) is any (integrable)
function, as we originally assumed, then we must have \(\rho(x) = 0\) for \(x \neq 0\).[fn::Actually, only for almost all \(x\) but that doesn't change anything.]
But in that case, \(\int_V \rho(x) dx = 0\) for all volumes \(V\), which contradicts
our first observation.

For now, let's just "define this problem away": we'll say that \(\rho(x) = \delta(x)\),
where \(\delta(x)\) is an object such that
\[\int_V \delta(x) dx := 1 \text{ if } 0 \in V, \text{ otherwise } 0.\]
The word "object" here is code for "we're pretty confused and don't know what this thing
is but we'd like to have something that behaves this way".

We'll develop a formal definition of \(\delta\) soon. But first, let's extend
the original example a bit: suppose instead of being interested only in the charge inside
some volume, we now introduce a charged test particle and want to know the potential
energy it has due to the charge density \(\rho\). This potential is given by
\[\Phi \propto \int_{\mathbb{R}^3} \frac{1}{|x_0 - x|} \rho(x) dx\]
for a test particle at position \(x_0\). So what is the potential energy if we have the point charge from
before, \(\rho(x) = \delta(x)\)? So far, we have only defined \(\int_V \delta(x) dx\),
and if \(\delta(x)\) appears anywhere else, we don't really know what to
do with it. Remember, \(\int_V \delta(x) dx\) is just a notation we introduced to mean
"1 if \(0 \in V\) and 0 otherwise", it's not actually an integral in any usual sense.

So we will apply a powerful technique -- wishful thinking. We just assume that
\(\delta(x)\) behaves the way we would intuitively like it to, and then worry later
about constructing something that actually does behave that way. Since for \(\rho(x) = \delta(x)\),
there is no charge outside the origin, all parts of the integral above except for \(x = 0\)
ought to vanish. So let's just write
\[\int_{\mathbb{R}^3} \frac{1}{|x_0 - x|}\delta(x) dx = \int_{\{0\}}\frac{1}{|x_0 - x|}\delta(x)dx.\]
Since we're only integrating over \(\{0\}\) now, we can set \(x = 0\) in \(|x_0 - x|\). Then
this part doesn't depend on \(x\) anymore and we get
\[\int_{\{0\}}\frac{1}{|x_0 - x|}\delta(x)dx = \frac{1}{|x_0|}\int_{\{0\}}\delta(x)dx.\]
But we know what to do with that last part, its' 1! So the potential should be \(\Phi \propto \frac{1}{|x_0|}\).

We can apply the same argument more generally to \(\int \varphi(x) \delta(x)dx\) for other functions \(\varphi\).
So let's "wish" that
\[\int_{\mathbb{R}^3} \varphi(x) \delta(x) dx := \varphi(0)\]
hold for all functions \(\varphi\). This contains our original definition of \(\delta(x)\) as a special
case, namely for the indicator function \(\varphi = 1_V\).

** Schwartz distributions
The defining property of \(\delta(x)\) that we would like to have is
\[\int_{\mathbb{R}^3} \delta(x) \varphi(x) dx := \varphi(0)\]
for arbitrary functions \(\varphi\). We have already noted that this cannot
be an actual (Lebesgue) integral, so it makes sense to get rid of that
notation. Instead, we will write
\[\langle \delta, \varphi\rangle := \varphi(0).\]
This hightlights the important part: \(\delta\) lets us take
any function \(\varphi\) and maps it to its value \(\varphi(0)\) at the origin. So \(\delta\)
is a function after all; just not from \(\mathbb{R}^3\) to \(\mathbb{R}\) but from
the space of /functions on/ \(\mathbb{R}^3\) to \(\mathbb{R}\)!

\(\delta\) is one example of /Schwartz distributions/ or /distributions/ for short,
which are all maps from a space of functions to the real numbers. Let's make this
more precise:

*Definition:* Let \(U \subseteq \mathbb{R}^n\) be an open subset. A /test function/ on \(U\)
is a smooth, compactly supported function \(\varphi: U \to \mathbb{R}\) and we write
\(\mathcal{D}(U)\) for the space of all such test functions. A /Schwartz distribution/
on \(U\) is then a /continuous linear function/ \(T: \mathcal{D}(U) \to \mathbb{R}\).
We write \(\mathcal{D}'(U)\) for the space of all such distributions on \(U\).

This definition requires some clarifications. First, Schwartz distributions are not
at all the same thing as probability distributions, and when I say "distribution"
in this series, I will always mean a Schwartz distribution. Second, if we want to
talk about continuity, we of course need to define a topology on the space \(\mathcal{D}(U)\)
of test functions. The topology we use here is called the /canonical LF topology/ but
we won't discuss that any further in this post.

The name /test function/ comes from the fact that these are the functions on which
we can "test", i.e. evaluate distributions. In our first example about the total
charge in some volume, we used indicator functions \(1_V\) as test functions.
The \(\delta\) distribution would in principle work on /any/ space of test functions.
But it turns out that a good choice for the general definition are smooth
compactly supported functions because this makes a lot of the theory very nice.

We will write \(\langle T, \varphi \rangle\) for the distribution \(T\) applied to
the test function \(\varphi\). But why did we write \(\int \delta(x) \varphi(x) dx\) before?
What does all of this have to do with integrals? The reason is the following:
let \(f : U \to \mathbb{R}\) be any locally integrable (read "somewhat reasonable") function. Then the map
\[\varphi \mapsto \int_U f(x) \varphi(x) dx\]
defines a distribution on \(U\), which we denote by \(T_f\). This is the sense in which
distributions are /generalized/ functions; each classical function induces a distribution.
So when we write \(\int \delta(x) \varphi(x) dx\), we are essentially pretending that
the delta distribution is induced by a function \(\delta(x)\). There is no such function,
but the notation is used very often anyway; probably in part for historical reasons
and in part because it turns out to work surprisingly well, as we'll see next.

We will revisit distributions in general in the next post but for now, we focus on the
\(\delta\) distribution again.

** Variations of the \(\delta\) distribution
We now have a formal understanding of terms of the form \(\int \delta(x) \varphi(x)dx\).
But in practice, the \(\delta\) distribution often appears in modified versions,
such as in terms like
\[\int \delta(x - x_0)\varphi(x) dx\]
or
\[\int \delta(ax)\varphi(x)dx.\]
So far, we haven't formally defined these terms. That means it's time to apply
the Power of Wishful Thinking again, in order to find good definitions for them.

It's pretty clear what \(\delta(x - x_0)\) should mean: it's just a shifted version
of \(\delta(x)\), with its "peak" at \(x_0\) instead of \(0\). More explicitly, it
makes sense to demand that
\[\int \delta(x - x_0)\varphi(x) dx = \int \delta(x) \varphi(x + x_0) dx\]
as would be the case if \(\delta\) was a regular function (all integrations are assumed
to be over all of \(\mathbb{R}^n\)). Then we can see that
\[\int \delta(x - x_0)\varphi(x) dx = \varphi(x_0).\]

Let's consider \(\int \delta(ax)\varphi(x)dx\) instead. You might argue as follows:
"\(\delta(ax) = 0\) for \(x \neq 0\), so we only need to consider \(x = 0\). In that case,
\(ax = 0 = x\), so \(\delta(ax)\) should be the same as \(\delta(x)\)".
But this is a misunderstanding caused by the (admittedly very confusing) notation
often used for the \(\delta\) distribution: \(\delta(x)\) doesn't mean that anything
is actually being evaluated at \(x\), it's just a notational convention /that only makes sense inside integrals/.
We don't want to demand that \(\delta(\cdot)\) behaves like functions when we plug in different things because
we never have \(\delta(x)\) appearing on its own anyway.

What we do want is that \(\delta(x)\) behaves like functions /inside an integral/.
In particular, for functions \(f\) and \(\varphi\) and a scalar \(a \neq 0\), we have
\[\int f(ax)\varphi(x)dx = \frac{1}{|a|^n}\int f(x)\varphi\left(\frac{x}{a}\right)dx.\]
So since we want \(\delta(x)\) to behave the way that functions behave inside integrals,
we define
\[\int \delta(ax)\varphi(x)dx := \frac{1}{|a|^n}\int\delta(x)\varphi\left(\frac{x}{a}\right)dx = \frac{1}{|a|^n}\varphi(0).\]

In fact, we can generalize this argument: for any diffeomorphism \(g\) of \(\mathbb{R}^n\), we have
\[\int f(g(x))\varphi(x)dx = \int |\operatorname{det} Dg(x)|^{-1} f(x)\varphi(g^{-1}(x))dx\]
where \(Dg\) is the derivative (Jacobian) of \(g\).
So in analogy, we can define \(\delta(g(x))\) for any diffeomorphism \(g\) by
\[\int \delta(g(x))\varphi(x)dx := \int |\operatorname{det} Dg(x)|^{-1} \delta(x)\varphi(g^{-1}(x))dx
= |\operatorname{det} Dg(0)|^{-1}\varphi(g^{-1}(0)).\]

I want to stress again that none of these arguments are "proofs" or "derivations" -- in the end, we have
to choose how to define all of these terms. But clearly some definitions make more sense than others
and in the examples here there is clearly one "right" way to define what \(\delta(g(x))\) etc. should
mean. This will become even more clear in the next post: we will continue the theme of finding
good definitions via "wishful thinking", only this time for arbitrary distributions and for many
more types of operations.

* DONE Extensions of Karger's algorithm
CLOSED: [2021-09-10 Fri 11:41]
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: karger-extensions
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true)) :reading_time 8 :unlisted true
:END:

#+begin_comment
Note: adjusted reading time manually because Hugo's default includes the spoiler
boxes.

Todos:
- Make image in figure 1 colored (red/blue), reverse segmentation colors
  and remove the F. Maybe even color the nodes in the bottom right image?
#+end_comment

Karger's contraction algorithm is a fast and very famous method for finding global minimum
graph cuts. First published in 1993, it helped start a wave of other randomized
algorithms for graph cut problems. And while many of these are asymptotically
even faster, Karger's algorithm remains important and fascinating in part because of its
extreme simplicity -- it's description wouldn't even require an entire postcard:
#+begin_quote
Contract a randomly chosen edge, i.e. merge the two nodes it connects into one
node. Repeat until only two nodes are left.
#+end_quote
We will discuss Karger's algorithm in a bit more detail shortly. But first, this
simplicity raises a question: can Karger's algorithm be extended to help with
other tasks than the global minimum cut problem it was originally meant to
solve? That is the question we aim to answer in [[/publication/karger][our new paper]] and which this
blog post will focus on.

One extension that immediately suggests itself, and which we will discuss first,
is that to \(s\)-\(t\)-mincuts. If you already know what \(s\)-\(t\)-mincuts are
and why we'd want to find them, you can read on; if you'd rather have a brief refresher,
there's one in the expandable box below.

{{< spoiler text="What are \(s\)-\(t\)-mincuts and why care about them?" >}}

\(s\)-\(t\)-mincuts (and the related max flows) play an enormously important
role in computer science in general. But instead of giving a huge list, we'll
focus on just one application to computer vision: Say you have an image that you
want to segment into fore- and background. You might have a neural network that
can tell you how likely each pixel is to belong to either of these classes. But
in the end, you don't want all these numbers, you want a single "best guess"
segmentation.

You could of course just assign each pixel to the more likely class: make a pixel
part of the foreground whenever the network says it's more likely to be foreground
than background. But this would ignore dependencies between pixels: if there is
a region where the network classifies most pixels as slightly more likely
to be background but there are a few exceptions, then you probably don't want
these exceptions to be part of the foreground. We know that fore- and background
are at least somewhat contiguous and we want to make use of this prior knowledge.

One approach to this is shown in the following figure:

#+caption: Example application of \(s\)-\(t\)-mincuts: we want to segment
#+caption: the top left image into foreground and background and have some degree of
#+caption: belief about the correct choice for each pixel. We combine all these degrees
#+caption: of belief into a graph (bottom left) and then find its \(s\)-\(t\)-mincut
#+caption: (bottom right), which corresponds to a segmentation of the original image
#+caption: (top right).
#+caption: Image from [[https://ieeexplore.ieee.org/document/7803617][Xiao, Pengfeng et al. (2017)]],
#+caption: which in turn was adapted from [[https://link.springer.com/article/10.1007/s11263-006-7934-5][Boykov, Yuri and Funka-Lea, Gareth (2006)]].
#+attr_html: :width 500px
#+attr_latex: :width 500px
[[file:fig/karger-extensions/st_cut.png]]

We turn our image into a graph; each pixel becomes one of the gray nodes in the graph.
Then we add two more nodes, \(s\) and \(t\) (in red and blue). They are "helper nodes"
that represent the foreground and background. We add edges from each pixel node to both
\(s\) and \(t\) and use the probabilities from our neural network to determine
positive weights for these edges (for example, a probability of 0.5 would mean that the
edge connecting that pixel to \(s\) has the same weight as the edge to \(t\)).

We also add edges between neighboring pixel nodes (the yellow edges in the figure).
Their weights describe how similar two pixels are; they could just be calculated
from brightness differences or they could come from an edge detector network.

The goal is now to /cut/ this entire graph into two halves, one containing \(s\)
and the other containing \(t\). And we want to do this in such a way that the total
weight of all edges that are cut is as small as possible -- strongly connected nodes
should remain on the same side of the cut.

For each pixel, we will have to cut either its edge to \(s\) or to \(t\); this is
where the probabilities predicted by the neural network come in. But we will also
have to cut edges between pixels themselves, and this is what biases the segmentation
towards one that doesn't split up homogeneous areas of the image.

{{< /spoiler >}}

** Global mincuts and Karger's algorithm
Karger's algorithm doesn't find \(s\)-\(t\)-mincuts, it finds /global/ minimum cuts.
This means that we want to separate a graph into two parts while minimizing the total
weight of edges that need to be cut -- all /without/ having seeds \(s\) and \(t\)
that need to be separated.

The following figure illustrates how Karger's algorithm works on a simple example graph:
we first selects an edge at random, in this case
the one between \(b\) and \(c\) (in red), and then contract the chosen edge.
We then repeat this process until only two nodes are left (on the very right).
These nodes define a cut of the original graph into the partitions \(\{a\}\) and \(\{b, c, d\}\).
#+NAME: karger_run
#+begin_src dot :cmd dot :file fig/karger-extensions/karger_run.svg :exports results
graph {
node [ shape=circle, width=0.25, fixedsize=true, fontsize=9 ];
subgraph cluster_1 {
    peripheries = 0;
    label="1. Select edge";
    margin = 0;
    a1 [label="a"];
    b1 [label="b"];
    c1 [label="c"];
    d1 [label="d"];
    a1 -- b1;
    b1 -- d1;
    b1 -- c1 [color=red];
    c1 -- d1;
    { rank=same; c1 d1 }
}

subgraph cluster_2 {
    peripheries = 0;
    label="2. Contract";
    margin = 0;
    a2 [label="a"];
    a2 -- b2;
    b2 -- d2 [penwidth=2];
    b2 [label="b, c", width=0.35];
    c2 [style=invis];
    b2 -- c2 [style=invis];
    d2 [label="d"];
    c2 -- d2 [style=invis];
    { rank=same; c2 d2 }
}

subgraph cluster_3 {
    peripheries = 0;
    label="3. Select edge";
    margin = 0;
    a3 [label="a"];
    a3 -- b3;
    b3 -- d3 [penwidth=2, color=red];
    b3 [label="b, c", width=0.35];
    c3 [style=invis];
    b3 -- c3 [style=invis];
    d3 [label="d"];
    c3 -- d3 [style=invis];
    { rank=same; c3 d3 }
}

subgraph cluster_4 {
    peripheries = 0;
    label="4. Contract";
    margin = 0;
    a4 [label="a"];
    a4 -- b4;
    b4 [label="b, c, d", width=0.4];
}
}
#+end_src

#+NAME: karger_run
#+attr_html: :width 700px
#+attr_latex: :width 700px
#+RESULTS: karger_run
[[file:fig/karger-extensions/karger_run.svg]]

This works for unweighted graphs and for graphs with positive edge weights (in which case
edges are selected for contraction with probability proportional to their weight).

Since Karger's algorithm uses /random/ contractions, it could of course produce a
different result on the same input graph, and in particular a non-minimal cut.
But importantly, Karger's algorithm produces a minimum cut with a reasonably
high probability: by running it a polynomial number of times, it becomes very
likely that at least one run will find a minimum cut. This is far from obvious:
there can be an /exponential/ number of cuts of a graph, so Karger's algorithm
needs a very strong bias towards minimum cuts to achieve its high success
probability.

** Karger's algorithm for \(s\)-\(t\)-mincuts?
We can now come to our first question: can methods similar to Karger's algorithm
be applied to the \(s\)-\(t\)-mincut problem, rather than the global one?

It's certainly easy to modify Karger's algorithm such that it always separates
\(s\) and \(t\): we simply never contract an edge that connects \(s\) and \(t\)
(or nodes that \(s\) or \(t\) have been merged into). This will make sure that
\(s\) and \(t\) are on opposite sides of the final remaining edge that defines
the cut.

But unfortunately, we show that a wide class of extensions of Karger's algorithm
/cannot/ find \(s\)-\(t\)-mincuts efficiently -- no matter how clever we are in modifying
Karger's algorithm, there will always be some graphs where the success probability
is exponentially low. Expand the box below if you want a few more details and intuition,
or see the paper itself for all the gory details.


{{< spoiler text="Karger's algorithm can't be extended to s-t-mincuts" >}}
Let's start by understanding why it's not enough to just slightly modify
Karger's algorithm such that it never merges \(s\) and \(t\). One kind of
example where this simple algorithm fails is the following:

#+NAME: simple_counterexample
#+begin_src dot :file fig/karger-extensions/simple_counterexample.svg :exports results
graph {
    margin = 0;
    node [ shape=circle, width=0.2, fixedsize=true, label="", fontsize=12 ];
    edge [ fontsize=8 ];
    rankdir=LR;
    s [ width=0.35, label="s" ];
    t [ width=0.35, label="t" ];
    s -- a [ penwidth=2, label=2];
    a -- t [label=1];
    s -- b [ penwidth=2 ];
    b -- t;
    s -- c [ penwidth=2 ];
    c -- t;
    s -- d [ penwidth=2 ];
    d -- t;
    s -- e [ penwidth=2 ];
    e -- t;
    { rank=same; a b c d e }
}
#+end_src

#+NAME: simple_counterexample
#+attr_html: :width 400px
#+attr_latex: :width 400px
#+RESULTS: simple_counterexample
[[file:fig/karger-extensions/simple_counterexample.svg]]

The \(s\)-\(t\)-mincut of this graph cuts all of the lighter edges (connected to
\(t\)). For each of the parallel paths from \(s\) to \(t\), the Karger-based
algorithm would choose either the heavy or the light edge for contraction at
some point. It (correctly) chooses the heavy edge with probability \(2/3\),
which sounds good at first. But its choices across all the paths are independent
and so the probability that it gets all of them right and thus finds the
\(s\)-\(t\)-mincut is only \((2/3)^n\), where \(n\) is the number of nodes
besides \(s\) and \(t\). So the success probability of a single run is
exponentially low and therefore, an exponential number of runs is needed to make
success likely.

Ok, so perhaps we just need to be a bit more clever? For example, in the graph
above, a greedy algorithm that always contracts the heaviest edge would work.
It's easy to construct counterexamples to that as well, but is there maybe /some/
extension of Karger's algorithm that does work for \(s\)-\(t\)-mincuts?

To make this question tractable, we need to be more precise about what
"extensions of Karger's algorithm" are. What we would like to keep is the basic
structure of randomly selecting edges and then contracting them. That leaves one
thing we can vary: how to calculate the probabilities with which each edge is
chosen for contraction. We call algorithms with the basic structure of Karger's
algorithm but with arbitrary contraction probabilities /(generalized) contraction
algorithms/.

So can generalized contraction algorithms solve the \(s\)-\(t\)-mincut problem?
The answer is in fact yes! But don't get excited because it's very impractical:
we can compute an \(s\)-\(t\)-mincut using some other algorithm and then set the
contraction probability of all edges that are part of this mincut to 0. This
way, we will always find an \(s\)-\(t\)-mincut. But of course this only shows
that the framework of generalized contraction algorithms is too broad for our
purposes, because we certainly don't want to include silly choices such as this.

Let's therefore consider two natural subclasses of contraction algorithms:
- /local/ contraction algorithms compute contraction probabilities based only on a
  neighborhood of each edge and on global properties of the entire graph. Think
  of it like this: to assign a probability to an edge \(e\), you may look at a
  small neighborhood of \(e\) and at the entire graph, but you won't be told
  which of the edges in this graph is \(e\).
- /continuous/ contraction algorithms can make use of global properties of edges,
  but the probabilities they compute have to be a continuous function of the
  edge weights of the graph. So if the weights are perturbed only slightly, then
  all of the contraction probabilities may only change slightly as well.
Karger's algorithm and its simple extension to \(s\)-\(t\)-cuts we described
above are both local and continuous, as are many other reasonable extensions.

For these large classes of contraction algorithms, we can answer the question
from above: they /cannot/ solve the \(s\)-\(t\)-mincut problem efficiently in
general -- we can find graphs on which their success probability is
exponentially low, meaning they would need to be run an exponential number of
times.

For the full proofs (and the formal definitions of local and continuous
contraction algorithms), see our paper. If you are only interested in the proof
ideas, you can also expand the box below (yes, a box within a box!).

{{< spoiler text="Proof ideas" >}}
For the impossibility result for local contraction algorithms, the idea is
similar to what we already used to show that the simplest extension of Karger's
algorithm has an exponentially low success probability: we have a graph with
many parallel paths between \(s\) and \(t\), all of which have to be
independently contracted. The tricky part is designing these in such a way that
no local algorithm can tell which edges to contract and which edges not to
contract. We can't simply have linear paths, instead we need to have broader
"bands". Each of these bands consists of somewhere between 9 and 18 nodes
(rather than just one node as in the graph above), but other than that, the idea
is the same.

The version for continuous contraction algorithms uses a very different but
simple idea: start with a graph that has an exponential number of
\(s\)-\(t\)-mincuts. Then there must be at least one cut that the algorithm
assigns exponentially low probability to. Now slightly perturb the graph to make
this cut the unique \(s\)-\(t\)-mincut. The probability of finding it will then
remain extremely low because of the continuity condition.
{{< /spoiler >}}

By the way, these results also hold for /normalized cuts/ (another important type
of graph cut problem). We've chosen to focus on \(s\)-\(t\)-mincuts in this post
purely because they are a bit simpler to define and more well-known, but the
impossibility results hold for both cut problems (and even with similar proofs).

{{< /spoiler >}}

** Karger's algorithm for seeded segmentation
So far, extending Karger's algorithm doesn't look promising. But it turns out
that there /are/ useful extension of Karger's algorithm -- we just have to switch
problems. Instead of a minimum cut, machine learning often requires a "good"
segmentation in a vaguer sense -- something that leads to good downstream
performance. So that's what we'll tackle next.

To keep things simple in this blog post, let's assume that our segmentation problem
has only two classes, fore- and background, and that there is only one seed in each
class.[fn::Generalizing to many classes and many seeds per class is straightforward, see our paper.]
We'll continue to call these seeds \(s\) and \(t\) and still want to separate them,
we just don't want a minimum cut any longer.

We could find a segmentation of the graph by just running Karger's algorithm once
and taking the cut we get that way. But that's a bad idea: it's extremely
noisy and the individual cuts that Karger's algorithm produces are often not good
segmentations.

So instead, we sample a lot of cuts and then take their "mean".
Mre precisely: we generate an ensemble of cuts using the
variation of Karger's algorithms that never merges the two seeds. Then for each
node \(v\), we count how often it ended up on the same side of the cut as \(s\)
and how often on the \(t\) side. We can define
the probability \(p_{\text{karger}}(v)\) that Karger's algorithm puts \(v\) on
the side of \(s\); our finite ensemble thus gives us an estimate of this
probability.

This is illustrated in the following figure: displayed are six cuts that
Karger's algorithm might produce for different runs on the same graph. \(v\) is
on the red side of the cut, the one belonging to \(s\), five out of six times,
so we would estimate \(p_{\text{karger}}(v) \approx 5/6\). In practice, we of
course use more than six runs.

<div class="figure-block">

#+NAME: ensemble_1
#+begin_src dot :cmd dot :file fig/karger-extensions/ensemble_1.svg :exports results
graph {
    graph [bgcolor=transparent layout=fdp];
    node [ shape=circle, width=0.25, fixedsize=true, fontsize=9, label="" ];
    margin = 0;
    a [ label="s", color=red, fillcolor="#FF8080", style=filled ];
    g [ label="t", color=blue, fillcolor=lightblue, style=filled ];
    a -- b [color="#C0C0C0"];
    a -- c [color=red];
    c [style=filled fillcolor=lightgray label="v"];
    c -- d [color=red];
    a -- f [color=red];
    a -- d [color=red];
    b -- d [color="#C0C0C0"];
    b -- e [color=blue];
    c -- e [color="#C0C0C0"];
    c -- f [color=red];
    b -- g [color=blue];
    e -- g [color=blue];
    b [color=blue];
    c [color=red];
    d [color=red];
    e [color=blue];
    f [color=red];
}
#+end_src

#+RESULTS: ensemble_1
[[file:fig/karger-extensions/ensemble_1.svg]]

#+NAME: ensemble_2
#+begin_src dot :cmd dot :file fig/karger-extensions/ensemble_2.svg :exports results
graph {
    graph [bgcolor=transparent layout=fdp];
    node [ shape=circle, width=0.25, fixedsize=true, fontsize=9, label="" ];
    margin = 0;
    a [ label="s", color=red, fillcolor="#FF8080", style=filled ];
    g [ label="t", color=blue, fillcolor=lightblue, style=filled ];
    a -- b [color=red];
    a -- c [color="#C0C0C0"];
    c [style=filled fillcolor=lightgray label="v"];
    c -- d [color="#C0C0C0"];
    a -- f [color=red];
    a -- d [color=red];
    b -- d [color=red];
    b -- e [color="#C0C0C0"];
    c -- e [color=blue];
    c -- f [color="#C0C0C0"];
    b -- g [color="#C0C0C0"];
    e -- g [color=blue];
    b [color=red];
    c [color=blue];
    d [color=red];
    e [color=blue];
    f [color=red];
}
#+end_src

#+RESULTS: ensemble_2
[[file:fig/karger-extensions/ensemble_2.svg]]

#+NAME: ensemble_3
#+begin_src dot :cmd dot :file fig/karger-extensions/ensemble_3.svg :exports results
graph {
    graph [bgcolor=transparent layout=fdp];
    node [ shape=circle, width=0.25, fixedsize=true, fontsize=9, label="" ];
    margin = 0;
    a [ label="s", color=red, fillcolor="#FF8080", style=filled ];
    g [ label="t", color=blue, fillcolor=lightblue, style=filled ];
    a -- b [color="#C0C0C0"];
    a -- c [color=red];
    c [style=filled fillcolor=lightgray label="v"];
    c -- d [color="#C0C0C0"];
    a -- f [color=red];
    a -- d [color="#C0C0C0"];
    b -- d [color=blue];
    b -- e [color=blue];
    c -- e [color="#C0C0C0"];
    c -- f [color=red];
    b -- g [color=blue];
    e -- g [color=blue];
    b [color=blue];
    c [color=red];
    d [color=blue];
    e [color=blue];
    f [color=red];
}
#+end_src

#+RESULTS: ensemble_3
[[file:fig/karger-extensions/ensemble_3.svg]]

#+NAME: ensemble_4
#+begin_src dot :cmd dot :file fig/karger-extensions/ensemble_4.svg :exports results
graph {
    graph [bgcolor=transparent layout=fdp];
    node [ shape=circle, width=0.25, fixedsize=true, fontsize=9, label="" ];
    margin = 0;
    a [ label="s", color=red, fillcolor="#FF8080", style=filled ];
    g [ label="t", color=blue, fillcolor=lightblue, style=filled ];
    a -- b [color=red];
    a -- c [color=red];
    c [style=filled fillcolor=lightgray label="v"];
    c -- d [color=red];
    a -- f [color=red];
    a -- d [color=red];
    b -- d [color=red];
    b -- e [color=red];
    c -- e [color=red];
    c -- f [color=red];
    b -- g [color="#C0C0C0"];
    e -- g [color="#C0C0C0"];
    b [color=red];
    c [color=red];
    d [color=red];
    e [color=red];
    f [color=red];
}
#+end_src

#+RESULTS: ensemble_4
[[file:fig/karger-extensions/ensemble_4.svg]]

#+NAME: ensemble_5
#+begin_src dot :cmd dot :file fig/karger-extensions/ensemble_5.svg :exports results
graph {
    graph [bgcolor=transparent layout=fdp];
    node [ shape=circle, width=0.25, fixedsize=true, fontsize=9, label="" ];
    margin = 0;
    a [ label="s", color=red, fillcolor="#FF8080", style=filled ];
    g [ label="t", color=blue, fillcolor=lightblue, style=filled ];
    a -- b [color="#C0C0C0"];
    a -- c [color=red];
    c [style=filled fillcolor=lightgray label="v"];
    c -- d [color=red];
    a -- f [color=red];
    a -- d [color=red];
    b -- d [color="#C0C0C0"];
    b -- e [color=blue];
    c -- e [color="#C0C0C0"];
    c -- f [color=red];
    b -- g [color=blue];
    e -- g [color=blue];
    b [color=blue];
    c [color=red];
    d [color=red];
    e [color=blue];
    f [color=red];
}
#+end_src

#+RESULTS: ensemble_5
[[file:fig/karger-extensions/ensemble_5.svg]]

#+NAME: ensemble_6
#+begin_src dot :cmd dot :file fig/karger-extensions/ensemble_6.svg :exports results
graph {
    graph [bgcolor=transparent layout=fdp];
    node [ shape=circle, width=0.25, fixedsize=true, fontsize=9, label="" ];
    margin = 0;
    a [ label="s", color=red, fillcolor="#FF8080", style=filled ];
    g [ label="t", color=blue, fillcolor=lightblue, style=filled ];
    a -- b [color="#C0C0C0"];
    a -- c [color=red];
    c [style=filled fillcolor=lightgray label="v"];
    c -- d [color="#C0C0C0"];
    a -- f [color=red];
    a -- d [color="#C0C0C0"];
    b -- d [color=blue];
    b -- e [color="#C0C0C0"];
    c -- e [color=red];
    c -- f [color=red];
    b -- g [color=blue];
    e -- g [color="#C0C0C0"];
    b [color=blue];
    c [color=red];
    d [color=blue];
    e [color=red];
    f [color=red];
}
#+end_src

#+RESULTS: ensemble_6
[[file:fig/karger-extensions/ensemble_6.svg]]

</div>

This probability \(p_{karger}(v)\) is a probabilistic segmentation of the input
graph. The following figure illustrates this and compares it to the potential that
the random walker algorithm produces:

#+CAPTION: Karger and random walker potentials in comparison.
[[file:fig/karger-extensions/flower.png]]

\(\beta\) is a parameter that influences how the edge weights are computed based
on the image. For large \(\beta\), the edge weights become very extreme and both
algorithms assign probabilities close to zero or one to each pixel.

If we need a hard cut, we can simply assign each pixel to the side
for which this estimate is higher.
This gives an algorithm for seeded graph segmentation. It's very simple and has a
great asymptotic runtime (linear in the number of edges of the graph).
We also empirically compared this Karger-based segmentation method to the random
walker and other algorithms on image segmentation and semi-supervised
classification tasks. In both cases we found that it performs at least as well
as these classical algorithms, see the paper for detailed results.

** Karger's algorithm and the random walker
We didn't compare the Karger-based segmentation algorithm to the random walker
/just/ because the latter is one of the most influential algorithms for seeded
segmentation of all time: it turns out that these two very different-seeming
algorithms are surprisingly similar from a theoretical perspective.

Explaining these similarities takes some setup, so here's the TL;DR: both algorithms
can be interpreted as forest-sampling methods, just sampling from different
distributions. The Karger distributions contains an additional dependency on the
topology of the graph, which we think makes it more "confident" (i.e. it assigns
probabilities closer to zero and one).

If this summary has whetted your appetite, then we encourage you to expand the box below
for details.

{{< spoiler text="The connection between Karger's algorithm and the random walker" >}}

First, if you're not already familiar with the random walker and spanning forests,
you can find a brief crash course in the following box-within-a-box:

{{< spoiler text="Random walker and spanning forests refresher" >}}
The random walker algorithm solves the same problem as our Karger-based
algorithm: given a graph with some seeds, assign labels to the remaining nodes.
Again, we will focus on the case with only one foreground and one background
seed.

The idea is simple and elegant: to assign a label to some node \(v\), imagine a
random walk starting at \(v\). At each step, we randomly move to one of the
neighbors of the current node, with probability proportional to the weight of
the edge connecting the nodes. The random walk stops once we reach one of the
two seeds. We can define the probability \(p_{\text{rw}}(v)\) that this walk
starting at \(v\) will end at the foreground seed. Just like the Karger-based
algorithm, we thus get a probabilistic assignment for each node, and we can
again get a hard segmentation by assigning each node to the more likely class.

So apart from the fact that they solve the same type of problem, why is the
random walker relevant to us? To answer this question, let's make yet another
excursion, namely to /forests/ and how they relate to segmentation. (I promise
this will all circle back to Karger's algorithm soon). A forest of a graph is a
set of disjoint trees (i.e. acyclic connected graphs) that together span the
entire graph. For example, in the following figure, the red edges define a
2-forest of the graph, i.e. a forest consisting of two trees:

#+NAME: forest
#+begin_src dot :file fig/karger-extensions/forest.svg :cmd neato :exports results
graph {
    margin=0;
    ratio=1;
    node [ shape=circle, label="" ];
    edge [ color=gray ];
    a;
    g;
    a -- b;
    a -- c [color=red, penwidth=2];
    c -- d;
    a -- f [color=red, penwidth=2];
    a -- d [color=red, penwidth=2];
    b -- d;
    b -- e [color=red, penwidth=2];
    c -- e;
    c -- f;
    b -- g;
    e -- g [color=red, penwidth=2];
}
#+end_src

#+RESULTS: forest
[[file:./fig/karger-extensions/forest.svg]]

In a seeded graph, we are particularly interested in forests that /separate the
seeds/, which in our setting just means 2-forests such that \(s\) and \(t\) are
in different trees. In the figure below, the forest on the left separates the
seeds but the forest on the right does not:

#+NAME: seeded_forests
#+begin_src dot :file fig/karger-extensions/seeded_forests.svg :cmd fdp :exports results
graph {
node [ shape=circle, label="" , fixedsize=true, width=0.35 ];
edge [ color=gray ];
subgraph cluster_1 {
    labelloc=b;
    peripheries=0;
    a1 [ label="s" ];
    g1 [ label="t" ];
    a1 -- b1;
    a1 -- c1 [color=red, penwidth=2];
    c1 -- d1;
    a1 -- f1 [color=red, penwidth=2];
    a1 -- d1 [color=red, penwidth=2];
    b1 -- d1;
    b1 -- e1 [color=red, penwidth=2];
    c1 -- e1;
    c1 -- f1;
    b1 -- g1;
    e1 -- g1 [color=red, penwidth=2];
}

subgraph cluster_2 {
    peripheries=0;
    labelloc=b;
    a2 [ label="s" ];
    g2 [ label="t" ];
    a2 -- b2;
    a2 -- c2;
    c2 -- d2;
    a2 -- f2;
    a2 -- d2 [color=red, penwidth=2];
    b2 -- d2 [color=red, penwidth=2];
    b2 -- e2;
    c2 -- e2 [color=red, penwidth=2];
    c2 -- f2 [color=red, penwidth=2];
    b2 -- g2 [color=red, penwidth=2];
    e2 -- g2;
}
}
#+end_src

#+RESULTS: seeded_forests
[[file:fig/karger-extensions/seeded_forests.svg]]

A forest that separates the seeds automatically defines an assigment for each
node -- exactly what we're after! One of the two trees will be the foreground
and one will be the background:

#+NAME: forest_segmentation
#+begin_src dot :file fig/karger-extensions/forest_segmentation.svg :cmd neato :exports results
graph {
    margin=0;
    ratio=1;
    node [ shape=circle, label="" ];
    edge [ color=gray ];
    a [ label="s", color=red, fillcolor="#ffa0a0", style=filled ];
    g [ label="t", color=blue, fillcolor="#a0a0ff", style=filled ];
    a -- b;
    a -- c [color=red, penwidth=2];
    c -- d;
    a -- f [color=red, penwidth=2];
    a -- d [color=red, penwidth=2];
    b -- d;
    b -- e [color=blue, penwidth=2];
    c -- e;
    c -- f;
    b -- g;
    e -- g [color=blue, penwidth=2];
    b [ color=blue ];
    e [ color=blue ];
    c [ color=red ];
    d [ color=red ];
    f [ color=red ];
}
#+end_src

#+RESULTS: forest_segmentation
[[file:fig/karger-extensions/forest_segmentation.svg]]

In this figure, the colored edges are the tree; all nodes are colored according
to the assigment defined by that tree.

Now comes the connection to the random walker: we can define a distribution over
these 2-forests that separate the seeds, where the probability of a forest \(f\)
consisting of edges \(e_1, \ldots, e_k\) is proportional to
\[w(f) := \prod_{i = 1}^k e_i,\]
called the /weight/ of the forest.
Then it turns out -- this is not at all obvious -- that the random walker
probability \(p_{\text{rw}}(v)\) defined previously is exactly the probability
that a forest sampled from this distribution assigns \(v\) to the foreground! So
the random walker could be interpreted completely differently as a forest
sampling method. In principle, we could sample lots of forests from this Gibbs
distribution and then counting how often each node is assigned to each seed.
Now, implementing the random walker like that would be computationally
nonsensical -- but it does sound notably similar to our Karger-based algorithm.
(I promised we'd get back there).

{{< /spoiler >}}

The key insight to see the connection between the random walker and Karger's
algorithm is the following:
#+begin_quote
Karger's algorithm samples spanning forests.
#+end_quote
Essentially, the \(n - 2\) edges that Karger's algorithm contracts during one
run define a spanning 2-forest of the graph. And if we modify Karger's algorithm
such that it never merges \(s\) and \(t\), then this forest will separate \(s\)
and \(t\). Furthermore, the two trees making up the sampled forest are precisely
the two sides of the cut produced by Karger's algorithm.

{{< spoiler text="Details for curious readers" >}}
Some care needs to be taken here: the way we described Karger's algorithm above,
the edges it contracts may be the product of combining several edges of the
original graph (because parallel edges are combined at each step). But as long
as we don't mind dealing with multigraphs, we can formulate a completely
equivalent version of Karger's algorithm that doesn't combine parallel edges.
This variation of Karger's algorithm has exactly the same probability of
producing any given cut as our formulation above. The only difference is that a
run of this algorithm corresponds one-to-one to an ordered sequence of \(n - 2\)
edges /in the original graph/.

Now it's easy to prove that these \(n - 2\) edges always form a spanning
2-forest of the graph. We only need to note that this set of edges cannot
contain any cycles because such cycles would correspond to self-loops in the
contracted graph and the edge that closes the cycle would have been removed
before it could be contracted.
{{< /spoiler >}}

The remaining difference between this Karger-based algorithm and the random
walker is the distribution over forests that they sample from. Recall that for
the random walker, the probability of sampling a forest \(f = (e_1, \ldots,
e_{n - 2})\) is simply proportional to the weight \(w(f)\) of the forest. The
distribution that Karger's algorithm implicitly samples from also contains this
term, but it has an additional dependency on the forest. It assigns a
probability of
\[
p(f) = w(f) \sum_{\sigma \in S_{n - 2}} \prod_{i = 1}^{n - 2}
\frac{1}{c\left(E \setminus \mathcal{C}(\{e_{\sigma(1)}, \ldots, e_{\sigma(i - 1)}\})\right)}
\]
to a forest \(f = (e_1, \ldots, e_{n - 2})\). Understanding this equation is not
important for the remainder of this post; you can find some details below and
the full derivation in our paper.

{{< spoiler text="Details on the Karger distribution" >}}
This equation is best understood starting from the right. \(E\) is simply the
set of edges in the graph. \(\mathcal{C}(\hat{E})\) for a subset \(\hat{E}
\subseteq E\) of edges is the set of all edges that together with \(\hat{E}\)
form cycles or a connection between \(s\) and \(t\), as well as the edges in
\(\hat{E}\) itself. These are precisely the edges that have been removed from
the graph after contracting all the edges in \(\hat{E}\). \(c(\hat{E})\) is the
sum of all the edge weights of edges in \(\hat{E}\). The fraction on the right
side is therefore the normalization constant appearing in the probability of
contracting any given edge. Taking the product over these (together with the
weight \(w(f)\)) gives the probability of a particular run of Karger's
algorithm. A run corresponds to an /ordered/ sequence of edges, whereas the
ordering doesn't matter for the forest defined by the run. So we need to sum
over the set \(S_{n - 2}\) of all possible permutations of \(1, \ldots, n - 2\).
{{< /spoiler >}}

As we argue in the paper, this additional term makes the Karger potential more
"confident" than the random walker potential, in the sense that it assigns more
extreme probabilities to each node. This can also be seen in the plot of the
potentials for an image we had above.

{{< /spoiler >}}

** Conclusion
Karger's algorithm is a powerful and very influential tool for finding global
minimum cuts, so it is natural to ask whether this success can be extended to
other important graph problems. We have proven that extending it to the
\(s\)-\(t\)-mincut problem is impossible in a formal sense. However, we did
adapt Karger's algorithm to the problem of seeded graph segmentation,
demonstrating its usefulness beyond minimum cuts. In particular, the new
algorithm shows that the /distribution/ produced by Karger's algorithm is itself
an interesting subject of study, rather than only the smallest cut found. We
have made some steps towards understanding this distribution by linking it to
the random walker. But there certainly remains a lot of space to improve our
theoretical understanding of the "Karger distribution", as well as the potential
to use it for other purposes than the one we presented.

* TODO Distributions Part II: Derivatives, convolutions, and all the rest
Theme: you can extend all kinds of constructions for functions
to distributions and it's all compatible

Addition, multiplication with functions, convolutions, derivatives, Fourier transform, composition
* TODO Averaging graph cuts
Basically what I did for my Bachelor's thesis but then didn't include
(discuss relation to our Karger paper and to typical cuts).

Text from thesis draft:
** Averaging methods
In cref:random_walker, we saw a very different way of applying contraction
algorithms: instead of choosing the cut with the smallest cost that was found,
we used all the sampled cuts to calculate an "average" cut by assigning each
vertex to the seed it was most likely to be connected to by a random run of
the algorithm.

In this section, we will present a more general framework for averaging graph
cuts. The segmentation method described in cref:random_walker will be a
natural special case of this framework. But the general setting will allow
us to apply similar methods to global cuts sampled by Karger's algorithm.
This is very reminiscent of the /typical cut algorithm/\nbsp{}cite:gdalyahuSelforganizationVisionStochastic2001
and indeed this algorithm fits quite well into our framework.

To define the notion of an average for graph cuts, we will use the
following definition:
#+begin_definition
Let \((X, d)\) be a metric space and \(\{x_1, \ldots, x_N\} \subset X\)
a set of points. If
\begin{equation}\label{eq:frechet_mean}
\bar{x} = \argmin_{x \in X} \sum_{i = 1}^N d(x, x_i)^2
\end{equation}
exists and is unique, it is called the /Frchet mean/ of \(\{x_1, \ldots, x_N\}\).
#+end_definition
Note that the arithmetic mean in euclidean spaces is the Frchet mean with respect
to the euclidean distance.

This definition means that a distance function between graph cuts induces a notion
of a mean[fn::\(\bar{x}\) always exists because the space of cuts is finite.
Whether it is unique depends on the graph and the distance function. In the usually rare cases,
where there are several minima, we can arbitrarily choose one to return or return
several possible cuts.]. We can in principle sample cuts using Karger's algorithm
and then find the mean of the sampled cuts.

For \(s\)-\(t\)-cuts, there is a very natural distance function: represent
each cut by a label vector \(x \in \{0, 1\}^n\) where \(x_i = 0\) if vertex
\(i\) belongs to \(s\) and \(x_i = 1\) otherwise. Then define \(d\) as the
euclidean or Hamming distance on this space of label vectors.

It's easy to see that the Frchet mean with these definitions is precisely
the cut described in cref:random_walker. To be more explicit: calculate the
arithmetic mean of all \(N\) sampled label vectors in the euclidean space
\(\R^n\), then round each component of the mean to 0 or 1.

For averaging global cuts, there are many possible distance functions one
might use. As one example, consider the representation of cuts as "similarity matrices":
each cut is encoded as a matrix \(S \in \{0, 1\}^{n \times n}\) where
\(S_{ij} = 1\) if and only if vertices \(i\) and \(j\) are on the same
side of the cut. We then again use the euclidean or Hamming distance on this space.

Compared to the case of \(s\)-\(t\)-cuts, this setting has a big disadvantage:
not every matrix in \(\{0, 1\}^{n \times n}\) corresponds to a valid cut.
Previously, we solved the minimization problem cref:eq:frechet_mean
by first solving the relaxed problem which allowed label vectors
in \([0, 1]^n\) (calculating the arithmetic mean) and then rounding
to the nearest corner of that hypercube to find the correct mean.
This does not work here because not every corner of the hypercube
is a valid cut.

The typical cut algorithm\nbsp{}cite:gdalyahuSelforganizationVisionStochastic2001
can be interpreted as a way to approximate the "true" mean defined
this way. It solves the relaxed problem first by calculating the arithmetic mean in \([0, 1]^{n \times n}\).
To discretize this mean, it then rounds up every entry above 0.5.
This forces some more entries to be 1 as well and afterwards, the remaining entries
are rounded down to 0.

The problem of discretizing the relaxed solution can also be formulated
as a correlation clustering problem. In this case, the typical cut
algorithm corresponds to not cutting any edges with positive weight
(the edges with negative weights are effectively ignored).
\todo{Elaborate in an appendix or remove this paragraph}

The computational difficulty of finding the Frchet mean occurs
for other distance functions between global cuts as well which might
make this approach less suited for global cuts than for \(s\)-\(t\)-cuts.
Furthermore, the choice of distance function is a bit more arbitrary
than the natural one presented for \(s\)-\(t\)-cuts. Whether this
approach is promising at all depends on whether the Frchet mean
captures the intuitive notion of an "average" of global cuts for
at least some of these distance functions.

* TODO Discounted calculus on graphs
See the Google doc: https://docs.google.com/document/d/1KFM6N5FD96JxKIEyncmujJyguC9o3s3uIsBbVvs0m7A/edit
Interesting part is probably gradient, path integral and fundamental theorem
and how this relates to potential shaping. Doesn't seem like it can be extended
much further sensibly (e.g. divergence and consequently Laplacian just seem a bit
weird). Can mention Bellman equation though.
* TODO Variable horizons considered harmful
Coordinate with Adam on this
* TODO Geometric view on random vs grid search
Idea: we have a hypercube of parameters to search over.
But it's not really a cube; some parameters are more important,
so the sides for those should be longer. Then we want to minimize
the distance between a random point and a tested point. Grid search
isn't good at that for very extreme differences between side lengths.

In the limit: if we set a side length to 0 (parameter doesn't matter
at all), grid search collapses points on top of each other, "wasting"
them. Random search doesn't do this. In general, grid search will have
clusters of points that are very close together (in the space that
has an importance-based metric), whereas random search produces somewhat
more even distributions (I think).
* TODO Hierarchy of computational models
Adding more layers of oracles for the halting problem leads to
an infinite hierarchy of computational models (and I think even one
for every ordinal?)

Discusses this with Christian.

See also
https://en.wikipedia.org/wiki/Arithmetical_hierarchy#Relation_to_Turing_machines
and
https://en.wikipedia.org/wiki/Turing_degree

Philosophical question: most of the problems we care about are relatively
low in this hierarchy I think (perhaps even solvable with just one oracle?).
Is this only because of our universe? Or is there something fundamental about
this level, independent of physics? It is very close to the bottom I think,
in a sense the least powerful model that is still pretty general.
* TODO CHAI repository setup
Docker, Pipfile, etc.
* TODO Translation-equivariant maps
Famously, translation-equivariant maps are exactly convolutions in a discrete setting.
But what about the continuous one? They still are, you just need to use distributions!
But L_2 might not work?
* TODO The mistake in the Adam convergence proof
https://www.youtube.com/watch?v=n65pElMp6x8
https://openreview.net/forum?id=ryQu7f-RZ&noteId=B1PDZUChG

But: AMSGrad might not be better in practice: https://fdlm.github.io/post/amsgrad/
* TODO Implementing sparse convolutions in Pytorch
* TODO Intuition for the deadly triad
Off-policy means that the distribution of visited states is different
than it would be on-policy. This part of the discrepancy can't be corrected
by importance sampling. Doesn't matter in tabular setting because state
distribution doesn't influence the limit we converge to, but it does with
function approximation.

What about bootstrapping?
* TODO Discretizing differential operators
FD, RBF-FD. But should probably also talk about finite elements?
So maybe just rename it to RBF-FD
* TODO Reproducing TF in Pytorch
- pay attention to defaults
- transfer weights, look at layer by layer activations
* TODO Things you should try
Theme: easy and cheap (~ <2h, <50$ to try), reasonable chance
of a huge impact, better lights (?), better mouse + keyboard

Beeminder, Focusmate, Audiobooks, sleeping mask (?),
Queal/...

Also look for similar lists beforehand
* TODO The many shapes of Noether's theorem
There are versions for CM, GR, QM and probably others. What is the
"one true" theorem?
* TODO Topological perspective on Buridan's principle
:PROPERTIES:
:EXPORT_HUGO_BUNDLE: topological-buridan
:EXPORT_FILE_NAME: index
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :image '((preview_only . true))
:END:
Buridan's ass is an old and famous conundrum: image a hungry donkey somewhere between
two equally appealing stacks of hay. Naturally, it will choose to go to the stack
that is closest. But what if it is precisely in the middle between the two stacks?
Since there is no reason to prefer one stack over the other, the argument goes,
the donkey will never move and starve because it can't decide.

The conclusion sounds a bit ridiculous but in 1984, Leslie Lamport showed that
under certain assumptions, this really is what must happen. He states [[https://lamport.azurewebsites.net/pubs/buridan.pdf][Buridan's principle]],
which goes as follows:
#+begin_quote
A discrete decision based upon an input having a continuous range of values
cannot be made within a bounded length of time.
#+end_quote
In the case of Buridan's ass, the donkey must make the discrete decision of which
pile to go to. The possible inputs are the initial positions of the donkey, which form
a continuous range. Buridan's principle states that there is some initial position (or "input")
for which the donkey can't make a decision and starves to death[fn::The quote above is from Lamport's paper
and makes it sound as though a discrete decision might always be possible, just not with a uniform bound
on the decision time over all inputs. I think that a stronger principle holds: there is at least one input
for which /no/ decision is possible, i.e. for which it would take an "infinite" time].

Why should this be true? To model the situation mathematically, write \(x\) for the initial position of the donkey
at time \(t = 0\). Given such an initial position, the position at later times is determined by a time
evolution function \(A_t(x)\), which describes the position of the donkey at time \(t\) if it started at
position \(x\). The stacks of hay are at positions 0 and 1.

There are two important assumptions that Lamport makes to prove that the ass starves for some initial
position:
1. For any fixed time \(t\), the time evolution function \(A_t: \mathbb{R} \to \mathbb{R}\) is a continuous
   function
2. If the donkey starts at one of the stacks, it stays there: \(A_t(0) = 0\) and \(A_t(1) = 1\) for all \(t\)

Then for any time \(t\), there must be some \(x \in (0, 1)\) for which \(A_t(x) \in (0, 1)\) (by the intermediate
value theorem).

Even after hearing that argument, the conclusion might seem very unintuitive. Lamport spends quite
some time defending the argument against various potential objections and arguing that it holds
in the real world, I recommend reading the [[https://lamport.azurewebsites.net/pubs/buridan.pdf][paper]] if you're interested in that.

** Moving around obstacles
Consider a related situation that Lamport discusses: a driver wants to move around some obstacle,
such as a tree. She needs to make the decision of whether to go around the tree to the left or to the right.

Mathematically, we can describe the scenario as follows: the driver moves on the euclidean plane,
and her initial position at time \(t = 0\) is \(x, -1\), where \(x \in \mathbb{R}\) is the input mentioned
in Buridan's principle. Her goal is to get to the upper half plane, say to a position \(x', 1\). But there
is an obstacle at the origin, so she really can only move on \(\mathbb{R}\setminus\{0\}\).

We again assume a continuous time evolution function \(A_t: \mathbb{R} \to \mathbb{R}^2\setminus\{0\}\).
But this time, we also need to assume that \(A_t(x)\) is continuous in \(t\), i.e. that the driver can't teleport.

Furthermore, we assume that if the driver is already far to the right of the tree, it crosses to the right,
and vice versa. Formally: there is some \(\xi > 0 \) such that \(A_t(\xi)_1 > 0\) for all \(t\) and \(A_t(-\xi)_1 < 0\)
(here the subscript denotes the first component of \(A_t(\xi)\), i.e. the \(x\)-position).

*Claim:* For any time \(t\), there is some initial position \(x\), such that \(A_t(x)\) is in the lower half plane.
In other words, the driver can't move around the obstacle in a bounded time.

*Proof:* ?
\(A: \mathbb{R} \times I \to \mathbb{R}^2 \setminus\{0\}\)

** Flying asses
** Homotopy version of Buridan's principle
Let \(X\) be some state space (any topological space), and \(A\) a space of inputs
(another topological space). Let further \(\iota: A \to X\) be an embedding that specified for
each input which initial condition it corresponds to. Let \(H: A \times \mathbb{R} \to X\)
be a continuous function which describes the time evolution of any initial state, with
\(H(a, 0) = \iota(a)\) for all \(a \in A\). Finally, let \(S \subset X\) be a set of goal states
and let \(B \subset X\) be a set of "boundary condition points", meaning that once the system
once the system is in \(B\) it doesn't leave it: \(A_t(x) \in B \implies \(A_{t + \tau}(x) \in B \forall \tau \geq 0\).
* TODO Preference vector fields and the VNM theorem
* TODO Averages of ensembles of graph cuts
Frechet mean, why this is much easier with seeds, relation of our and Gdalyahu's method.
Probably after out paper is out. See Bachelor's thesis for details.
* TODO Natural Turing machines
It feels like there should be a notion of "reasonable UTMs" for
defining Kolmogorov complexity. For example, Python is reasonable,
Brainfuck is also reasonable, but a TM that outputs some random
large numbers on its first Googolplex input and then behaves exactly like
Python is much less reasonable.

One thing we can do at least is to define a distance between
UTMs based on the supremum of the Kolmogorov complexity differences
over all strings. Then e.g. Python and Brainfuck are reasonably close
because an interpreter for one can be written in the other rather
easily (of course a Python interpreter in Brainfuck would
still be a nightmare but not a Googol-length nightmare).

But then we still need to identify the "cluster" of reasonable
UTMs. Not sure if we can do this without resorting to the physical
universe we happen to live in. But even then, how do we ground it
in physics?
* TODO Structureless universes :structure:
* TODO Time turners and fixed point theorems
* TODO List of useful Tools
More about knowing which tools exist, less about 5min concrete tips
- git commit hooks
- sshfs

* TODO Series: Differential calculus on graphs
Theme: graphs as a "generalization" of differential geometry.

Topics like:
- potentials and rotation-free "vector fields" on graphs
  (example: nodes = currencies, edges = log exchange rates.
  EMH = there are no cycles with negative sum of costs)
- Laplacian matrix and Laplacian from differential geometry

* TODO Convolutions and Quantum Mechanics (clickbait!)
Important resource: https://towardsdatascience.com/deriving-convolution-from-first-principles-4ff124888028

Convolutions are circulant matrices, which generalize permutation matrices
and have the same eigenvectors (namely a Fourier basis).
In QM, the momentum operator generates the translation operator
and its eigenbasis are also plain waves.

If we look at spectral graph convolutions, we use a Laplacian eigenbasis,
which is the square of the momentum operator. The plot thickens.

I'm not really sure yet what the overarching story here is but there
seem to be interesting and deep connections. This might be a follow-up
to the Calculus on Graphs series/post, because for convolutions (in the CNN context),
we use discrete versions of the continuous QM phenomena.
* TODO Better orgmode LaTeX export
* TODO Distropy
* TODO PPO tutorial
I personally didn't like any of the tutorials I found, which is kind of surprising,
I thought for popular methods like that there was essentially always a good
one. But it's somewhat new and RL is a bit more niche than other topics maybe?

In any case, if I take the time to really understand PPO at some point,
it'd make sense to invest the time to write a really good tutorial on it,
that would be a really useful resource for people.
* TODO Cartesian structure :structure:
If a set is equipped with a factorization into a cartesian product, this feels
like an important additional structure (in the sense of groups, topology etc.)
Does it fulfill the things I talk about in the previous post? What are the morphisms?

The automorphisms should be products of permutations, i.e. each factor of the
cartesian product is permutated separately, I think. We could extend this idea
of products of morphisms to morphisms between cartesian products with equally many
factors (though I'm less sure how much sense it makes there).

* TODO Generative models
Nachdem ich davon eine Weile so verwirrt war, bietet sich das an, ich glaube ich kriege
langsam einen guten berblick.
- Grund framework: wollen likelihood maximieren, oder equivalent KL divergence minimieren
- Goodfellow's overview benutzen
- Zusammenhang zwischen VAEs und variational inference klar machen (vllt. separater post)
* TODO Lessons learned from replicating ML papers
* TODO Clarifying transparency in AI alignment
Probably on LW rather than blog.

Three levels of understanding ML systems:
- Understanding a learning method/inductive biases (SGD, unreasonable effectiveness of deep learning).
  This one is a bit separate perhaps, at least not usually called transparency
- Understanding a trained model (global interpretability)
- Understanding a particular prediction

These play different roles for AI alignment, clarify those.

The mainstream interpretability field focuses on the third one, I think (though there are some
global methods). It also focuses on model-agnostic methods, which is usually not at all what
we want in AI alignment.
There, we want to do things like detect inner optimizers. That will probably require somewhat
model-specific transparency tools (and certainly global ones).
* TODO Towards a Theoretical Understanding of Deep Learning
The fact that deep neural networks are so successful is often regarded
as somewhat of a mystery. The pace of practical performance improvements
over the last years has been enormous but theoretical insights have
been comparatively rare.

Nevertheless, there are some avenues that could potentially shed more
light on some of the key questions in deep learning. In this blog post I
will give an overview of the ones I'm aware of.

** Deep questions in deep learning
This blog post deals with "deep" theoretical questions, i.e. those questions
that to me seem to be central to understanding why neural networks work
well in practice.

Posts on the unreasonable effectiveness of DL:
- https://www.lesswrong.com/posts/PQu2YPtcm2dQLSsu9/the-unreasonable-effectiveness-of-deep-learning
- https://www.pnas.org/content/early/2020/01/23/1907373117
- https://www.pnas.org/content/113/48/E7655

Questions:
- Why do overparametrized networks trained with SGD generalize so well?
  (even without or with little regularization)
- What's up with adversarial examples?

** What we know/Avenues of research
* TODO Daily Idea
I've recently started a new habit, the "Daily Idea". Once a week, during my weekly review,
I choose a topic on which I wand to collect ideas. For example, I've had "Self-studying more effectively",
"Blog posts I could write" and "Topics to collect ideas on". Then, once a day, I set a timer
for 5 minutes and brainstorm ideas for that prompt. At each weekly review, I look at
the ideas from the past week and evaluate them: I make the good ones actionable and put
them on my todo list, and any that I might want to look at again later go on the someday/maybe list
(in GTD terminology). There are usually also plenty of ideas that are obviously garbage,
and I simply ignore those.

I see two benefits of this habit. First, regularly trying to collect as many ideas as possible
ought to train idea generation. I've observed this to some degree, though I'm less sure how
well this effect transfers to settings other than deliberately sitting down and collecting
as many ideas as I can on one topic.
Second, I've actually had several good ideas with this method that have been practically
valuable. This was not the main motivation originally but it's now the reason why I'm keeping
up this habit even though I'm not sure if there are still any gains in terms of improving
my idea generation ability.

Some takeaways from my experience with this habit:
- It's important not to filter ideas while generating them. There's nothing wrong with writing
  down bad ideas, that's why we evaluate them at the end of the week instead of just blindly
  putting them on the todo list. I've trained myself to write down ideas even if I notice they
  are terrible in the moment between having the idea and writing it down.

* TODO Replacing Jupyter notebooks with Emacs
Target audience: people somewhat familiar with emacs
and orgmode (they know what "M-x" means and can put stuff
into their init.el and run it) but who haven't necessarily
used it for scientific notebooks or literate programming.

Talk about
- emacs-jupyter
- Setting up jupyter keybindings (S-RET, C-RET) plus other useful
  keybindings (e.g. inserting cells)
- how to use with virtual environments. Two approaches:
  1. install jupyter in every virtual environment and use
     =pyvenv= or similar. In that case: create hook to automatically
     reload kernelspec list if environment changes
  2. install environments as jupyter kernels into default environment
- export options for pdf/html/...
  - (don't) automatically rerun everything on export
  - include code/results/both
- exporting to jupyter notebooks?
- importing from jupyter notebooks?
- automatically displaying image output
- rescaling image output to a sane default size
** Why use Org mode for notebooks?
First, to get it out of the way: Org can do essentially everything that
Jupyter can (with =emacs-jupyter=): code blocks, formatted texts, inline
images (either pasted or as output), all the same programming languages
(and more!), embedded LaTeX (but much more than just embedded formulas
like in Jupyter markdown!)

Advantages:
- Completely decouples kernels from notebooks: you can have code blocks
  attached to several different kernels (with different languages!)
  in the same notebook. You can attach ipython consoles to the same
  kernel you're using for a notebook. You can share a kernel between
  notebooks (is there a scenario where you'd want to do that?)
- Org mode > Markdown. You can produce beautifully typeset files
  with Org mode, write entire articles in it. This means that
  the distinction between notebooks and documents vanishes
  because you use the same tool for both anyway. The same just isn't
  possible with Jupyter notebooks (e.g. citations, arbitrary LaTeX blocks)
- all the extensibility and editor powers of emacs (e.g. snippets, writing
  arbitrary custom elisp code, magit, easier switching between windows
  and buffers, powerful editing commands)
- notebooks are human-readable text files, not json (though admittedly
  reading them outside emacs is a way less pleasent experience).
  Main advantage of this are nice git diffs.
- export is probably even better than for Jupyter notebooks (e.g.
  things like ox-hugo)
- output can be converted to org tables or lists
- you can easily transfer data between arbitrary programming languages.
  You can also input data into an Org table (optionally using spreadsheet
  capabilities) and then use it in arbitrary languages.
  Note: the [[http://beakerx.com/][Beaker]] extension for Jupyter can apparently do this too
- All the other org goodness: Outlining, collapsing code blocks, built-in spreadsheets,
  TODOs

Disadvantages:
- no or less nice support for widgets (which might become more important
  in the future)
- most other people work with .ipynb format. Export is there but of course
  extra step and things can go wrong. Import not yet possible
- Uncertain whether future improvements to the Jupyter experience will
  be ported
- Much more time needs to be invested upfront for learning and setup
* TODO My orgmode productivity setup
* TODO Systems beat (and build) habits
Idea: use systems instead of relying on habits whereever possible.
Examples: have a habit tracker, a daily checklist, maybe use
automation on computer, decision journal with checklists.
The key idea is that having such systems in place makes it really
easy to incorporate new ideas and best practices or just try
them out. Habits might then follow if you use a system for some
time.

* TODO Nudging with software
- Morning welcome screen
- Assistant for daily checklist
- Theme switching for work/play
- Blockpage with suggestions for other things to do (when visiting Reddit etc)
- Close Telegram etc. automatically in the evening
- Automatically close unproductive tabs when suspending
- Don't autocomplete things like youtube.com in omnibar

* TODO Causal diagrams
Idea: whenever a habit fails or something else doesn't go well,
actually draw a causal diagram for that specific instance.
Then consider which interventions would have prevented /that particular instance/
and implement all of them (even if one would have sufficed).

* TODO Principle of independent mechanisms
* TODO How color works
* TODO The simple math of everything
* TODO Why are observables hermitian operators?
* TODO Fun facts about high-dimensional spaces
* TODO Bayesian interpretations of regularization
I don't think very many things are MAP inference, so maybe formulate
more broadly. Also note that "regularization" sometimes just refers to
terms added to the loss (which are MAP inference), not to anything that
has a "regularizing" effect in the broader sense.

There's a Wikipedia page with a similar name:
https://en.wikipedia.org/wiki/Bayesian_interpretation_of_kernel_regularization
But even though it looks very fancy, I think it only covers the L2 case for
linear regression.

Overview over priors in NNs (long, doctoral thesis):
https://escholarship.org/content/qt1jq6z904/qt1jq6z904.pdf?t=pdak4f

- Mention https://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/
  as a more beginner-friendly introduction (covers L1 and L2 specifically for linear regression)
  Alternative source: https://rohanvarma.me/Regularization/
  (also covers L2 for linear regression, additionally something about sparsity with KL divergence?)
- Regularization = MAP inference is at least not new:
  https://www.cs.toronto.edu/~duvenaud/talks/early-stopping-bnp.pdf (second slide)
  How well known is this idea, why didn't I find any articles on that?
- Gaussian prior => L2 regularization
- Laplace prior => L1 regularization
- There's also stuff on using Jeffreys priors: http://web.engr.oregonstate.edu/~nguyeta4/pub/c/SSP_TamNguyen.pdf
- Yet another prior: https://arxiv.org/pdf/0901.4011.pdf
- Do other regularization techniques also have bayesian interpretations?
  E.g. for dropout: http://proceedings.mlr.press/v97/nalisnick19a/nalisnick19a.pdf
  And the famous https://arxiv.org/pdf/1506.02142.pdf
- For SGD/batch size, maybe relevant: https://openreview.net/forum?id=BJij4yg0Z
- What about the "regularization" term in VAEs? Prior would probably have
  the form exp(D_KL(...||...)), right? See [[https://www.researchgate.net/publication/255646117_Some_Properties_for_the_Exponential_of_the_Kullback-Leibler_Divergence][paper on properties of that expression]]
  Addendum: actually, this "regularization" term depends on the input, doesn't it?
  So it's not purely a prior over weights. Maybe it can be split into a sum?
- Early stopping: http://proceedings.mlr.press/v51/duvenaud16.pdf
* TODO KL divergences in ML :ml_insights:
Apparently, they occur quite regularly (especially as a nice justification for maximum likelihood)
* TODO Autoencoders und PCA :ml_insights:
PCA = linear autoencoder

*Caution*: actually, they find the same subspace but apparently not the same
basis for that subspace

More interestingly, maybe probabilistic PCA = VAE?

This could also be part of a "ML insights" series of short posts about one particular
insight. the L1 vs L2 regularization would be another good candidate, as well as the
KL divergences in ML. Would make be feel less hesitant about publishing things
like this that are not new or work-heavy.
* TODO April Fools: MLPs as activation functions
Use MLPs as activation functions. Because the activation functions is a hyperparameter,
so are the weights of the MLP. By tuning hyperparameters on the test set, as is common
practice, we can achieve astonishing performance, even with only a single non-linearity!

(formulate this idea in an overly complicated way so that it sound scientific and sensible at first)

