[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a Master\u0026rsquo;s student in Artificial Intelligence at the University of Amsterdam. My goal is to have a positive impact on the world by making sure that AI will be aligned with humanity’s interests.\nI\u0026rsquo;m currently doing a research internship at the Center for Human-Compatible AI, where I work on interpreting reward models. I have also done work on equivariant deep learning and graph-based segmentation.\n","date":1627776000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627776000,"objectID":"a4e464a1a97bf0184b407fe8db196761","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I\u0026rsquo;m a Master\u0026rsquo;s student in Artificial Intelligence at the University of Amsterdam. My goal is to have a positive impact on the world by making sure that AI will be aligned with humanity’s interests.","tags":null,"title":"Erik Jenner","type":"authors"},{"authors":["Erik Jenner","Enrique Fita Sanmartín","Fred A. Hamprecht"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"f011d63ac54021d562d3a87a24e84dff","permalink":"https://ejenner.com/publication/karger/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/karger/","section":"publication","summary":"We prove impossibility results showing that Karger's contraction algorithm cannot be extended to $s$-$t$-mincuts or normalized cuts. However, we show how extensions of Karger's algorithm can still be useful for seeded segmentation.","tags":["Graphs"],"title":"Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice","type":"publication"},{"authors":null,"categories":null,"content":"Schwartz distributions are a generalization of functions from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\): strictly speaking, they aren\u0026rsquo;t such functions themselves, but you can do a lot of the same stuff with them that you can do with normal functions, such as taking derivatives, computing convolutions, and even Fourier transforms (at least in certain cases). And in some ways, they even make life easier compared to functions. For example, every distribution is infinitely differentiable! But of course, we do have to give up some things: distributions can\u0026rsquo;t be evaluated at a single point and it\u0026rsquo;s in general impossible to multiply two distributions.\nIn this series, we\u0026rsquo;ll try to understand all of these properties of distributions and more. I will focus on intuition but still give formal definitions of all the concepts we look at. As a secondary purpose, studying distributions will also be an excellent opportunity to practice finding good definitions. We will introduce many different operations on distributions and in each case, we will try to understand how one could come up with the definition in a natural way.\nMotivation In electrostatics, charge densities are used to model the amount of electric charge in different places. Such a charge density is a function \\(\\rho: \\mathbb{R}^3 \\to \\mathbb{R}\\) that assigns an amount of charge per volume to every point \\(x \\in \\mathbb{R}^3\\). From an experimental standpoint, these densities are only useful abstractions; what we can measure is at best the total charge in some volume. This charge \\(Q\\) is given by the integral of the density over the volume: \\[Q(V) = \\int_V \\rho(x) dx\\] for any subset \\(V \\subseteq \\mathbb{R}^3\\). You can even think of this as the definition of the density \\(\\rho\\): the only thing we care about is that when we measure the charge \\(Q(V)\\) in any volume \\(V\\), we get \\(\\int_V \\rho(x) dx\\).\nNow assume we observe the following: \\(Q(V) = 1\\) for any volume \\(V\\) that contains the origin but \\(Q(V) = 0\\) if \\(V\\) does not contain the origin. Intuitively, we conclude that there is a point charge with value 1 in the origin and no charge anywhere else. But how can we model this using a density \\(\\rho\\)? If \\(\\rho\\) is any (integrable) function, as we originally assumed, then we must have \\(\\rho(x) = 0\\) for \\(x \\neq 0\\).1 But in that case, \\(\\int_V \\rho(x) dx = 0\\) for all volumes \\(V\\), which contradicts our first observation.\nFor now, let\u0026rsquo;s just \u0026ldquo;define this problem away\u0026rdquo;: we\u0026rsquo;ll say that \\(\\rho(x) = \\delta(x)\\), where \\(\\delta(x)\\) is an object such that \\[\\int_V \\delta(x) dx := 1 \\text{ if } 0 \\in V, \\text{ otherwise } 0.\\] The word \u0026ldquo;object\u0026rdquo; here is code for \u0026ldquo;we\u0026rsquo;re pretty confused and don\u0026rsquo;t know what this thing is but we\u0026rsquo;d like to have something that behaves this way\u0026rdquo;.\nWe\u0026rsquo;ll develop a formal definition of \\(\\delta\\) soon. But first, let\u0026rsquo;s extend the original example a bit: suppose instead of being interested only in the charge inside some volume, we now introduce a charged test particle and want to know the potential energy it has due to the charge density \\(\\rho\\). This potential is given by \\[\\Phi \\propto \\int_{\\mathbb{R}^3} \\frac{1}{|x_0 - x|} \\rho(x) dx\\] for a test particle at position \\(x_0\\). So what is the potential energy if we have the point charge from before, \\(\\rho(x) = \\delta(x)\\)? So far, we have only defined \\(\\int_V \\delta(x) dx\\), and if \\(\\delta(x)\\) appears anywhere else, we don\u0026rsquo;t really know what to do with it. Remember, \\(\\int_V \\delta(x) dx\\) is just a notation we introduced to mean \u0026ldquo;1 if \\(0 \\in V\\) and 0 otherwise\u0026rdquo;, it\u0026rsquo;s not actually an integral in any usual sense.\nSo we will apply a powerful technique \u0026ndash; wishful thinking. We just assume that \\(\\delta(x)\\) behaves the way we would intuitively like it to, and then worry later about constructing something that actually does behave that way. Since for \\(\\rho(x) = \\delta(x)\\), there is no charge outside the origin, all parts of the integral above except for \\(x = 0\\) ought to vanish. So let\u0026rsquo;s just write \\[\\int_{\\mathbb{R}^3} \\frac{1}{|x_0 - x|}\\delta(x) dx = \\int_{\\{0\\}}\\frac{1}{|x_0 - x|}\\delta(x)dx.\\] Since we\u0026rsquo;re only integrating over \\(\\{0\\}\\) now, we can set \\(x = 0\\) in \\(|x_0 - x|\\). Then this part doesn\u0026rsquo;t depend on \\(x\\) anymore and we get \\[\\int_{\\{0\\}}\\frac{1}{|x_0 - x|}\\delta(x)dx = \\frac{1}{|x_0|}\\int_{\\{0\\}}\\delta(x)dx.\\] But we know what to do with that last part, its' 1! So the potential should be \\(\\Phi \\propto \\frac{1}{|x_0|}\\).\nWe can apply the same argument more generally to \\(\\int \\varphi(x) \\delta(x)dx\\) for other functions \\(\\varphi\\). So let\u0026rsquo;s \u0026ldquo;wish\u0026rdquo; that \\[\\int_{\\mathbb{R}^3} \\varphi(x) \\delta(x) dx := \\varphi(0)\\] hold for all functions \\(\\varphi\\). This contains our original definition of \\(\\delta(x)\\) as a special case, namely for the indicator function \\(\\varphi = 1_V\\).\nSchwartz distributions The defining property of \\(\\delta(x)\\) that we would like to have is \\[\\int_{\\mathbb{R}^3} \\delta(x) \\varphi(x) dx := \\varphi(0)\\] for arbitrary functions \\(\\varphi\\). We have already noted that this cannot be an actual (Lebesgue) integral, so it makes sense to get rid of that notation. Instead, we will write \\[\\langle \\delta, \\varphi\\rangle := \\varphi(0).\\] This hightlights the important part: \\(\\delta\\) lets us take any function \\(\\varphi\\) and maps it to its value \\(\\varphi(0)\\) at the origin. So \\(\\delta\\) is a function after all; just not from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\) but from the space of functions on \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\)!\n\\(\\delta\\) is one example of Schwartz distributions or distributions for short, which are all maps from a space of functions to the real numbers. Let\u0026rsquo;s make this more precise:\nDefinition: Let \\(U \\subseteq \\mathbb{R}^n\\) be an open subset. A test function on \\(U\\) is a smooth, compactly supported function \\(\\varphi: U \\to \\mathbb{R}\\) and we write \\(\\mathcal{D}(U)\\) for the space of all such test functions. A Schwartz distribution on \\(U\\) is then a continuous linear function \\(T: \\mathcal{D}(U) \\to \\mathbb{R}\\). We write \\(\\mathcal{D}'(U)\\) for the space of all such distributions on \\(U\\).\nThis definition requires some clarifications. First, Schwartz distributions are not at all the same thing as probability distributions, and when I say \u0026ldquo;distribution\u0026rdquo; in this series, I will always mean a Schwartz distribution. Second, if we want to talk about continuity, we of course need to define a topology on the space \\(\\mathcal{D}(U)\\) of test functions. The topology we use here is called the canonical LF topology but we won\u0026rsquo;t discuss that any further in this post.\nThe name test function comes from the fact that these are the functions on which we can \u0026ldquo;test\u0026rdquo;, i.e. evaluate distributions. In our first example about the total charge in some volume, we used indicator functions \\(1_V\\) as test functions. The \\(\\delta\\) distribution would in principle work on any space of test functions. But it turns out that a good choice for the general definition are smooth compactly supported functions because this makes a lot of the theory very nice.\nWe will write \\(\\langle T, \\varphi \\rangle\\) for the distribution \\(T\\) applied to the test function \\(\\varphi\\). But why did we write \\(\\int \\delta(x) \\varphi(x) dx\\) before? What does all of this have to do with integrals? The reason is the following: let \\(f : U \\to \\mathbb{R}\\) be any locally integrable (read \u0026ldquo;somewhat reasonable\u0026rdquo;) function. Then the map \\[\\varphi \\mapsto \\int_U f(x) \\varphi(x) dx\\] defines a distribution on \\(U\\), which we denote by \\(T_f\\). This is the sense in which distributions are generalized functions; each classical function induces a distribution. So when we write \\(\\int \\delta(x) \\varphi(x) dx\\), we are essentially pretending that the delta distribution is induced by a function \\(\\delta(x)\\). There is no such function, but the notation is used very often anyway; probably in part for historical reasons and in part because it turns out to work surprisingly well, as we\u0026rsquo;ll see next.\nWe will revisit distributions in general in the next post but for now, we focus on the \\(\\delta\\) distribution again.\nVariations of the \\(\\delta\\) distribution We now have a formal understanding of terms of the form \\(\\int \\delta(x) \\varphi(x)dx\\). But in practice, the \\(\\delta\\) distribution often appears in modified versions, such as in terms like \\[\\int \\delta(x - x_0)\\varphi(x) dx\\] or \\[\\int \\delta(ax)\\varphi(x)dx.\\] So far, we haven\u0026rsquo;t formally defined these terms. That means it\u0026rsquo;s time to apply the Power of Wishful Thinking again, in order to find good definitions for them.\nIt\u0026rsquo;s pretty clear what \\(\\delta(x - x_0)\\) should mean: it\u0026rsquo;s just a shifted version of \\(\\delta(x)\\), with its \u0026ldquo;peak\u0026rdquo; at \\(x_0\\) instead of \\(0\\). More explicitly, it makes sense to demand that \\[\\int \\delta(x - x_0)\\varphi(x) dx = \\int \\delta(x) \\varphi(x + x_0) dx\\] as would be the case if \\(\\delta\\) was a regular function (all integrations are assumed to be over all of \\(\\mathbb{R}^n\\)). Then we can see that \\[\\int \\delta(x - x_0)\\varphi(x) dx = \\varphi(x_0).\\]\nLet\u0026rsquo;s consider \\(\\int \\delta(ax)\\varphi(x)dx\\) instead. You might argue as follows: \u0026ldquo;\\(\\delta(ax) = 0\\) for \\(x \\neq 0\\), so we only need to consider \\(x = 0\\). In that case, \\(ax = 0 = x\\), so \\(\\delta(ax)\\) should be the same as \\(\\delta(x)\\)\u0026rdquo;. But this is a misunderstanding caused by the (admittedly very confusing) notation often used for the \\(\\delta\\) distribution: \\(\\delta(x)\\) doesn\u0026rsquo;t mean that anything is actually being evaluated at \\(x\\), it\u0026rsquo;s just a notational convention that only makes sense inside integrals. We don\u0026rsquo;t want to demand that \\(\\delta(\\cdot)\\) behaves like functions when we plug in different things because we never have \\(\\delta(x)\\) appearing on its own anyway.\nWhat we do want is that \\(\\delta(x)\\) behaves like functions inside an integral. In particular, for functions \\(f\\) and \\(\\varphi\\) and a scalar \\(a \\neq 0\\), we have \\[\\int f(ax)\\varphi(x)dx = \\frac{1}{|a|^n}\\int f(x)\\varphi\\left(\\frac{x}{a}\\right)dx.\\] So since we want \\(\\delta(x)\\) to behave the way that functions behave inside integrals, we define \\[\\int \\delta(ax)\\varphi(x)dx := \\frac{1}{|a|^n}\\int\\delta(x)\\varphi\\left(\\frac{x}{a}\\right)dx = \\frac{1}{|a|^n}\\varphi(0).\\]\nIn fact, we can generalize this argument: for any diffeomorphism \\(g\\) of \\(\\mathbb{R}^n\\), we have \\[\\int f(g(x))\\varphi(x)dx = \\int |\\operatorname{det} Dg(x)|^{-1} f(x)\\varphi(g^{-1}(x))dx\\] where \\(Dg\\) is the derivative (Jacobian) of \\(g\\). So in analogy, we can define \\(\\delta(g(x))\\) for any diffeomorphism \\(g\\) by \\[\\int \\delta(g(x))\\varphi(x)dx := \\int |\\operatorname{det} Dg(x)|^{-1} \\delta(x)\\varphi(g^{-1}(x))dx = |\\operatorname{det} Dg(0)|^{-1}\\varphi(g^{-1}(0)).\\]\nI want to stress again that none of these arguments are \u0026ldquo;proofs\u0026rdquo; or \u0026ldquo;derivations\u0026rdquo; \u0026ndash; in the end, we have to choose how to define all of these terms. But clearly some definitions make more sense than others and in the examples here there is clearly one \u0026ldquo;right\u0026rdquo; way to define what \\(\\delta(g(x))\\) etc. should mean. This will become even more clear in the next post: we will continue the theme of finding good definitions via \u0026ldquo;wishful thinking\u0026rdquo;, only this time for arbitrary distributions and for many more types of operations.\n  Actually, only for almost all \\(x\\) but that doesn\u0026rsquo;t change anything.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1625573700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625573700,"objectID":"d4559bf76103b76afb837e21099513a8","permalink":"https://ejenner.com/post/distributions-intro/","publishdate":"2021-07-06T14:15:00+02:00","relpermalink":"/post/distributions-intro/","section":"post","summary":"  Did you always want to know kind of object this weird Dirac delta \"function\"\n  actually is? Well, it's a Schwartz distribution. If that doesn't help much,\n  then keep reading.\n  ","tags":["Math"],"title":"Distributions Part I: the Delta distribution","type":"post"},{"authors":null,"categories":null,"content":"This is just a short PSA: if you can code, you can write small scripts to support your habits and productivity routines. I\u0026rsquo;m not talking about automating long, tedious tasks. Rather, I mean automating tasks that take about five or ten seconds but that you do every day, or at least very often.\nThe point is not actually that you\u0026rsquo;ll save five seconds. Instead, such scripts can give you the right nudges at the right time, make your life slightly less annoying, or automate stuff so you can\u0026rsquo;t accidentially forget doing it. The philosophy is that even very small inconveniences matter, and if you can spend a few minutes to make every day that follows even a tiny bit more convenient, it\u0026rsquo;s probably worth doing.\nThis is best illustrated by some examples, so that\u0026rsquo;s what the rest of this post consists of. But I\u0026rsquo;m sure this is only scratching the surface, so take these as inspiration and not as a comprehensive list of possibilities.\nChecklists One of my most important routines is a \u0026ldquo;daily checklist\u0026rdquo; that I go through at the end of each day, where I reflect a bit but mainly plan the next day. I used to just have a list of all the steps in a text file and went through those, but as the checklist grew, this became ever so slightly annoying, and I was liable to skip steps sometimes. So I wrote a small script (inside Emacs), which takes me through the checklist. I press a keyboard shortcut to start the checklist and I\u0026rsquo;m prompted with the first item, then I press the same shortcut again and I\u0026rsquo;m prompted with the next item and so on. For me, this alone is already an improvement over a list in a textfile, because I\u0026rsquo;m less likely to press the shortcut without actually doing the current item than I was to just skip to the next one when reading a checklist. But even better, for many items my script can give me additional nudges to make the checklist less annoying. For example, I have several items where I look at my weekly goals, my scheduled TODO items for the next day etc. Instead of opening the right files by hand, the script does that automatically once the corresponding prompt comes up. It doesn\u0026rsquo;t sound like much, but it adds up and makes going through the checklist much less annoying, which means I\u0026rsquo;m more likely to do it diligently. Using a script for this checklist has other advantages, which I\u0026rsquo;ll talk about below.\nOf course, this is not specific about a daily checklist you go through every evening, it applies to any checklist you use regularly and which has a reasonably large nummber of items.\nAutomatically close distracting programs Cal Newport recommends ending the work day with a shutdown ritual and truly relaxing afterwards. This doesn\u0026rsquo;t really work if you still have programs such as Slack open that distract you with work-related notifications, so my daily checklist script closes distracting programs automatically at the end of the checklist.\nOne thing I haven\u0026rsquo;t yet looked into but that would be nice is to also close individual browser tabs automatically. For example, you could close distracting websites whenever you suspend your PC, or whenever they were idle for a specific time, or when you start a Pomodoro, etc. A website blocker can of course serve a similar function, but unless you always block those website, the slight nudge from closing tabs automatically could be useful on top of a blocker.\nWelcome screen When I start my computer in the morning, I\u0026rsquo;m greeted by a fullscreen wallpaper, with a nice quote and my top priority for the day. For example, it might look like this1:   The top priority is another thing where my daily checklist script comes in handy: it prompts me to enter one for the next day and stores it; the welcome screen can then later read it from disk and display it.\nUse APIs for the apps you use This is of course very specific to the apps you use, but I\u0026rsquo;ll give an example. I use Complice to plan my day and Beeminder for accountability for my goals. Yet another thing my daily checklist script does is to fetch all goals from Beeminder for which some progress is due the next day, using the Beeminder API. It then adds corresponding TODOs to Complice, using the Complice API. So before I add TODOs manually, the list on Complice is already pre-populated with what I need to do to stay on track with my Beeminder goals.\nOf course this is only possible when the services you want to automate stuff for expose an API, but it\u0026rsquo;s worth checking whether that\u0026rsquo;s the case. If it is, it\u0026rsquo;s often surprisingly easy to use, as long as you only want to do a few simple things. What I just described as an example can be implemented as a bash script with just a few lines, using curl to access the API and jq to parse JSON.\nFinal notes In case you\u0026rsquo;re interested, here is the script that displays the welcome screen and here is the one for the Complice/Beeminder API. That repository also contains the other things I mentioned in this post.\nI\u0026rsquo;m interested to apply this idea of automating parts of my routines even more. If you have ideas in that direction, I would appreaciate a message. Also feel free to shoot me an email if you are trying to set up any of the things I\u0026rsquo;ve described here and want more details on how to do that.\n  Wallpaper source\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1618422120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618422120,"objectID":"225a74febf302eee5917bde9972bc037","permalink":"https://ejenner.com/post/automation-productivity/","publishdate":"2021-04-14T19:42:00+02:00","relpermalink":"/post/automation-productivity/","section":"post","summary":"  If you can program, you can use that to support your habits and automate\n  some routines. This post gives a few examples.\n  ","tags":["Productivity"],"title":"Scripting for personal productivity","type":"post"},{"authors":null,"categories":null,"content":"(Last updated: 2021-04-22)\nArguably one of the most important concepts in machine learning, taught in any introductory course, is under- and overfitting. The story goes like this: if your model is too simplistic, you won\u0026rsquo;t be able to fit the data well and get a large error, you underfit. On the other hand, if your model is too complex, it will fit any noise that is present in the data, i.e. you overfit. Such a model won\u0026rsquo;t generalize to the test set, so you also get a large error. Somewhere in between those two is a sweet spot with minimal test error.\nThe choice of words, under- and overfitting, already implies that we believe a tradeoff exists: they are two ends of a scale, and we need to find the point in the middle where we\u0026rsquo;re neither under- nor overfitting too much.\nUnder- and overfitting can be formalized using the notions of bias and variance (there\u0026rsquo;ll be a short recap in the next section). Underfitting means that we have a high test error because of high bias, while overfitting means that high variance causes a high error. Phrased in those terms, the tradeoff between under- and overfitting becomes the bias-variance tradeoff: methods with low bias tend to have high variance and vice versa.\nThis idea is ubiquitous in machine learning. So when I recently wanted to look up some details, I expected to find troves of information. Tons of empirical evidence, a formal definition of what exactly we mean by \u0026ldquo;tradeoff\u0026rdquo;, and hopefully even theorems showing that this tradeoff exists. As you can tell from the setup, that\u0026rsquo;s not what happened. So in this post, I\u0026rsquo;m going to describe what I found (and what I did not find). My goal is to clear up some potential misconceptions, and hopefully convince you that the bias-variance tradeoff is less simple and more interesting than you thought.\nPrimer: Bias and variance Under- and overfitting can be explained in terms of bias and variance. I\u0026rsquo;m going to discuss everything in a supervised learning setting. So the setup is the following:\n There is a true (unknown) function \\(f(x)\\), which we want to approximate We have a dataset \\(D = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}\\) of datapoints that we use to learn an approximation, where \\(y_i = f(x_i)\\) We have some training process that takes in this dataset \\(D\\) and produces a function \\(\\hat{f}(x; D)\\) that approximates \\(f(x)\\) Finally, we imagine there is some distribution over datasets \\(D\\). This distribution is unknown an it\u0026rsquo;s a somewhat elusive concept, but think of it like this: we created our dataset with some process, e.g. by taking lots of photos and then having people label them. The distribution over datasets describes how likely this process is to produce any particular dataset.  Now we can define precisely what we mean by bias and variance:\n The bias is the difference between the expected value of \\(\\hat{f}(x; D)\\) and the true value \\(f(x)\\), i.e. \\[\\mathbb{E}_D[\\hat{f}(x; D)] - f(x)\\] By \u0026ldquo;variance\u0026rdquo; we mean the variance of \\(\\hat{f}(x; D)\\) with respect to \\(D\\), i.e. \\[\\mathbb{E}_D \\left(\\mathbb{E}_D[\\hat{f}(x; D)] - \\hat{f}(x; D)\\right)^2\\]  Ideally, we want both bias and variance to be small. The reason is the bias-variance decomposition of the expected squared error1: \\[\\mathbb{E}_D (\\hat{f}(x; D) - f(x))^2 = \\text{bias}^2 + \\text{variance}\\] Assuming we ultimately want to minimize this expected squared error, it\u0026rsquo;s clear that all else being equal, we prefer methods with low bias and variance.\nOk, now what about the tradeoff between bias and variance? The idea is that methods with low bias tend to have high variance, and those with low variance tend to have high bias. So we can\u0026rsquo;t get both low bias and low variance, instead we need to find a tradeoff between the two, such that the expected squared error is minimized. This is illustrated by the following figure (from this essay, which I recommend if you want to gain more intuition about bias, variance, and their tradeoff):   If we use an overly simplistic method, we have a high bias because our model just can\u0026rsquo;t get close to the true function \\(f\\), no matter what we feed in as training data. With a more complex model, we can fit the true function better, but the trained model \\(\\hat{f}\\) also depends a lot more on the training data \\(D\\), so the variance increases.\nThis explanation sounds sort of intuitive, but I find it a bit unsatisfying. Why exactly do complex models vary more depending on the training data? When does this hold? Can I have a theorem, please? And what is this \u0026ldquo;model complexity\u0026rdquo; anyways?\nA non-answer Unfortunately, the bias-variance decomposition and the bias-variance tradeoff are often conflated somewhat, so let\u0026rsquo;s get one thing out of the way: the bias-variance decomposition is not an explanation for the tradeoff (even though some handy-wavy explanations suggest this with varying degrees of explicitness). The only way this could work is if the total error was constant; this would indeed imply a tradeoff between bias and variance. But it clearly isn\u0026rsquo;t constant (neither in the figure above, nor in practice). If the total error was constant, then we wouldn\u0026rsquo;t care about the tradeoff between bias and variance: it wouldn\u0026rsquo;t matter which model we chose.\nInstead, the bias-variance decomposition just motivates why we care about bias and variance at all. So it explains why the bias-variance tradeoff is important, but it can\u0026rsquo;t explain why it\u0026rsquo;s a thing in the first place.\nEvidence for a tradeoff Why do we believe in a tradeoff between bias and variance? Well, I mainly do because others have told me about it and it seems pretty intuitive. But those aren\u0026rsquo;t very good reasons, so what to we have in terms of hard evidence?\nAs an example, let\u0026rsquo;s look at what Elements of Statistical Learning by Hastie et al. has to say. It considers two cases in Section 7.3: k-NN regression and linear regression. For k-NN, it gives a theoretical expression for bias and variance, and at least the variance does indeed increase with increasing model complexity (i.e. decreasing \\(k\\)). A caveat is that the formula assumes the input points \\(x_i\\) are fixed and only the targets \\(y_i\\) vary \u0026ndash; a very strong and usually unrealistic assumption. Similarly, there are theoretical expressions in the case of ridge regression, and based on those, variance should typically decrease and bias increase with stronger regularization. In the same section, there are small toy experiments that demonstrate a tradeoff empirically for k-NN and for linear models.\nThis seems to be fairly representative of the kind of evidence we currently have in favor of a tradeoff. If you are curious you should look at Section 3.4 in this thesis, which is more comprehensive than I\u0026rsquo;m going to be in a blog post. But in brief, there is empirical evidence for methods such as decision trees, fitting polynomials, or kernel regression. Then there are some general theoretical results that are weak evidence for a tradeoff if you squint a bit. And that\u0026rsquo;s pretty much it.\nIf you are one of the two or three people working with neural networks, this doesn\u0026rsquo;t really inspire confidence. After all, the fact that something holds for linear regression and polynomials isn\u0026rsquo;t very strong evidence that it\u0026rsquo;s also true for a 25 million parameter ResNet. Which brings us to the question: does the bias-variance tradeoff exist outside the simple methods named above?\nDouble Descent Over the last few years, the bias-variance tradeoff has ben supplemented by a more complicated narrative, dubbed \u0026ldquo;Double descent\u0026rdquo; by Belkin et al. in 2018. The following figure illustrates this concept:   On the left, we have the classical U-shaped curve for the squared error already shown above. But the claim is that if you increase the model complexity past the interpolation threshold, where the model can perfectly fit the training data, then the error decreases again.\nThere is quite a bit of empirical evidence for this double descent curve, especially in the context of neural networks2. The reason seems to be that the variance curve is unimodal, i.e. it first increases with model complexity, as we\u0026rsquo;d classicaly suspect, but then decreases again. Of course that just passes the buck: why does the variance decrease again after the interpolation threshold?\nI think that this is an extremely interesting question, but in this post I\u0026rsquo;m not going to speculate on it. Instead, I\u0026rsquo;m still interested in the bias-variance tradeoff in the classical regime: as long as the model complexity is below the interpolation threshold, does a tradeoff always exist? If so, why? If not, when does it exist? Understanding this question better should also help understanding the behavior beyond the interpolation threshold: if we really understood why the tradeoff exists at all, we would also know how it could potentially be violated.\nGeneral theorems are hard In an ideal world, we could give a formal statement of the bias-variance tradeoff and then prove that it occurs under some fairly general circumstances. Maybe that is indeed possible, but it\u0026rsquo;s easy to rule out the most general kinds of theorems through a few counterexamples.\nThe strongest form of bias-variance tradeoff would be the claim \u0026ldquo;any decrease in variance leads to an increase in bias\u0026rdquo; (and vice versa). This is clearly false. For example, let\u0026rsquo;s say we start with a traing procedure that chooses \\(\\hat{f}(x; D) = \\operatorname{sha1}(D)\\), i.e. which uses a hash of (some serialization of) the training data for its prediction (there\u0026rsquo;s nothing special about this example, just think \u0026ldquo;a really ridiculous training procedure\u0026rdquo;). This procedure has high variance, but also high bias. Switching to any reasonable training procedure will decrease both considerably.\nBut let\u0026rsquo;s consider a more reasonable statement of the bias-variance-tradeoff: assume we have a class of models, and the training procedure picks the model from that class that has the lowest training error. We now want to know how the choice of model class influences bias and variance. Furthermore, we only consider a nested set of model classes. So we have an ordering of model classes from simple to complex, where successively more complex classes contain the simpler ones.\nNote that this is a rather typical example of what we mean when we talk about \u0026ldquo;model complexity\u0026rdquo; and the bias-variance tradeoff in practical contexts. For example, a wider neural network can always instantiate any function that a more narrow one can, by having some weights be zero.\nIn this setting, the bias-variance tradeoff can be formulated as two separate claims: the bias decreases with increasing complexity, while the variance increases. Unfortunately, neither one is true in general.\nA case where bias increases is easy to construct: Let\u0026rsquo;s say the inputs \\(x\\) and the targets \\(y\\) are both real numbers, and there is some noise, i.e. \\(y = f(x) + \\varepsilon\\), with \\(\\mathbb{E} \\varepsilon = 0\\). One very simple model class is \\(\\{f\\}\\), and this class of course leads to a bias of zero. Then there is a more complex model class \\(\\{f, f + 1\\}\\), where we have added a model that always predicts one more than \\(f\\). Because of the noise, we might get unlucky with the training data and pick this second model. So this more complex model class has non-zero bias.\nA slight modification leads to an example where variance decreases with model complexity: we take \\(\\{f - 1, f + 1\\}\\) as the simple model class and \\(\\{f - 1, f, f + 1\\}\\) as a larger class. Assuming that we have a very large amount of training data, we will almost always pick \\(f\\) in the second case, so the variance will be very low. In contrast, in the first case, we pick each of \\(f - 1\\) and \\(f + 1\\) half the time, so we have a constant variance, no matter how much training data we have.\nNote that this decrease in variance happens in the classical regime, in the sense that in either case, there is at most one model that fits the data perfectly. So we haven\u0026rsquo;t crossed the interpolation threshold yet, and still the variance went down when we increased model complexity.\nObviously, these examples are somewhat silly, and certainly not representative of real-world scenarios. They do not constitute a good argument that there is no bias-variance tradeoff in practice, but they do put some limits on what kinds of bias-variance tradeoffs we can prove. So we\u0026rsquo;ll have to put in a bit of work into finding the right formalization of what we mean by \u0026ldquo;bias-variance tradeoff\u0026rdquo;, and in particular when this tradeoff is supposed to hold. As far as I\u0026rsquo;m aware, such a formalization does not exist yet.\nModel complexity Formalizing the bias-variance tradeoff probably also requires formalizing the notion of \u0026ldquo;model complexity\u0026rdquo; to some extent. This is a somewhat elusive concept, with many different aspects of the training procedure falling under its umbrella. The prototypical example of model complexity is the size of the class of models we use. For example, increasing the width of a neural network, or using a larger basis of features for linear regression, both increase the size of the model class under consideration.\nBut the size of the model class is not the entire story. For example, regularization terms in the loss function don\u0026rsquo;t change the model class but instead affect which model is selected from that class. The same is true for the number of trainings steps, or more generally the optimization procedure.\nNakkiran et al. define the effective model complexity of a training procedure as the maximum number of training points for which the expected training error remains below some threshold \\(\\varepsilon\\). That definition captures a lot of my intuition about what we mean by \u0026ldquo;complexity\u0026rdquo; in the context of the bias-variance tradeoff. On the other hand, it seems slightly ad-hoc and I\u0026rsquo;m still hoping for an even better notion of model complexity.\nOne thing that comes to mind when talking about \u0026ldquo;complexity\u0026rdquo; is of course Kolmogorov complexity. But note that we don\u0026rsquo;t want the (expected) Kolmogorov complexity of the learned model. For example, a randomly initialized neural network has high Kolmogorov complexity, but its model complexity should be zero (it\u0026rsquo;s all the way at \u0026ldquo;high bias, low variance3\u0026quot;). What we might be able to use instead is the algorithmic mutual information between the learned model and the dataset. Another approach would be the (information-theoretic) mutual information of dataset and model with respect to the distribution over datasets. Both of these would probably be hard to estimate in practice (and I haven\u0026rsquo;t thought very long about whether they make sense theoretically and intuitively). But they illustrate the type of more fundamental definition that I\u0026rsquo;m hoping exists.\nWhether a good definition of model complexity would lead to a formalization of the bias-variance tradeoff is not clear. For example, just saying \u0026ldquo;bias/variance decreases/increases with increasing effective model complexity\u0026rdquo; doesn\u0026rsquo;t work (the examples from the previous sections are still counterexamples). But at least I suspect that having the right notion of model comlexity is necessary for finding a general formal statement about the bias-variance tradeoff, even if we then need additional conditions, under which it holds (for example that it is only true in the classical regime, i.e. when the model isn\u0026rsquo;t overparameterized).\nConclusion After putting the bias-variance tradeoff under a lot of scrutiny, I also want to emphasize the other side of the coin: it\u0026rsquo;s clear that the bias-variance tradeoff is real, that it appears in many different settings, and that it\u0026rsquo;s extremely important to keep in mind when doing machine learning.\nBut I do think that we don\u0026rsquo;t understand it nearly as well as we could, and that the way the tradeoff is often presented doesn\u0026rsquo;t do a good job of hightlighting the parts we don\u0026rsquo;t understand. I highly recommend this essay, which makes a similar point more explicitly.\n  I\u0026rsquo;m ignoring label noise here. More generally, if the labels are sampled from \\(y = f(x) + \\varepsilon\\) with \\(\\mathbb{E}[\\varepsilon] = 0\\), there would be an additional \\(\\varepsilon^2\\)-term, the irreducible error.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though just to make things even weirder: this only seems to occur when increasing the width of networks. Increasing the depth often isn\u0026rsquo;t tested in papers on the topic, and when it is, there\u0026rsquo;s no double descent curve.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n To forestall potential confusion: the variance with respect to the training data is low. Of course there is high variance with respect to the random initialization itself, but that\u0026rsquo;s not what we\u0026rsquo;re interested in.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1617812340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617812340,"objectID":"5d061a6cb27d83343f625cf39cd2364f","permalink":"https://ejenner.com/post/bias-variance-tradeoff/","publishdate":"2021-04-07T18:19:00+02:00","relpermalink":"/post/bias-variance-tradeoff/","section":"post","summary":"  The bias-variance tradeoff is a key idea in machine learning. But I'll\n  argue that we know surprisingly little about it: when does it hold?\n  How does it relate to the Double Descent phenomenon? And what do we\n  even formally mean when we talk about it?\n  ","tags":["Machine learning"],"title":"Troubles with the Bias-Variance tradeoff","type":"post"},{"authors":null,"categories":null,"content":"This is a list of tips for improving your experience working with your computer. I focus on things that are quick to implement (say, 5 minutes to half an hour). Spending 5 minutes on something becomes worthwhile as soon as it saves you 5 seconds per month over 5 years, and I think all the tips here easily clear that bar. I\u0026rsquo;m not mentioning some things that are extremely valuable but take more time to do, stuff like \u0026ldquo;learn vim keybindings\u0026rdquo;. That\u0026rsquo;s not because those aren\u0026rsquo;t important \u0026ndash; they might even be more important than these quick hacks. But they don\u0026rsquo;t fit well into this format because I would need to give a lot more context: who they are useful for, why it\u0026rsquo;s worth investing time in them etc.\nI\u0026rsquo;ve grouped these tips and tried to sort them by descending usefulness inside each category, but that\u0026rsquo;s of course quite subjective. As a final caveat, I don\u0026rsquo;t explain in detail how to set all of this up. If you\u0026rsquo;ve played around with configuration files before, the pointers I give are hopefully enough. If you haven\u0026rsquo;t, this might not be the easiest place to start.\nGeneral  Make use of your Caps Lock key The Caps Lock key takes up extremely valuable keyboard real estate, even though most people never use it. I suggest mapping it to Control for non-vim users. If you use vim keybindings, you\u0026rsquo;ve probably already remapped it to Escape; in that case I would suggest using it as an Escape key when pressed and as Control when held down while pressing another key. How that works depends on your OS (I\u0026rsquo;m using caps2esc on Archlinux). Fuzzy finder for opening files Opening files using a file browser or by first opening an application and then using the \u0026ldquo;Open file\u0026rdquo; dialog is really slow. Instead, you can use a launcher that you can invoke with a keyboard shortcut. You then type in part of the path or filename and once you confirm your selection, the file is opened. On Linux, you can install for example Alfred or ULauncher (which also have additional functionality rather than just opening files). Or you can use rofi, which is extremely flexible but will require a bit more setup. Adjust your typematic delay and rate If you hold a key down, this simulates pressing that key a bunch of times at a high frequency. The typematic rate is this frequency and the typematic delay is the time delay before this effect kicks in. You can adjust these values to your liking, how that works depends on your OS/Desktop environment. On Linux with X, you can use xset r rate \u0026lt;delay in ms\u0026gt; \u0026lt;rate in Hz\u0026gt; (this is temporary, so put this in a script that will be executed on startup). Redshift Use redshift or f.lux to automatically adjust the color temperature of your screen according to the time of day. This will gradually make your screen look warmer during the evening. Dotfiles Put your dotfiles in a git repository. This page contains a few ideas as inspiration on how to best set this up. Personally, I use Dotbot, which means I can put all my configuration files into one directory and they will be symlinked to the right places. You can also do something similar for your /etc files using etckeeper. sshfs This lets you mount a remote directory inside your local file system, after which you can edit, create, move and delete files there using whichever tools you like to use for that on your local machine. It\u0026rsquo;s in the package repositories of most Linux distributions and using it is as simple as $ sshfs [user@]hostname:[directory] mountpoint    Shell  Use vim keybindings everywhere There are extensions for most browsers that let you browse web pages with vim keybindings. You can also use them in zsh (add bindkey -v to you ~/.zshrc or use this extension for some improved features). In fish, you can use fish_vi_key_bindings inside your config, and for bash it\u0026rsquo;s set -o vi. You can even enable them for all readline programs, such as the Python REPL, by adding set editing-mode vi set keymap vi  to your ~/.inputrc.\n Ctrl-R history search Pressing Ctrl-R inside your terminal will let you search through your history of commands and paste the one you select to your prompt, after which you can edit it. Aliases Pay attention to commands you\u0026rsquo;re using frequently and create aliases for them. Using this command, you can also show your most common commands, maybe that gives some ideas (though what we really care about is closer to \u0026ldquo;most common long prefixes of commands\u0026rdquo;; might be worth it to write a script for that instead). fzf fzf is a general purpose fuzzy finder and you can use it for all kinds of things. But more importantly, for this list, it comes with three pre-defined shell-keybindings: Ctrl-R is replaced with an improved history search, and Alt-C lets you fuzzy search the directories on your machine and will cd to the chosen one. Finally, Ctrl-T lets you search files inside the current directory (recursively) and pastes the selected path into the prompt, which is much faster for typing long paths than the usual TAB completion. Autosuggestions Autosuggestions in your shell mean that the shell always tries to guess which command you\u0026rsquo;re entering, typically based on the TAB completions as well as your history. It shows this guess and you can hit a keybinding to complete it. Make sure to remap this to something sensible (for example, zsh uses the right arrow key, which is way too far away, I use Ctrl-Space insted). fish has these built in, and there is an extension for zsh. As far as I know, bash doesn\u0026rsquo;t have autosuggestions at the moment. autojump autojump watches the directories you visit in your shell and maintains a database of which ones you use the most. Then you can type $ j \u0026lt;part of directory path\u0026gt; and autojump will try to guess which directory you want to go to and cd there. A couple of letters from the directory name are usually enough for that. This is a good alternative to fzf\u0026rsquo;s Alt-C search. Syntax highlighting This just means different parts of your command are colored differently, much more pleasant to work with. Again, fish has this built in, for zsh you can use another extension, and I don\u0026rsquo;t think there\u0026rsquo;s an easy way to do this in bash. !! !! expands to the previous command. So for example, $ sudo !! reruns your previous command as a super user. Output coloring Many shell commands can color their output but have this disabled by default. In particular:  alias ls='ls --color=auto' to always have colored ls output bat is a cat replacement with syntax highlighting among other things. It can also be used to color man pages The ArchWiki contains many more cases (most of them not specific to Arch)   Color theme Most terminal emulators allow choosing your own color theme, and there are configurations online for most pairs of terminal emulator and popular color theme. So you can for example use the same theme you use inside your editor for your terminal as well.  Zathura Zathura is a lightweight PDF viewer with vim keybindings. I\u0026rsquo;m a big fan; in case you are as well, here are two tips to make it even better:\n  When you\u0026rsquo;re on e.g. page 10 of a PDF but the page number in the footer is 1 (because of title pages etc.), type :offset 9 to tell Zathura about this mismatch. Whenever you type \u0026lt;page number\u0026gt; G later, Zathura will subtract this offset and you will be on the page with \u0026lt;page number\u0026gt; in the footer. Extremely useful if the PDF has no hyperlinked table of contents. Zathura remembers this setting for each file.\n  Recoloring allows you to give PDFs a custom foreground and background color. For example, you could display PDFs as light text on a dark background. Or use the following in your zathurarc, which displays PDFs in a Solarized light color scheme:\nset recolor set recolor-darkcolor \u0026quot;#586e75\u0026quot; set recolor-lightcolor \u0026quot;#fdf6e3\u0026quot; set recolor-keephue  The last line means that the colors of images should be preserved (though they\u0026rsquo;ll be less vibrant). This works for all pdfs, even scanned images! Ctrl-R will toggle recolorization on and off, in case you want to switch back to the original. For the ideal visual experience, you can also set the color of all the GUI elements, see https://github.com/lennonwoo/zathura-solarized for a Solarized version.\n  ","date":1617194940,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617194940,"objectID":"65d07f9dd600b716c9117954ee15c8e9","permalink":"https://ejenner.com/post/computer-tips/","publishdate":"2021-03-31T14:49:00+02:00","relpermalink":"/post/computer-tips/","section":"post","summary":"  Many of us spend a lot of time working with our computer, so it's worth\n  spending some time to make that experience as pleasent and productive\n  as possible. This is a collection of tips that are relatively quick\n  to implement and still very valuable in the long run in my opinion.\n  Mainly geared towards developers and others who work with the shell\n  a lot.\n  ","tags":["Productivity"],"title":"Collection of quick computer tips","type":"post"},{"authors":null,"categories":null,"content":"Evan Chen\u0026rsquo;s Infinitely Large Napkin is my go-to resource when I want to learn about some new area of mathematics (or at least it used to be; I\u0026rsquo;m increasingly often running into the issue that it doesn\u0026rsquo;t have a chapter about what I want to learn \u0026ndash; at barely over 900 pages it\u0026rsquo;s just way too short). I recently asked myself what it is about the Napkin that I like so much, and this post is part of my answer.\nNote that I\u0026rsquo;m describing the way I like to learn math, not the objectively best way. If you happen to like learning that way too, maybe this post can give you a more explicit idea of what \u0026ldquo;that way\u0026rdquo; is. Otherwise, it will at least give you some insight into the brain of someone who learns differently than you do, and maybe that will help if you want to teach math to other people.\nOne way of teaching mathematical concepts is the approach typically employed by textbooks: give some definitions, probably a few examples, state a few theorems, prove them. In some cases, you might also get an intuitive explanation of what these definitions are all about or why a theorem is interesting or how the proof roughly works. But often, these explanations are sparse and confined to the beginning of a section, so as not to dilute the mathematical purity of the remaining text.\nAnother style of explanation is the one that consists almost entirely of hand-waving. Evan Chen describes it as follows:\n Someone tells you about the hairy ball theorem in the form “you can’t comb the hair on a spherical cat”, then doesn’t tell you anything about why it should be true, what it means to actually “comb the hair”, or any of the underlying theory, leaving you with just some vague notion in your head.\n We could think of these two approaches as opposite ends of a \u0026ldquo;rigor\u0026rdquo;-spectrum. Somewhere in the middle, you might have the way that theoretical physics is often taught: few things are formally defined, but concepts are at least made explicit enough that you are able to use them in calculations, and equations are usually derived (though less rigorously than in math).\nWhere does the Napkin fit in on this axis? Is it more or less rigorous than theoretical physics courses? I think that\u0026rsquo;s the wrong question to ask because the one-dimensional model of approaches to teaching math is too simplistic. The space of all the ways you could explain mathematical concepts is very large and treating it as one-dimensional isn\u0026rsquo;t such a great approximation (though I think that the \u0026ldquo;rigor\u0026rdquo;-axis might be the best one can do with only one dimension).\nSo what\u0026rsquo;s a better model? I want to suggest thinking about mathematical explanations using a two-dimensional approximation. On one axis, we have the rigor used when stating definitions or results, while the other axis is the rigor of the derivations of these results. Textbooks have both formal statements and formal derivations, casual hand-wavy explanations have neither. Physics is somewhere in between, though to me the derivations often seem more rigorous than the statements1. Finally, the Napkin takes the opposite approach: it states things with essentially the same rigor as textbooks but then places emphasis on very informal derivations or explanations of why these statements are true.\n  Figure 1: Various styles for teaching mathematics. Yes, it would look better as a TikZ figure, but it was either this or nothing.  I find that this approach \u0026ndash; stating things formally but reasoning about them informally \u0026ndash; works extremely well for me when I first learn about a subject. I\u0026rsquo;m happy to take someone\u0026rsquo;s word that a statement is true, at least initially and if the statement seems like it should be true. But if I don\u0026rsquo;t know precisely what the statement is, I find it much harder to get to grips with the subject.\nAs a caveat, note that I said \u0026ldquo;when I first learn about a subject\u0026rdquo;. This approach doesn\u0026rsquo;t teach how to write formal proofs, and it\u0026rsquo;s by design not as comprehensive as a textbook. But sometimes an intuitive understanding is enough for my purposes, and even if I know I\u0026rsquo;ll later want to learn a subject in more depth, I find it helpful to build this intuitive understanding before going through all the details.\n  This sounded a bit absurd to me when writing it down \u0026ndash; how can you rigorously derive something you haven\u0026rsquo;t even really stated? But I think there\u0026rsquo;s some truth to it, and what I mean is roughly this: the calculations done in physics courses are often essentially the same that you\u0026rsquo;d do for a formal proof. But the objects used in those calculations aren\u0026rsquo;t formally defined, it\u0026rsquo;s just taken for granted that everyone has a sense of what they are and how they behave (or maybe it\u0026rsquo;s explicitly stated how they behave, but not whether they are uniquely characterized by that behavior).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1616580000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616580000,"objectID":"e5c187e8ace2517605939e939b0d8a70","permalink":"https://ejenner.com/post/state-formally-reason-informally/","publishdate":"2021-03-24T11:00:00+01:00","relpermalink":"/post/state-formally-reason-informally/","section":"post","summary":"  There's a style of teaching mathematics that I really like: stating definitions\n  and theorems as formally as in any textbook, but focusing on informal arguments\n  for why they should be true.\n  ","tags":["Math"],"title":"State formally, reason informally","type":"post"},{"authors":null,"categories":null,"content":"The purpose of this post is to point you towards some great features and packages if you\u0026rsquo;re already using Emacs to edit LaTeX, and to make you jealous if you\u0026rsquo;re using some other editor1.\nThis isn\u0026rsquo;t a tutorial for Emacs or even a tutorial on how to write LaTeX inside Emacs. Rather, it\u0026rsquo;s supposed to give an idea of what\u0026rsquo;s possible, either as inspiration or to convince you to give Emacs a try. One problem is that setting all of this up can be a huge time sink, so you might want to use a framework such as Doom, where you just need to enable the LaTeX module and get almost everything I describe here.\nThe basics Of course you get all the basics you would expect from a LaTeX editor. Synctex is supported (meaning you can jump from a certain line in the LaTeX code to the corresponding place inside your PDF viewer and the other way around), you can compile files from inside emacs, you can jump to compilation errors if there are any, there is auto-completion and so on.\nVisuals LaTeX can produce beautiful documents, but the source code isn\u0026rsquo;t very readable when writing mathematical expressions:\n\\alpha \\mapsto \\int_{\\R}e^{-\\alpha x^2}\\,dx  Emacs and AUCTeX (which is the de-facto standard package for using LaTeX inside Emacs) have several features that improve this situation:\n preview-latex replaces equations (and other parts of the LaTeX document) with images by compiling them. This means they look exactly the same inside the editor as they will in the compiled document. When the cursor is on an equation, this image preview is automatically replaced by the underlying text so you can still easily edit equations. However, this method of course has a noticeable delay because it requires a call to the LaTeX compiler. LaTeX superscripts and subscripts are displayed as super-/subscripts inside the editor. This is a purely visual feature, editing them doesn\u0026rsquo;t require \u0026ldquo;entering\u0026rdquo; or \u0026ldquo;exiting\u0026rdquo; the subscript or anything like that. prettify-symbols-mode allows you to replace any string with any unicode symbol. AUCTeX comes with a fairly comprehensive predefined list, which replaces LaTeX commands such as greek letters, arrows and others with symbolic representations. But you can also add your own. For example, the example above uses \\R, which my custom style file defines as \\mathbb{R}, and it\u0026rsquo;s possible to add replacement rules for such custom commands (as long as there is a fitting Unicode symbol). This makes the line above look like this in my editor:   When the cursor moves over one of those Unicode symbols, it is expanded to the underlying text. And the nice thing about this is that it\u0026rsquo;s essentially instantaneous because nothing needs to be compiled. Folding is something similar but more general (though unlike prettify-symbols-mode it\u0026rsquo;s specific to LaTeX). It doesn\u0026rsquo;t just allow replacing fixed strings but also more complicated expressions. By default, this is used for example to display \\label{some_label} as [l] (which as always expands when the cursor moves over it). The reasoning here is that some elements such as labels are just distractions when reading LaTeX source code. But you can also use this to further improve how math is displayed, see this config for some ideas (and in general for more ideas on how to beautify LaTeX inside Emacs).  Editing AUCTeX has a couple of nice features that make typing LaTeX a bit easier. For example, you can let it automatically insert braces {} when typing _ or ^ inside a math environment, you can let it insert \\(\\) when typing a dollar sign, and even \\enquote{} when typing \u0026quot; 2.\nBut things get even better with the evil-tex package. As the name suggests, this is only relevant if you\u0026rsquo;re using evil-mode (vim keybindings inside emacs), but if so, it\u0026rsquo;s definitely worth trying. Just a few examples of what this allows you to do:\n  Say you\u0026rsquo;ve typed\n\\(ax^{2} + b\\)  and suddenly realize that this is supposed to go into an exponent. With your cursor anywhere on this math environment, type ysim^ (\u0026ldquo;surround everything inside the math environment as an exponent\u0026rdquo;) and you\u0026rsquo;ll get\n\\(^{ax^{2} + b}\\)  with the cursor at the ^. Now you just need to enter the base.\n  Your equations is now\n\\(e^{ax^{2} + b}\\)  and you decide that this merits its own displayed rather than inline equation. So you type csmee (\u0026ldquo;change the surrounding math environment to equation\u0026quot;) and get\n\\begin{equation} e^{ax^{2} + b} \\end{equation}    After a bit more editing, you have (for some reason)\n\\begin{equation} \\beta(e^{ax^{2} + b} + \\frac{1}{x}) \\end{equation}  Of course this looks ugly in the compiled document, you need to use \\left( and \\right). With evil-tex, you can just type mtd (\u0026ldquo;toggle delimiter\u0026rdquo;) with the cursor anywhere inside the parantheses, and it will add \\left and \\right for you. Type mtd again to go back to just the parantheses.\n  Emacs calc\u0026rsquo;s embedded mode calc is the built-in calculator for Emacs; though saying \u0026ldquo;calculator\u0026rdquo; is a bit misleading because it can do symbolic differentiation, unit conversion, linear algebra and more. If your press C-x * e with your cursor on any LaTeX equation, you will start calc in \u0026ldquo;embedded mode\u0026rdquo;. This means that calc will parse the LaTeX code and then let you do any calculations you want involving the expression. The result will automatically be converted back to LaTeX and written into the buffer.\nFor example, say you have\n\\[\\sin\\left( x^2 + \\sqrt{x} \\right)\\]  and want to know the derivative. You can enter embedded mode and type ad to differentiate, then type x when prompted for the variable with respect to which to differentiate. And just like that, you will have\n\\[\\left( 2 x + \\frac{0.5}{\\sqrt{x}} \\right) \\cos\\left( x^2 + \\sqrt{x} \\right)\\]  inside your buffer. calc can even parse and output things like \\begin{pmatrix}...\\end{pmatrix}, so you can multiply matrices as well.\nAnd more I\u0026rsquo;ve only covered some of my personal favorite features when it comes to writing LaTeX inside Emacs, there\u0026rsquo;s much more. For example, LaTeX-math-mode allows you to very quickly enter mathematical symbols and RefTeX as well as other packages make handling references, labels and citations very efficient. And of course there are a gazillion other packages that can make writing LaTeX easier \u0026ndash; this is Emacs after all.\nThe downside is of course that there is a pretty steep learning curve. But for people who need to write LaTeX documents all the time, I\u0026rsquo;d argue it\u0026rsquo;s worth it.\n  Though mostly the first part \u0026ndash; I haven\u0026rsquo;t actually tried many others and they may be just as amazing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Using TeX-electric-sub-and-superscript, TeX-electric-math, and LaTeX-csquotes-open-quote / LaTeX-csquotes-close-quote\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1615986840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615986840,"objectID":"f3eb33525f848a2903798bff9fa23b61","permalink":"https://ejenner.com/post/latex-emacs/","publishdate":"2021-03-17T14:14:00+01:00","relpermalink":"/post/latex-emacs/","section":"post","summary":"  Emacs has some really amazing features for writing LaTeX; this post gives\n  an overview of some of them, either to convince you to give Emacs a try,\n  or to make you aware that these features exist if you're already using\n  Emacs but didn't know about them.\n  ","tags":["Productivity"],"title":"Emacs as an amazing LaTeX editor","type":"post"},{"authors":null,"categories":null,"content":"Spherical harmonics appear in lots of different places and have different interpretations that at first sight don\u0026rsquo;t seem to have anything to do with one another. In this post, I\u0026rsquo;ll try to connect three very common ones (namely as harmonic polynomials, as eigenfunctions of the Laplacian and as irreps of \\(\\operatorname{SO}(3)\\)).\nWe\u0026rsquo;re going to define spherical harmonics as homogeneous harmonic polynomials \\(\\mathbb{R}^3 \\to \\mathbb{C}\\). Let\u0026rsquo;s break this down:\n A polynomial of three variables is a finite sum of the form \\[\\sum_{\\alpha} a_\\alpha x^{\\alpha_x}y^{\\alpha_y}z^{\\alpha_z}\\] over multi-indices \\(\\alpha \\in \\mathbb{N}_0^3\\). Some examples are \\(x^2y + 2z\\) or \\(xyz + y^2\\). The coefficients \\(a_\\alpha\\) can be complex numbers, but we will only plug in real numbers for \\(x\\), \\(y\\) and \\(z\\). That\u0026rsquo;s why we interpret polynomials as functions \\(\\mathbb{R}^3 \\to \\mathbb{C}\\). Homogeneous mean that \\(\\alpha_x + \\alpha_y + \\alpha_z\\) is the same for all the \\(\\alpha\\) we sum over, so all the terms in the sum have the same degree. For example, \\(x^2 + 2yz + xz\\) is homogeneous, while \\(xy + z\\) is not. Harmonic means that the Laplacian of the polynomial vanishes: \\(p\\) is harmonic if \\(\\Delta p = 0\\). Here, \\(\\Delta = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} + \\frac{\\partial^2}{\\partial z^2}\\).  We will write \\(\\mathcal{H}_l\\) for the space of all homogeneous harmonic polymonials of degree \\(l\\) (meaning \\(\\alpha_x + \\alpha_y + \\alpha_z = l\\) for all summands).\nIf you\u0026rsquo;ve seen spherical harmonics before (and you presumably have, if you\u0026rsquo;re reading this post), it\u0026rsquo;s probably been in the form of functions \\(Y_l^m(\\theta, \\varphi)\\) defined on the sphere. So why are we talking about these polynomials on \\(\\mathbb{R}^3\\) instead?\nThe answer is that every polynomial \\(p \\in \\mathcal{H}_l\\) can be written in spherical coordinates as \\[p(x, y, z) = r^l Y(\\theta, \\varphi)\\] for some function \\(Y: S^2 \\to \\mathbb{C}\\). To see why, write \\(x\\), \\(y\\) and \\(z\\) in spherical coordinates and plug them into the polynomial. They each have a factor of \\(r\\) and then some factors depending on \\(\\theta\\) and \\(\\varphi\\). So because \\(p\\) is homogeneous, each summand consists of a factor \\(r^l\\) times something that depends only on \\(\\theta\\) and \\(\\varphi\\). So we can think of homogeneous polynomials as polynomials defined on the sphere \u0026ndash; their continuation to \\(\\mathbb{R}^3\\) is automatically determined by their degree \\(l\\). Therefore, we won\u0026rsquo;t really distinguish between homogeneous harmonic polynomials defined on \\(\\mathbb{R}^3\\) and their restrictions to \\(S^2\\), we will refer to both as spherical harmonics.\nThis should also explain the name: spherical harmonics are harmonic polynomials living on the sphere.\nThe functions \\(Y_l^m\\) that you may have seen are just a particular choice of basis for the vector space of spherical harmonics. If you multiply them by \\(r^l\\), you get polynomials in \\(\\mathcal{H}_l\\), and \\[\\{r^l Y_l^m| -l \\leq m \\leq l\\}\\] is a basis for \\(\\mathcal{H}_l\\).\nEigenfunctions of the Laplacian One of the reasons that spherical harmonics are so ubiquitous is that they are the eigenfunctions of the spherical Laplacian \\(\\Delta_{S^2}\\). They key to that is the following fact (which is just a brief calculation): for a function \\(Y: S^2 \\to \\mathbb{C}\\), \\[\\Delta (r^l Y) = r^{l - 2}\\left(l(l + 1)Y + \\Delta_{S^2}Y\\right)\\thinspace.\\] So \\(r^l Y(\\theta, \\varphi)\\) is harmonic if and only if \\[\\Delta_{S^2}Y = -l(l + 1)Y\\thinspace.\\] This already proves that spherical harmonics are eigenfunctions of the spherical Laplacian.\nBut we can say more than that: if we take any eigenfunction \\(f: S^2 \\to \\mathbb{C}\\) of the spherical Laplacian and multiply by \\(r^l\\) (with \\(l\\) such that \\(-l(l + 1)\\) gives the eigenvalue1), then \\(r^l f(\\theta, \\varphi)\\) must be harmonic. So the eigenfunctions of the spherical Laplacian are in fact in 1-to-1 correspondence with harmonic homogeneous functions on \\(\\mathbb{R}^3\\). It then turns out \u0026ndash; and this part is far from obvious \u0026ndash; that all such functions are polynomials2! So the spherical harmonics aren\u0026rsquo;t just eigenfunctions of the spherical Laplacian, they make up all of its eigenfunctions.\nIrreducible representations of \\(\\operatorname{SO}(3)\\) Another famous role that spherical harmonics play is as the irreducible representations of \\(\\operatorname{SO}(3)\\) (more precisely: the (complex) irreducible representations of \\(\\operatorname{SO}(3)\\) are exactly the spaces \\(\\mathcal{H}_l\\)). This is connected to the fact that they are the eigenfunctions of the spherical Laplacian.\nThat the eigenspaces of the spherical Laplacian are representations of \\(\\operatorname{SO}(3)\\) follows directly from the fact that the Laplacian commutes with rotations: we have a representation \\(G \\curvearrowright L^2(S^2, \\mathbb{C})\\) via \\[(r \\cdot f)(x) := f(r^{-1}x)\\] for any rotation \\(r\\) and \\(f \\in L^2(S^2, \\mathbb{C})\\). For an eigenfunction of the Laplacian, we get \\[\\Delta_{S^2}(r \\cdot f) = r \\cdot \\Delta_{S^2} f = r \\cdot \\lambda f = \\lambda (r \\cdot f)\\thinspace,\\] so each eigenspace is invariant under the action of \\(\\operatorname{SO}(3)\\). Therefore, the representation on \\(L^2(S^2)\\) can be restricted to each eigenspace, so each \\(\\mathcal{H}_l\\) gives a representation of \\(\\operatorname{SO}(3)\\).\nShowing that these representations are in fact irreducible is much more difficult (there\u0026rsquo;s a proof here for example, if you really want to dive into that). But if we just take that for granted, it\u0026rsquo;s again easy to show that every irreducible subrepresentation of \\(L^2(S^2)\\) is a space of spherical harmonics: because the Laplacian is an equivariant map on each such representation, Schur\u0026rsquo;s Lemma implies that it must be either the zero map (which it isn\u0026rsquo;t) or multipication by a constant \\(\\lambda \\in \\mathbb{C}\\). Therefore, each irreducible representation is contained in an eigenspace of the Laplacian. But these eigenspaces are themselves irreducible, so the representation in question must already be equal to the eigenspace.\nFinally, it\u0026rsquo;s possible to show that all irreducible representations of \\(\\operatorname{SO}(3)\\) are subrepresentations of \\(L^2(S^2)\\). This is again much more difficult and is also a very special fact about \\(\\operatorname{SO}(3)\\) (for example, the Laplacian\u0026rsquo;s eigenspaces are still irreducible representations in higher dimensions, but they are not the only ones anymore). But combining this with our results from above, the spherical harmonics make up all the irreducible representations of \\(\\operatorname{SO}(3)\\).\n  I\u0026rsquo;m skipping over some details here, see for example Claim 4.0.1 here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n See Corollary 4.0.6 in the same document for a proof\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1615392420,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615392420,"objectID":"78933a14cf291c6fdecf2edf63a600cc","permalink":"https://ejenner.com/post/spherical-harmonics/","publishdate":"2021-03-10T17:07:00+01:00","relpermalink":"/post/spherical-harmonics/","section":"post","summary":"  Spherical harmonics are ubiquitous in math and physics, in part because\n  they naturally appear as solutions to several problems; in particular they\n  are the eigenfunctions of the spherical Laplacian and the irreducible\n  representations of SO(3). But why should the solutions to these problems\n  be the same? And why are they called spherical harmonics?\n  ","tags":["Math"],"title":"Perspectives on spherical harmonics","type":"post"},{"authors":null,"categories":null,"content":"Implicit Layers Layers in neural networks are almost exclusively explicitly specified. That just means that the output \\(y\\) is described as a (usually rather simple) function of the input \\(x\\) and some parameters \\(\\theta\\), i.e. \\[y = f(x; \\theta)\\thinspace.\\]\nThe idea behind implicit layers is the following:\n Instead of specifying how to compute the layer’s output from the input, we specify the conditions that we want the layer’s output to satisfy.\n (that quote is from the Implicit layers tutorial given by Zico Kolter, David Duvenaud and Matt Johnson at NeurIPS 2020, on which this post is based. I definitely recommend you check it out if you\u0026rsquo;re interested in all the details that I\u0026rsquo;ll skip over)\n\u0026ldquo;Conditions that we want the output to satisfy\u0026rdquo; is a bit vague, what does this look like concretely? Well, it\u0026rsquo;s vague on purpose because implicit layers are a very general framework. But one simple way to specify a condition is with \\[g(x, y; \\theta) = 0\\thinspace.\\] Whereas a forward pass in a neural network classically means applying the function \\(f\\) at each layer, we now need to solve this equation for \\(y\\) \u0026ndash; that solution will be the output of our layer.\nThis may sound a bit insane. Isn\u0026rsquo;t solving an equation like that much more expensive than just applying an explicit function? Why would you want to do this during each forward pass? There\u0026rsquo;s some truth to that of course but it\u0026rsquo;s not as bad as it may sound. First, it turns out that relatively few or even just one implicit layer are often enough, so while each layer is more expensive to compute, you need fewer of them. And secondly, it\u0026rsquo;s sometimes possible to ensure that the equation describing the layer can be solved reasonably easily. After all, we have control over the class of equations that we need to solve by choosing the network architecture.\nSo it\u0026rsquo;s not as bad as it could be, but still, what to we gain? One very general answer is that implicit layers can be very expressive. Even using a very simple function \\(g\\), for example, the implicit function \\(x \\mapsto y\\) defined by solving the equation may be quite complex (this ties into the fact that one or a few implicit layers are often enough to do the job). On an even more abstract level, implicit layers decouple what properties we want the output to have from how to compute the output. The learned parameters only need to describe some conditions that the output should satisfy and we can then use any method we want to actually find such an output.\nOne detail I\u0026rsquo;ve quietly swept under the rug is whether a solution \\(y\\) even exists and whether it is unique. And for the most part, I\u0026rsquo;m going to continue ignoring this issue because this is meant to be a relatively informal introduction. I\u0026rsquo;ll just mention that in some cases, you actually get existence and uniqueness guarantees, and in others, you can still hope that it works out empirically.\nBut what about my gradients? So you\u0026rsquo;ve specified a model architecture (the function \\(g(x, y; \\theta)\\)) and you have some method for doing a forward pass (i.e. finding a \\(y\\) such that \\(g(x, y; \\theta) = 0\\) for a given input \\(x\\)). Now you want to train your model with gradient descent. But you don\u0026rsquo;t have any explicit function to take the gradient of, so how does that work?\nYou could backpropagate through your solver: after all, you computed the output \\(y\\) somehow, in principle you could backpropagate through that calculation. But that\u0026rsquo;s inefficient, so let\u0026rsquo;s try to find a better way.\nTo simplify the notation, we\u0026rsquo;ll ignore the parameters \\(\\theta\\). You can think of them as being a part of the input \\(x\\) \u0026ndash; for our purposes there\u0026rsquo;s really not much difference between the input to the layer and the parameters, since we need gradients with respect to both.\nSo for a given input \\(x\\), we now want to find the Jacobian \\(\\frac{\\partial y^*}{\\partial x}\\), where \\(y^*\\) is the output such that \\(g(x, y^*) = 0\\). We can think of \\(y^*\\) as a function of \\(x\\): for each input \\(x\\), we have some solution \\(y^*(x)\\). To find the Jacobian of \\(y^*\\), we can use implicit differentiation. We know that \\(g(x, y^*(x)) = 0\\) for all \\(x\\), so if we read the LHS as a function of \\(x\\), it\u0026rsquo;s just the constant zero function. The derivative of the zero function is of course also 0, so \\(\\frac{d}{dx} g(x, y^*(x)) = 0\\). On the other hand, we can apply the chain rule, \\[\\frac{d}{dx}g(x, y^*(x)) = \\frac{\\partial g}{\\partial x} + \\frac{\\partial g}{\\partial y}\\frac{d y^*}{dx}\\thinspace.\\] Since this expression has to be zero, we can rearrange and get \\[\\frac{\\partial g}{\\partial y}\\frac{d y^*}{dx} = -\\frac{\\partial g}{\\partial x}\\thinspace,\\] which we can further rewrite as1 \\[\\frac{d y^*}{dx} = -\\left(\\frac{\\partial g}{\\partial y}\\right)^{-1}\\frac{\\partial g}{\\partial x}\\thinspace.\\] So now we have the Jacobian of \\(y^*\\) in terms of the Jacobian of \\(g\\), which means we can calculate gradients without backpropagating through the solver.\nMatrix inversion considered harmful There\u0026rsquo;s one remaining issue: while we now know how to calculate the Jacobian \\(\\frac{d y^*}{dx}\\), doing so requires us to invert a matrix, which is expensive. Luckily, we don\u0026rsquo;t actually need to explicitly compute the Jacobian for gradient descent. What we ultimately care about is the gradient of the loss, which is a scalar function. For example, if the implicit layer described by \\(g\\) is the last one before the loss function \\(L\\), we want \\[\\frac{dL}{dx} = \\frac{dL}{dy}\\frac{dy^*}{dx}\\thinspace,\\] where \\(\\frac{dL}{dy}\\) is the gradient of \\(L\\) as a row vector. More generally, we only need to be able to compute products of the form \\[w^T \\frac{dy^*}{dx} = -w^T\\left(\\frac{\\partial g}{\\partial y}\\right)^{-1}\\frac{\\partial g}{\\partial x}\\thinspace,\\] not the Jacobian itself. We can do this by first solving \\[u^T\\frac{\\partial g}{\\partial y} = -w^T\\] for \\(u\\), the so called adjoint variable. Crucially, this is possible without explicitly computing and storing the inverse (for example using iterative methods). Then we can calculate \\[w^T \\frac{dy^*}{dx} = u^T\\frac{\\partial g}{\\partial x}\\thinspace.\\]\nWhat\u0026rsquo;s next? We\u0026rsquo;ve seen some of the basic ideas and themes surrounding implicit layers. Instead of explicitly describing how to compute the output, they specify a condition that the output should satisfy. Using implicit differentiation, we can still effectively backpropagate through these layers, independent of the solver we use in the forward pass.\nBut we haven\u0026rsquo;t talked at all about concrete instantiations of implicit layers. What would a network using these actually look like? And can we use contraints other than those of the form \\(g(x, y; \\theta) = 0\\)? All of that and more is discussed in the implicit layers tutorial that I already mentioned above. It starts with Deep equilbrium models (which essentially use layers defined by \\(g(x, y) = 0\\) as in this post, just framed differently) but then applies the same ideas to constraints described by ODEs, leading to Neural ODEs.\n  This is assuming that the Jacobian \\(\\frac{\\partial g}{\\partial y}\\) is invertible. This is exactly the condition under which the implicit function theorem holds. In that case, \\(y^*\\) is indeed differentiable (so we can apply the chain rule as we did). And if we drop the assumption that \\(g(x, y) = 0\\) is uniquely solvable, then the implicit function theorem at least guarantees that a unique solution \\(y*(x)\\) exists locally around a point \\((x_0, y_0)\\). So this theorem is sort of a theoretical backbone, which guarantees that what we do actually works. But if we assume that a solution function \\(y^*\\) exists and is differentiable, then computing it\u0026rsquo;s Jacobian doesn\u0026rsquo;t require any heavy machinery: as you can see, we just apply the chain rule once and then rearrange a bit.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1614779280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614779280,"objectID":"e1c190c8c312e57a66371180b06dd5d5","permalink":"https://ejenner.com/post/implicit-layers/","publishdate":"2021-03-03T14:48:00+01:00","relpermalink":"/post/implicit-layers/","section":"post","summary":"  Several new architectures for neural networks, such as Neural ODEs and\n  deep equlibirum models can be understood as replacing classical layers\n  that explicitly specify how to compute the output with implicit layers.\n  These layers describe which conditions the output should specify but\n  leave the actual computation up to some solver that can be chosen arbitrarily.\n  This post contains a brief introduction to the main ideas behind implicit layers.\n  ","tags":["Deep learning"],"title":"Deep Implicit layers","type":"post"},{"authors":null,"categories":null,"content":"In Part 1 and Part 2, we\u0026rsquo;ve seen different methods for learning good policies. One thing that all of them had in common was that they only used trajectories sampled from the environment to do so. This is what\u0026rsquo;s called model-free RL. In this final post, we will generalize to model-based RL, where we make use of a learned model of the environment to improve the training process.\nThe 10,000-mile satellite\u0026rsquo;s-eye view on RL From very far away, we can treat all the methods we\u0026rsquo;ve previously seen as functions that map a trajectory and a current parameter to an update for that parameter. The parameter could describe a value function or a policy. Training an agent means repeatedly sampling a trajectory, calculating that update, and updating the parameter.\nIt will be useful to think about this from the lense of types: an update method is a function that takes in an object of type \u0026ldquo;trajectory\u0026rdquo; and one of type \u0026ldquo;parameters\u0026rdquo; and returns an update of type \\(\\Delta\\text{parameters}\\)1: \\[\\text{trajectory} \\times \\text{parameters} \\to \\Delta\\text{parameters}\\] Often, we can decompose this function. For example, a 1-step method such as Sarsa calculates update based on individual \\((s, a, r, s, a)\\) tuples, which we\u0026rsquo;ll call \u0026ldquo;experience\u0026rdquo;. So Sarsa really defines a function with signature \\[\\text{experience} \\times \\text{parameters} \\to \\Delta\\text{parameters}\\] We then get the function signature from above by splitting up a trajectory into its underlying experience tuples, applying the Sarsa update to each one, and summing the results. This is a simple example but it illustrates the main idea of this post: to compose small functions in different ways in order to get complete RL algorithms.\nAs a slightly more complex example, consider Actor-Critic methods. We use a policy optimization method (the actor) with a signature such as \\[A: \\text{trajectory} \\times \\text{actor-param} \\times \\text{V-function} \\to \\Delta\\text{actor-param}\\] At the same time, we use some method for learning value functions, which has the signature \\[C: \\text{trajectory} \\times \\text{critic-param} \\to \\Delta\\text{critic-param}\\] And finally the model for the critic, which can be written as \\[V: \\text{critic-param} \\to \\text{V-function}\\] We can combine these functions using some pretty simple boilerplate code, to get a function with the \\(\\text{trajectory} \\times \\text{params} \\to \\Delta\\text{params}\\) signature that we want, where \\(\\text{params} := \\text{actor-param} \\times \\text{critic-param}\\) is the type of the complete collection of parameters. In Python, this might look as follows:\ndef train(trajectory, params): actor_param, critic_param = params v_function = V(critic_param) actor_update = A(trajectory, actor_param, v_function) critic_update = C(trajectory, critic_param) return (actor_update, critic_update)  None  You should read this more as pseudo-code: the point is not that we would actually implement an agent exactly like this, but just to show how these individual functions come together to define an update for the entire agent.\nThe code above is completely agnostic to the choice of \\(A, C\\) and \\(V\\), which is an important point throughout this post: we only care about the function signatures of the methods we use as building blocks, not about how they work internally.\nA complete training loop Our ultimate goal is not to compute updates but to find a good policy. For that we need two more components. First, a function \\[\\text{parameters} \\to \\text{policy}\\] In the case of policy optimization methods, this is simply the parameterization of the policy, i.e. the model of the actor. If we use value-based methods, this can instead be decomposed into the value model \\[\\text{parameters} \\to \\text{Q-function}\\] and a function that determines the policy based on the Q-estimate, e.g. an \\(\\varepsilon\\)-greedy policy, \\[\\text{Q-function} \\to \\text{policy}\\]\nThe second thing we need is a function that samples trajectories \u0026ndash; this is the role of the environment: \\[\\text{policy} \\to \\text{trajectory}\\] Then we can combine all of these into one function with the signature2 \\[\\text{parameters} \\to \\text{policy}\\] which takes initial parameter values and then trains a policy (until convergence or some stopping criterion).\nModeling the environment The only role played by the environment in our training algorithm is to provide a function \\[\\text{policy} \\to \\text{trajectory}\\] for sampling trajectories. But remember our motto: we only care about function signatures, not the internals of the functions themselves. So if we can define a function like that in some other way, we can plug it into our training algorithm without changing anything else.\nBefore we discuss how to define such a function, let\u0026rsquo;s first take a step back and consider how this signature is actually implemented by the environment. An environment is defined by its transition probabilities \\[p(s', r|s, a)\\] i.e. the probability that the next state will be \\(s'\\) and the reward \\(r\\) if action \\(a\\) is taken in state \\(s\\). So this defines a function \\[\\text{state} \\times \\text{action} \\to D(\\text{state} \\times \\text{reward})\\] where \\(D(\\cdot)\\) denotes distributions with values of a given type. We can call this the distributional function defined by the environment.\nWe can sample from distributions, meaning we have a function \\(D(x) \\to x\\) for any type \\(x\\). By composing this with the environment function, we get the signature \\[\\text{state} \\times \\text{action} \\to \\text{state} \\times \\text{reward}\\] which we\u0026rsquo;ll call the sample function induced by the distributional function from above.\nThe policy type is not an atomic type, it can itself be written as \\[\\text{state} \\to D(\\text{action})\\] and by composing with the sampling map, defines a function \\(\\text{state} \\to \\text{action}\\). That makes it clear how we can get the desired signature \\(\\text{policy} \\to \\text{trajectory}\\) from the sample function. We start with some initial state (perhaps also sampled from a distribution), then apply the policy to get an action, then apply the environment function to get a reward and a new state. Repeat until the episode ends.\nSo an environment defines three different functions:\n Distributional function: \\(\\text{state} \\times \\text{action} \\to D(\\text{state} \\times \\text{reward})\\) Sample function: \\(\\text{state} \\times \\text{action} \\to \\text{state} \\times \\text{reward}\\) Trajectory function: \\(\\text{policy} \\to \\text{trajectory}\\)  where each one is induced by the one above it in a natural way.\nThis gives us three different levels on which our environment model could work: we could model any of these three functions and will at the end get out a trajectory model \\(\\text{policy} \\to \\text{trajectory}\\) to plug into our training algorithm. That said, if we have a distributional or sample model, we can do somewhat more clever things with that, which we\u0026rsquo;ll talk about in a moment.\nWhere do models come from? We\u0026rsquo;ve seen how we can plug in models into the training loop to replace the role of the environment. This will always give us a valid method (in the sense that it doesn\u0026rsquo;t have type errors) but that method will only be useful to the extent to which the model matches the environment.\nSo how do we get a good model? In some cases it might be feasible to hard-code one. More generally, we can treat this as a supervised learning problem. In that case, we use the real environment to gather trajectories and then use those trajectories to train our model. We can then either use only the model to train the agent, or use both real and simulated trajectories for that.\nThis means we need to extend the training loop, which now also has to train the model. This is where we make choices such as how many trajectories to simulate per real trajectory that trains the model etc.\nWhat else can we do with models? As hinted before, using a model to replace the environment (by simulating trajectories) is only one of many applications.\nFirst, let\u0026rsquo;s consider sample models (i.e. models that can generate \\((s', r)\\) tuples for any state-action input). As we saw at the beginning of this post, many RL methods don\u0026rsquo;t need entire trajectories to learn, they instead work on smaller sequences of experience tuples. For example, 1-step methods compute updates based on single \\((s, a, r, s, a)\\) tuples. One immediate consequence is that we don\u0026rsquo;t need to simulate an entire trajectory before updating the policy or value function, we can do so after however many experience tuples we want.\nA more interesting advantage of such methods is that we can use prioritized sweeping. We saw that a sample model can create trajectories by using its output state as the next input. But no one is forcing us to use that state as the input! Instead of sampling cohesive trajectories, we can at each step sample experience tuples for those states or state-action pairs where we\u0026rsquo;re most uncertain (e.g. as measured by the size of the last update for those states).\nDistributional models allow us to do even more. In particular, we can apply dynamic programming techniques (value or policy iteration), which we briefly mentioned in Part 1. You may also recall that for control, we usually need to learn Q-functions, because a V-function on its own doesn\u0026rsquo;t tell us which actions are good. But if we have a distributional model of the environment, then we can turn a V-function into a Q-function by taking the expectation over next states and rewards for any given action. So this is one of the rare cases where learning a V-function is enough to find a good policy. And of course since a distributional model induces a sample model, the ideas we saw above still work.\nFinally, we can use models during decision time. That means we don\u0026rsquo;t (just) use a model to learn the policy \u0026ndash; instead, the policy itself makes use of the model. Concretely, we can do a search over possible actions, use the model to predict what would happen in each case, and then use that to improve our estimate of how good an action is. Monte Carlo Tree Search is one instance of that and works with any sample model.\nConclusion We\u0026rsquo;ve seen three different types of environment models: distributional models, sample models and trajectory models. Each of those can create trajectories for a given policy and thus replace the role of the environment in any RL algorithm. But distributional and sample models allow us to do additional things that are not possible otherwise.\nThis post also marks the end of the Building Blocks series. The framework of composing functions based on their signatures hopefully sheds some light onto how all of these building blocks fit together. It also demonstrates that the perspective of atomic building blocks can be applied on multiple levels: the previous posts showed how building blocks such as certain update targets can be combined to define different functions for mapping experience or trajectories to updates for value functions or policies. This post showed that these functions are themselves building blocks for complete training loops and can be composed in different ways with models and sampling procedures.\n  Since parameters in deep learning are usually vectors, there isn\u0026rsquo;t any formal difference between a parameter and a change of a parameter. But we\u0026rsquo;ll still distinguish them because there is a big conceptual difference and because it\u0026rsquo;s more general that way.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Usually, the type \\(A \\to B\\) refers to pure functions that map objects with type \\(A\\) to type \\(B\\). I use it slightly differently to also include stochastic functions, which have a random output of type \\(B\\). For example, the sampled trajectory is not a deterministic function of the policy, it will instead be different each time the sampling function is applied.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1614159660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614159660,"objectID":"8ec00efeadffcd03eee4a0ad29cabc92","permalink":"https://ejenner.com/post/rl-building-blocks-3/","publishdate":"2021-02-24T10:41:00+01:00","relpermalink":"/post/rl-building-blocks-3/","section":"post","summary":"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is\n  the conclusion of a series with such an overview, covering model-based RL.\n  ","tags":["Reinforcement learning"],"title":"Building Blocks of RL Part III: Model-based RL","type":"post"},{"authors":null,"categories":null,"content":"Additive \\(L_1\\) or \\(L_2\\) penalties are two common regularization methods and their most famous difference is probably that \\(L_1\\) regularization leads to sparse weights (i.e. some weights being exactly 0) whereas \\(L_2\\) regularization doesn\u0026rsquo;t. There are many pictures and intuitive explanations for this out there but while those are great to build some understanding, I think they conceal the arguably deeper reason why \\(L_1\\) regularization leads to sparse weights. But before we discuss that, we need to understand why \\(L_2\\) regularization does not help to get sparse weights.\n\\(L_2\\) regularization doesn\u0026rsquo;t lead to any sparsity Let \\(w\\) be a vector of parameters and \\(\\mathcal{L}(w)\\) be any continuously differentiable loss function1. For \\(L_2\\) regularization, we want to find \\[\\operatorname*{argmax}_w \\mathcal{L}(w) + \\beta\\Vert w\\Vert_2^2\\] This means that the gradient has to be zero: \\[\\nabla \\mathcal{L}(w) + 2\\beta w = 0\\] or in components: \\[\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_i}\\right\\rvert_{w_i = 0} + 2\\beta w_i = 0\\]\nSo we can get \\(w_i = 0\\) as the optimal solution only if \\(\\frac{\\partial \\mathcal{L}}{\\partial w_i}\\big\\rvert_{w_i = 0} = 0\\), i.e. if \\(w_i = 0\\) is already optimal without regularization! So \\(L_2\\) regularization doesn\u0026rsquo;t help to get sparsity at all. The same is true for \\(L_p\\) regularization for any \\(p \u0026gt; 1\\), because \\[\\left.\\frac{\\partial}{\\partial w_i} \\Vert w\\Vert_p^p \\right\\rvert_{w_i = 0} = 0\\]\n\\(L_1\\) regularization: non-differentiability to the rescue \\(L_1\\) regularization just uses the 1-norm instead of the Euclidean norm: \\[\\operatorname*{argmax}_w \\mathcal{L}(w) + \\beta\\Vert w\\Vert_1\\] How does that change things? Well, the 1-norm of a vector is not differentiable at 0. More precisely: \\[\\frac{\\partial}{\\partial w_i} \\Vert w\\Vert_1 = \\begin{cases} +1, \\quad w_i \u0026gt; 0\\\\\\\n-1,\\quad w_i \u0026lt; 0\\\\\\\n\\text{undefined for } w_i = 0 \\end{cases} \\] So when can \\(w_i = 0\\) be a local minimum of the regularized loss? We can\u0026rsquo;t just set the derivative to zero as before, because the derivative doesn\u0026rsquo;t exist.\nTo understand what we can do instead, let\u0026rsquo;s first recall why setting the derivative to zero works for differentiable functions. If \\(f(x)\\) has a local minimum at 0, then this means that \\(f(x) \\geq f(0)\\) for all sufficiently small \\(x\\). Since we assumed \\(f\\) to be differentiable at \\(0\\), \\(f(x)\\) is well approximated2 by \\[f(x) \\approx f(0) + f'(0)x\\] for small \\(x\\). So if \\(f'(0) \u0026gt; 0\\), then \\(f(x) \u0026lt; f(0)\\) for small negative \\(x\\), and if \\(f'(0) \u0026lt; 0\\) we get the same for positive \\(x\\). So the derivative at the minimum has to be zero, because otherwise taking a small step in the right direction would decrease the value.\nWe can apply the same idea to the loss with \\(L_1\\) regularization: what happens if we take a small step \\(h\\) away from \\(w_i = 0\\)? The loss is differentiable and thus changes approximately linearly: \\[\\mathcal{L}\\Big\\rvert_{w_i = h} \\approx \\mathcal{L}\\Big\\rvert_{w_i = 0} + h\\frac{\\partial\\mathcal{L}}{\\partial w_i}\\] But for the regularization term, the change is always just \\(|h|\\), instead of a linear term: \\[\\Vert w\\Vert_1 \\bigg\\rvert_{w_i = h} = \\Vert w \\Vert_1\\bigg\\rvert_{w_i = 0} + |h|\\]\nSo if we write \\(\\tilde{\\mathcal{L}}\\) for the regularized loss, then we get \\[\\tilde{\\mathcal{L}}\\Big\\rvert_{w_i = h} \\approx \\tilde{\\mathcal{L}}\\Big\\rvert_{w_i = 0} + h\\frac{\\partial\\mathcal{L}}{\\partial w_i} + \\beta |h|\\] As long as \\(\\left|\\frac{\\partial\\mathcal{L}}{\\partial w_i}\\right| \u0026lt; \\beta\\), this is larger than the regularized loss at \\(w_i = 0\\), because the \\(+ \\beta |h|\\) term will dominate. That is why \\(L_1\\) regularization leads to sparser weights: it pulls all those weights to zero whose partial derivative at 0 has absolute value less than \\(\\beta\\)3.\nInterlude: Priors If the loss function \\(\\mathcal{L}\\) models some log-likelihood, then regularization can be interpreted as performing maximum a posteriori (MAP) estimation rather than maximum likelihood estimation (MLE). This means we start with some prior over possible values of the parameter \\(w\\), update this distribution using the evidence from the loss function, and then pick the parameters which are the most probable according to the posterior distribution.\n\\(L_2\\) regularization corresponds to a Gaussian prior and \\(L_1\\) regularization to a Laplace prior (in both cases centered around 0). So it\u0026rsquo;s natural to try to explain the sparsity behavior of these regularization methods in terms of the underlying priors.\nHere\u0026rsquo;s what a Gaussian (red) and Laplace (blue) distribution look like, both with unit variance and properly normalized:\n  Figure 1: Gaussian and Laplace distribution with unit variance (created using https://www.desmos.com/)  One difference is that the Laplace distribution has a higher density at (and around) 0. I\u0026rsquo;ve seen this used as an explanation for sparsity several times: the Laplace distribution seems more \u0026ldquo;concentrated\u0026rdquo; around 0, i.e. assigns a higher prior to 0, which is why we get sparse solutions.\nBut that is very misleading (and depending on what is meant by \u0026ldquo;concentrated\u0026rdquo; just wrong). Consider the following figure:\n  Figure 2: Narrower Gaussian  These are still a normalized Gaussian and a Laplace distribution, the only difference is that I\u0026rsquo;ve chosen a much smaller variance for the Gaussian. This corresponds to choosing a higher coefficient \\(\\beta\\) for the \\(L_2\\) penalty. I\u0026rsquo;d argue that in this case the Gaussian is much more \u0026ldquo;concentrated around 0\u0026rdquo;, at least its density is much higher. But even with arbitrarily high \\(\\beta\\), \\(L_2\\) regularization won\u0026rsquo;t lead to sparse solutions: you can make the prior as narrow as you like, and you\u0026rsquo;ll get weights that are closer and closer to zero but never precisely.\nThe real difference is the singularity (i.e. non-differentiability) of the Laplace distribution at 0. Since the logarithm is a diffeomorphism, singularities of the prior correspond 1-to-1 with singularities of the log prior, i.e. the regularization term.\nSingularities are necessary for sparsity We\u0026rsquo;ve seen that the difference between \\(L_1\\) and \\(L_2\\) regularization can be explained by the fact that the \\(L_1\\) norm has a singularity while the \\(L_2\\) norm doesn\u0026rsquo;t, or equivalently that the Laplace prior has one while the Gaussian prior doesn\u0026rsquo;t.\nBut we can say more than that: a singularity is in fact unavoidable if we want to make sparse weights likely. If we choose any continuously differentiable prior \\(p\\) (or any continuously differentiable additive regularization term), then the overall objective \\(\\tilde{\\mathcal{L}}\\) is continuously differentiable and therefore, the gradient has to be zero at a local minimum. So for \\(w_i = 0\\) to be a local minimum, we\u0026rsquo;d need \\[\\frac{\\partial \\mathcal{L}}{\\partial w_i}\\bigg\\rvert_{w_i = 0} + \\frac{\\partial \\log p}{\\partial w_i}\\bigg\\rvert_{w_i = 0} = 0\\] which puts an equality constrain on \\(\\frac{\\partial \\mathcal{L}}{\\partial w_i}\\): we only get sparse weights if the gradient has precisely the right value. Typically, this will almost surely not happen (in the mathematical sense, i.e. with probability 0), so non-singular regularization won\u0026rsquo;t lead to sparse weights.\nIn contrast, we saw that the singularity of the \\(L_1\\) norm (or the Laplace prior) creates an inequality constraint for the partial derivative: it leads to \\(w_i = 0\\) as long as the derivative lies in a certain range of values. This is what makes sparse weights likely.\n  This is a restriction, for example a model containign ReLUs will typically only be differentiable almost everywhere, and as we\u0026rsquo;ll see, individual non-differentiable points will play a big role. It might be possible to argue that the types of non-differentiable points created by ReLUs don\u0026rsquo;t change the conclusions of following discussion but we\u0026rsquo;ll just assume a differentiable loss so we can focus on the conceptual insights.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n meaning that the error \\(f(x) - (f(0) + f'(0)x)\\) doesn\u0026rsquo;t just approach 0 for \\(x \\to 0\\) (that would be continuity), it approaches 0 fast enough that even \\[\\frac{f(x) - (f(0) + f'(0)x)}{x} \\to 0\\] This is enough to make the rest of argument work out.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Of course this is not guaranteed for complex loss functions: there might be another local optimum somewhere else. This is just the condition for 0 to be one of the local optima at which we might end up.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1613550780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613550780,"objectID":"ec394e2df73aae24ab882f806438adb0","permalink":"https://ejenner.com/post/sparsity-singularities/","publishdate":"2021-02-17T09:33:00+01:00","relpermalink":"/post/sparsity-singularities/","section":"post","summary":"  L1 regularization is famous for leading to sparse optima, in contrast to\n  L2 regularization. There are several ways of understanding this but I'll\n  argue that it's really all about one fact: the L1 norm has a singularity\n  at the origin, while the L2 norm does not. And this is not just true\n  for L1 and L2 regularization: singularities are always necessary to get sparse weights.\n  ","tags":["Machine learning"],"title":"L1 regularization: sparsity through singularities","type":"post"},{"authors":null,"categories":null,"content":"Informally, Chaitin\u0026rsquo;s incompleteness theorem states that there is a constant \\(L\\), such that we can\u0026rsquo;t prove the Kolmogorov complexity of any specific bit string to be larger than \\(L\\). We can of course prove that there are infinitely many bit strings with higher complexity than \\(L\\) \u0026ndash; but we can\u0026rsquo;t name a single one!\nJohn Baez calls this constant \\(L\\) the complexity barrier. And surprisingly, he argues that it is probably very low (on the order of a few thousand bits for reasonable encoding schemes)!\nAt least to me, this is a pretty amazing fact. Consider all the material available on the internet for instance: everything ever written online, all the videos and images, and binaries for every computer program out there. \u0026ldquo;Obviously\u0026rdquo; we can\u0026rsquo;t just write a Python program of a few kilobytes that outputs all of this, \u0026hellip; right? Well, I\u0026rsquo;m pretty sure we can\u0026rsquo;t, but somewhat incredibly, there\u0026rsquo;s no proof of that!\nTake a moment to be properly astonished by this result because the aim of this post is to make it as obvious as possible. We\u0026rsquo;ll get there soon enough, but first let\u0026rsquo;s look at some fun paradoxa.\nThere are no boring numbers \u0026hellip; The follwing \u0026ldquo;paradox\u0026rdquo; is quite famous:\n Assume there was an uninteresting natural number. Then the smallest such number would be interesting \u0026ndash; because it\u0026rsquo;s the smallest uninteresting number, that\u0026rsquo;s quite an interesting property! This is a contradiction, so there can be no uninteresting natural numbers.\n We can formalize it as follows: we have some boolean \u0026ldquo;boringness\u0026rdquo; property, call it \\(P\\), defined over natural numbers. So \\(P(n)\\) just means \u0026ldquo;\\(n\\) is boring\u0026rdquo;. Being the lowest boring number is itself interesting: \\[P\\left(\\min_{P(n)} n\\right)\\quad \\text{is false}\\] This is self-contradictory if there are any \\(n\\) such that \\(P(n)\\), so no natural numbers can have property \\(P\\).\n\u0026hellip; and every number can be described in 13 words or less There\u0026rsquo;s a arguably more interesting variation of this paradox: let \\(P_k(n)\\) mean \u0026ldquo;\\(n\\) cannot be described in fewer than \\(k\\) words\u0026rdquo;. Consider then the description \u0026ldquo;the smallest natural number which cannot be described in fewer than 14 words\u0026rdquo;. In our notation, this would be \\[\\min_{P_{14}(n)} n\\] However, this description is only 13 words long, so \\[P_{14}\\left(\\min_{P_{14}(n)} n\\right)\\quad \\text{is false}\\] which is a contradiction if such a number exists. Therefore, every number can be described in at most 13 words.\nThis seems suspicious: while there are many 13-word phrases, there\u0026rsquo;s still only a finite number of them1. So there aren\u0026rsquo;t enough short descriptions to go around for each natural number to get one.\nThere are arbitrarily complex strings \u0026hellip; Of course the problem is that \u0026ldquo;cannot be described in fewer than \\(k\\) words\u0026rdquo; is not a well-defined property because there is no unambiguous mapping from English descriptions to numbers.\nBut what if we replace English by a formal language to circumvent this issue? For example, a Turing machine without any input either halts and outputs a number (in the form of a bit string), or it runs forever. If we fix an encoding for Turing machines, any Turing machine has a length, so we can define \\(P_k(n)\\) as \u0026ldquo;\\(n\\) is not the output of any halting Turing machine with length less than \\(k\\)\u0026rdquo;. Or more briefly: \u0026ldquo;the Kolmogorov complexity of \\(n\\) is at least \\(k\\)\u0026rdquo;.\nSo let\u0026rsquo;s repeat our argument with Turing machines instead of natural language. We need to define a program that outputs \\(\\min_{P_k(n)} n\\) while being itself shorter than \\(k\\). Given a subroutine that checks whether \\(P_k(n)\\) for arbitrary \\(n\\), we could simply iterate over \\(n = 1, 2, \\ldots\\) until we find one such that \\(P_k(n)\\) and then output that. If such an \\(n\\) existed, this program would output \\(\\min_{P_k(n)} n\\), so by the same argument as before, there can\u0026rsquo;t be any \\(n\\) with complexity higher than \\(k\\) \u0026hellip;\n\u0026hellip; which can\u0026rsquo;t be right, because just as with natural language descriptions, there are only finitely many programs of length \\(\\leq k\\), but infinitely many bit strings.\nThis time, the issue is the phrase \u0026ldquo;Given a subroutine \u0026hellip;\u0026quot;: \\(P_k\\) is undecidable, so this subroutine unfortunately doesn\u0026rsquo;t exist2. Or fortunately, if you value the consistency of mathematics.\nIn essense, the problem is this: in English, the smallest number with a simple property can be described in few words because you only need to describe the property and a few additional words. But the same is not true for Kolmogorov complexity if the property isn\u0026rsquo;t decidable.\nFor any decidable property, the argument works: the smallest number with that property will have low Kolmogorov complexity (where \u0026ldquo;low\u0026rdquo; means \u0026ldquo;not much larger than the complexity of the property\u0026rdquo;). Let\u0026rsquo;s see what we can get by applying that insight.\n\u0026hellip; but not provably complex ones! This post is supposed to be about Chaitin\u0026rsquo;s incompleteness theorem, so we\u0026rsquo;d better connect all of this talk about paradoxa and complexity to that. The only missing ingredient is to consider provably large complexities, rather than just large complexities as before.\nThis means \\(P_k(n)\\) will now be \u0026ldquo;\\(n\\) encodes a proof that some specific bit string has Kolmogorov complexity higher than \\(k\\)\u0026rdquo;. We need some system of logic to make \u0026ldquo;proof\u0026rdquo; well-defined and an encoding scheme of proofs as natural numbers but we\u0026rsquo;ll ignore that since our goal is just to gain intuition.\nThis new \\(P_k\\) is clearly decidable: we just need to check whether \\(n\\) is a valid encoding of a proof and whether that proof shows that some specific number has Kolmogorov complexity higher than \\(k\\).\nThis program has length \\(\\log k\\) (to encode \\(k\\)) plus some constant. As we saw, this means that \\(\\min_{P_k(n)} n\\) also has Kolmogorov complexity at most \\(\\log k\\) (up to a constant): we can iterate over \\(n\\) and return the first one for which \\(P_k(n)\\) holds.\nSo far there\u0026rsquo;s no contradiction: \\(n\\) proves that the Kolmogorov complexity of some specific number, call if \\(M(n)\\), is larger than \\(k\\). And we\u0026rsquo;ve only seen that the Kolmogorov complexity of \\(\\min_{P_k(n)} n\\) is low. But of course \\(M\\) is itself computable and in fact by a pretty short program (which just looks at what \\(n\\) proves). So \\(M_k := M\\left(\\min_{P_k(n)} n\\right)\\) also has a small complexity, more precisely: \\[K(M_k) \\leq \\log k + \\text{const}\\]\nBut by construction, \\(K(M_k) \u0026gt; k\\) (that\u0026rsquo;s what \\(\\min_{P_k(n)} n\\) encodes a proof of). So we get \\[k \u0026lt; K(M_k) \u0026lt; \\log k + \\text{const}\\] which obviously can\u0026rsquo;t hold if \\(k\\) is sufficiently large. So for \\(k\\) greater than some constant \\(L\\), we run into a paradox \u0026hellip; if any \\(n\\) encoding such a proof exists. If there is no \\(n\\) encoding a proof that some specific bit string has complexity higher than \\(k\\), then there is no smallest such \\(n\\), and we can\u0026rsquo;t define \\(M_k\\). This proves Chaitin\u0026rsquo;s Incompleteness Theorem:\n There is some constant \\(L\\) such that for any given bit string, we can\u0026rsquo;t prove it has Kolmogorov complexity higher than \\(L\\).\n Further reading  The \u0026ldquo;fact\u0026rdquo; that every number can be described in at most 13 words is known as the Berry paradox You might like John Baez\u0026rsquo;s post that I already linked above. In addition to a discussion of Chaitin\u0026rsquo;s incompleteness theorem, he talks about how another famous paradox \u0026ndash; the surprise examination paradox \u0026ndash; motivates a proof of Gödel\u0026rsquo;s second incompleteness theorem! Or you could read the paper by Kritchman and Raz that introduced that proof. Chaitin himself briefly points out the similarity of his impossibility result to Berry\u0026rsquo;s paradoxon in Gödel\u0026rsquo;s Theorem and Information (1982)    At least in languages where you can\u0026rsquo;t just create an arbitrary number of new words by composing existing ones. If you can, then one word is enough to \u0026ldquo;describe\u0026rdquo; any number\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It might seem like we\u0026rsquo;ve in fact proven that \\(P_k\\) is undecidable, because if it was decidable, we\u0026rsquo;d get a contradiction. But actuallly, we\u0026rsquo;ve only shown that \\(P_k\\) can\u0026rsquo;t be computed by any program of length less than \\(k\\). A longer program doesn\u0026rsquo;t immediately lead to a contradiction.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1612970820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612970820,"objectID":"52ebefb10109c5c62868524ac984663d","permalink":"https://ejenner.com/post/boring-numbers/","publishdate":"2021-02-10T16:27:00+01:00","relpermalink":"/post/boring-numbers/","section":"post","summary":"  There is a \"complexity barrier\": a number such that we can't prove\n  the Kolmogorov complexity of any specific string to be larger than\n  that. The proof of this astonishing fact is closely related to some\n  famous paradoxa and we'll use this connection to get a better intuition\n  for why the complexity barrier exists.\n  ","tags":null,"title":"Boring numbers, complexity and Chaitin's incompleteness theorem","type":"post"},{"authors":null,"categories":null,"content":"This is part 2 of a three-part series. Part 1 covered value-based methods and also gave some introduction and defined some notation. Part 3 will cover model-based RL.\nSo far, we have looked at the building blocks necessary to learn value functions for a given policy, called policy evaluation. We have also seen that with GPI, we can use policy evaluation for control, i.e. to find optimal value functions. The policy was always derived from the value function, by picking actions (\\(\\varepsilon\\)-)greedily.\nIn this post, we take a more direct approach to control: what we really want to learn is a good policy, so why not optimize the policy directly, without the detour of learning a value function?\nPolicy optimization and policy gradient methods Policy optimization in general means that we have a parameterized family of policies \\(\\pi_\\theta(a|s)\\) and want to maximize the expected return with respect to the parameters \\(\\theta\\): \\[\\operatorname*{argmax}_{\\theta} J(\\theta)\\] where \\(J\\) is the expected return: \\[J(\\theta) := \\mathbb{E}_{\\tau \\sim \\pi_\\theta, \\mu} R(\\tau)\\] Here \\(\\tau\\) is a trajectory which is sampled using the initial state distribution \\(\\mu\\) and policy \\(\\pi_\\theta\\). \\(R(\\tau)\\) is the return of that trajectory.\nIn principle, there are many ways we could solve this optimization problem. For example, we could perform a grid search over parameters \\(\\theta\\) and evaluate the expected return for each parameter by sampling lots of episodes. But this wouldn\u0026rsquo;t scale well (\\(\\theta\\) might very well be a vector with millions of dimensions if we use Deep RL). In practice, most methods instead use stochastic gradient ascent or variations thereof and that is all we will cover in this post.\nOne sidenote before we dive in: why do we use a parameterized policy at all? For value-based methods, we started in a tabular setting, where we could directly assign values to each state. The difference is that even in a tabular setting, the policy is not an arbitrary function \u0026ndash; it has to be normalized over actions. So we can\u0026rsquo;t just update a single probability \\(\\pi(a|s)\\) without also adjusting others.\nSome theory: the policy gradient theorem If you\u0026rsquo;re only interested in a description of some policy optimization methods, you can skip this and the next section. But it sheds some light onto why these methods are designed the way they are and why they work.\nWe want to optimize the expected return \\(J(\\theta)\\). To see what that entails, we can write it out explicitly as \\[J(\\theta) := \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} q^{\\pi_\\theta}(s, a) \\pi_\\theta(a|s)\\] where \\(\\mu^\\pi\\) is the on-policy state distribution of \\(\\pi\\). To optimize this function using gradient ascent, we need to find \\(\\nabla_\\theta J(\\theta)\\). But finding the gradient of \\(\\mu^{\\pi_\\theta}\\) with respect to \\(\\theta\\) is not really possible if we don\u0026rsquo;t know the dynamics of the environment.\nFortunately, the policy gradient theorem comes to the rescue. It states that \\[\\nabla_\\theta J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} q^{\\pi_\\theta}(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\] i.e. we can just apply the gradient to the policy itself and don\u0026rsquo;t need to know how the state distribution depends on the policy. Section 13.2 of Sutton and Barto\u0026rsquo;s textbook contains more details and a proof.\nAnother very useful fact is that we can subtract a baseline from the state value: \\[\\nabla_\\theta J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\left(q^{\\pi_\\theta}(s, a) - b(s)\\right) \\nabla_\\theta \\pi_\\theta(a|s) \\] (the only difference to the previous equation is the \\(- b(s)\\) term). This fact is also sometimes called the policy gradient theorem. \\(b(s)\\) may be any function or random variable, as long as it doesn\u0026rsquo;t depend on \\(a\\).\nMore theory: score function estimators Typically, we will still be unable to evaluate the gradient \\[\\nabla_\\theta J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} q^{\\pi_\\theta}(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\] analytically. Some types of gradients of an expected value can be estimated by sampling: \\[\\nabla_x \\mathbb{E}_{x \\sim p} f(x) = \\mathbb{E}_{x \\sim p} \\nabla_x f(x)\\] so if we can sample from \\(p\\) and can calculate \\(\\nabla f(x)\\), we can estimate this gradient. But our case is different: ignoring the expectation over states (which doesn\u0026rsquo;t pose a problem), we want to evaluate a gradient of the form \\[\\nabla_\\theta \\mathbb{E}_{a \\sim \\pi_\\theta(a)} f(a)\\] The variable \\(\\theta\\), with respect to which we differentiate, appears in the distribution, so we can\u0026rsquo;t just approximate this gradient by sampling as we did in the other case.\nGradients of this form (called stochastic gradients) appear often in machine learning, not just in RL. One method to calculate them is the reparameterization trick, which you might know from variational autoencoders, but that requires assumptions that often aren\u0026rsquo;t met in RL. What we will use instead is the REINFORCE method or score function estimation. We can use the fact that \\(\\nabla g(x) = g(x) \\nabla \\log g(x)\\) for any \\(g\\) and write \\[\\nabla_\\theta \\mathbb{E}_{a \\sim \\pi_\\theta} f(a) = \\int_a f(a)\\nabla \\pi_\\theta(a) da = \\int_a \\pi_\\theta(a) f(a) \\nabla \\log \\pi_\\theta(a) da = \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[f(a)\\nabla \\log \\pi_\\theta(a)\\right]\\] The right hand side has the form we can deal with: an expectation over some term, with a probability distribution we can sample from. As long as we can evaluate \\(\\nabla \\log \\pi_\\theta\\), we can now estimate the gradient we need.\nRecall that \\(f(s, a) = q^{\\pi_\\theta}(s, a) - b(s)\\) where \\(b\\) is an arbitrary baseline. But more generally, we can use any function \\(f(s, a)\\) which has \\(q^{\\pi_\\theta}(s, a) - b(s)\\) as its expected value, and we will get an unbiased estimator for the gradient \\(\\nabla J(\\theta)\\). Keep this in mind and the various update targets we will soon see should make sense.\nThe general formula Similar to value-based methods, we can generate many algorithms for policy optimization using a single update equation.\nUsing the estimator for the gradient \\(\\nabla J\\), we can learn the parameter \\(\\theta\\) of the policy with stochastic gradient ascent. We will use samples \\(s_1, a_1, r_2, s_2, a_2, \\ldots\\) and then update according to \\[\\theta \\gets \\theta + \\alpha \\sum_t \\Psi_t \\nabla \\log p_\\theta(a_t|s_t)\\] where \\(\\Psi_t\\) is some estimate of \\(q^{\\pi_\\theta}(s_t, a_t)\\) and \\(\\alpha\\) is a learning rate.\nLater, we will generalize this to \\[\\theta \\gets \\theta + \\alpha \\sum_t \\Psi_t g_t\\] where the gradient \\(\\nabla \\log \\pi_\\theta\\) is replaced by a more general vector \\(g_t\\) that determines the direction of the update.\nIn the next section, we cover possible choices for \\(\\Psi_t\\), and in the section after that we will look at choices for \\(g_t\\).\nTargets Recall from the section on score function estimators that \\(\\Psi_t\\) should be an estimate of \\(q^{\\pi_\\theta}(s_t, a_t)\\). This means that we can use many of the targets we\u0026rsquo;ve already seen in part 1. In addition, we can subtract a baseline \\(b(s_t)\\) without changing the expected value of the update. In principle, \\(b(s_t\\)) can be any function of the state, but to reduce variance as much as possible, we usually use a learned state-value function, leading to so-called Actor-Critic methods. The baseline can be learned using any of the methods for learning \\(v_\\pi\\) from part 1 (or other policy evaluation methods).\nHere then are typical targets we can use:\n Monte Carlo \\(\\Psi_t = G_0\\) or \\(\\Psi_t = G_t\\) i.e. total or future return MC with baseline \\(\\Psi_t = G_0 - V(s_t)\\) or \\(\\Psi_t = G_t - V(s_t)\\) n-step TD with baseline \\(\\Psi_t = G_{t:t+n} - V(s_t)\\) (of course the baseline is optional) Generalized Advantage Estimation \\(\\Psi_t = G_t^{\\lambda} - V(s_t)\\) where \\[G_t^{\\lambda} := (1 - \\lambda) \\sum_{n = 1}^\\infty \\lambda^{n - 1} G_{t:t + n}\\] is the \\(\\lambda\\)-return. This is the TD(\\(\\lambda\\)) target with a baseline (which again is in principle optional but helps to reduce variance)  This is where the \u0026ldquo;building blocks\u0026rdquo; perspective really starts paying off: value-based methods and policy optimization are very different approaches in terms of their large-scale design, but on a smaller level, they are composed of some of the same parts.\nUpdating methods: VPG, NPG, TRPO As promised, we can not only choose the target \\(\\Psi_t\\) but also have some freedom when it comes to the vector \\(g_t\\) in whose direction we update the parameter \\(\\theta\\).\nThe simplest option is Vanilla policy gradient (VPG), which uses \\(g_t = \\nabla \\log \\pi_\\theta(a_t|s_t)\\). This is what we\u0026rsquo;ve already seen, it corresponds to stochastic gradient ascent on \\(J(\\theta)\\).\nBut this simple method has its drawbacks: gradient descent leads to small changes in the parameter \\(\\theta\\), but it doesn\u0026rsquo;t make any guarantees about the changes in the policy \\(\\pi\\) itself. If the policy is very sensitive to the parameter around some value \\(\\theta_0\\), then taking a gradient step from there might change the policy a lot and actually make it worse. To avoid that, we\u0026rsquo;ll need to use a small learning rate, which slows down convergence.\nThe solution is to use the Natural Policy Gradient (NPG1) instead of the usual gradient. Instead of limiting the size of the step in parameter space, it directly limits the change of the policy at each step (well, not really2 but that\u0026rsquo;s the intuition). Natural gradients are a general method for finding optimal probability distributions, not specific to Reinforcement Learning, but NPG is probably their most well-known application.\nComputationally, the natural gradient is just the normal gradient multiplied by the inverse Fisher matrix \\(F^{-1}\\) of the policy. If you want to know more, the Scholarpedia article has some details.\nFor both of these methods, we use a constant learning rate \\(\\alpha\\) (or one that is adapted using a fixed schedule, \\(\\alpha = \\alpha(t)\\)). The update vector \\(g_t\\) is given by:\n VPG \\(g_t = \\nabla \\log \\pi_\\theta(a_t|s_t)\\) NPG \\(g_t = F^{-1} \\nabla \\log \\pi_\\theta(a_t|s_t)\\) where \\(F\\) is the Fisher matrix of the policy  A third option is Trust-Region Policy Optimization (TRPO)3. The motivation is similar to that of NPG: limit how much the policy changes (in terms of the KL divergence). But it takes that idea further and actually guarantees an upper bound on how much the policy will change.\nWe can fit TRPO into our framework by using the same update vector as NPG with a learning rate that adapts at each step:\n TRPO \\(g_t = F^{-1} \\nabla \\log \\pi_\\theta(a_t|s_t)\\) and the adaptive learning rate \\(\\alpha = \\beta^j \\sqrt{\\frac{2\\delta}{\\tilde{g} F^{-1} \\tilde{g}}}\\) where \\(\\tilde{g} := \\Psi_t g_t\\) \\(\\beta \\in (0, 1)\\) and \\(\\delta\\) are hyperparameters and \\(j \\in \\mathbb{N}_0\\) is chosen minimally such that a constraint on the KL divergence between old and new policy is satisfied\n  Each of these updating methods can be combined with any of the targets, yielding a 2D grid of algorithms. In practice, some combinations are of course preferred, for example TRPO is typically used together with GAE. But these two aren\u0026rsquo;t connected in a fundamental way, it\u0026rsquo;s simply a choice that works well.\nConclusion We\u0026rsquo;ve seen that just like for value-based methods, we can get many different policy optimization methods by plugging in different terms into a single update equation. Moreover, the targets we can plug in aren\u0026rsquo;t new: we\u0026rsquo;ve used them for value-based methods too.\nOn the other hand, we\u0026rsquo;ve of course only scratched the surface when it comes to policy optimization methods. For example, we didn\u0026rsquo;t look at deterministic policy gradients or at PPO. And admittedly these methods don\u0026rsquo;t fit into the framework presented here as neatly as the ones we did consider. Furthermore, as I already discussed in Part 1, there are many details that determine whether a method will actually work in practice that we didn\u0026rsquo;t consider at all.\nSo I don\u0026rsquo;t want to create the false impression that all of policy optimization can be reduced to picking a target and an update vector. My goal is rather to convince you that thinking of RL methods as a combination of several composable building blocks is a better mental model than thinking about each method individually. The methods presented here simply fit this mental model especially well: you can combine any of the targets with any of the updating methods, so the building blocks are in some sense independent pieces.\nNext up: Part 3 which discusses model-based RL and concludes this series.\nResources  Spinning Up by OpenAI has explanations and implementations for several policy optimization algorithms. If you\u0026rsquo;d like a more practical perspective, this is a good place to start Lilian Weng has a long list of many policy gradient methods that might serve as an overview or as a quick reference for how a given method works More on score function estimators: http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/ and http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/    Sham Kakade, 2001. (pdf link)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Vanilla gradient descent can be interpreted as follows: we want to find \\(\\Delta \\theta\\) such that \\(J(\\theta + \\Delta \\theta)\\) is maximized, but under the constraint that the update isn\u0026rsquo;t too large, \\(\\Vert\\Delta \\theta\\Vert_2 \\leq c\\). As \\(c\\) gets smaller, the optimal update \\(\\Delta \\theta\\) aligns more and more with the gradient \\(\\nabla J(\\theta)\\). NPG does the same using the Kullback-Leibler divergence between \\(\\pi_\\theta\\) and \\(\\pi_{\\theta + \\Delta \\theta}\\) instead of the \\(L^2\\) distance between the parameters. So infinitesimally (i.e. as the learning rate approaches zero), it limits the change in the policy, but it doesn\u0026rsquo;t actually give any guarantees in the finite regime.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n John Schulman et al., 2015. (arxiv link)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1612334340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612334340,"objectID":"92f37957db5955fa584de51889e7df51","permalink":"https://ejenner.com/post/rl-building-blocks-2/","publishdate":"2021-02-03T07:39:00+01:00","relpermalink":"/post/rl-building-blocks-2/","section":"post","summary":"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 2\n  of a series with such an overview, covering some policy optimization methods.\n  ","tags":["Reinforcement learning"],"title":"Building Blocks of RL Part II: Policy Optimization","type":"post"},{"authors":null,"categories":null,"content":"When proving simple statements in point set topology, there is often only one obvious next step that can be done given the objects and statements you already have1. You don\u0026rsquo;t need to think about what you eventually want to prove because there is only one step that will lead to a proof of anything.\nAn example: continuous images of compact spaces are compact As an example, let\u0026rsquo;s go through the proof that the image of a compact space under a continuous map is again compact: we start with a compact space \\(X\\) and a map \\(f: X \\to Y\\). To make the notation less unwieldy, we\u0026rsquo;ll assume that \\(f\\) is surjective, so we\u0026rsquo;ll show that \\(Y\\) is compact, but the proof works exactly the same without this assumption (just replace every occurence of \\(Y\\) with \\(f(X)\\)). Because we want to show that \\(Y\\) is compact, i.e. that every open cover of \\(Y\\) has a finite subcover, we also start with a given open cover \\(Y = \\bigcup_i U_i\\).\nWith only these objects available, there isn\u0026rsquo;t a lot we can do. For example, if \\(X\\) were a normed vector space, we would have access to its zero vector and then could construct \\(f(0)\\) from that. That wouldn\u0026rsquo;t lead anywhere but it\u0026rsquo;s a branch in the tree of possible proofs that might distract us. Because \\(X\\) and \\(Y\\) have so little structure, these kinds of options simply don\u0026rsquo;t exist.\nThe only thing I can come up with is that we can look at the preimage of each of the \\(U_i\\) under \\(f\\). This gives us a collection \\(f^{-1}(U_i), i \\in I\\) of subsets of \\(X\\). Such a collection in itself still doesn\u0026rsquo;t allow us to do anything interesting, but because preimages preserve unions, we have \\[\\bigcup_i f^{-1}(U_i) = f^{-1}\\left(\\bigcup_i U_i\\right) = f^{-1}(Y) = X\\] so this collection is in fact a cover of \\(X\\). Because \\(f\\) is continuous, it is also an open cover.\nAgain, there isn\u0026rsquo;t much we can do with this newly constructed open cover. We could map it back to \\(Y\\) with \\(f\\) immediately but that just gives us back the open cover of \\(Y\\) we started with. The other thing we can do with an open cover of \\(X\\) is pick a finite subcover: \\(X\\) is compact, and we can think of that procedurally as a way of turning any open cover into a finite subcover.\nSo now we have a new object: a finite open subcover \\(X = U_{i_1} \\cup \\ldots \\cup U_{i_k}\\). Inside \\(X\\), there isn\u0026rsquo;t anything else we can do with a (finite) cover, so the only option is to now apply \\(f\\) again, which gives us sets \\(f(f^{-1}(U_{i_1})), \\ldots, f(f^{-1}(U_{i_k})) \\subset Y\\). Because \\(f\\left(f^{-1}(U_i)\\right) = U_i\\), this is a finite subset of the open cover we started with.\nAn then we\u0026rsquo;re done because it is also a cover: \\[\\bigcup_{j = 1}^k f\\left(U_{i_j}\\right) = f\\left(\\bigcup_{j = 1}^k U_{i_j}\\right) = f(X) = Y\\]\nThe thing that I hope you took away from this walkthrough is how few choices there were at each step. Apart from some steps that obviously didn\u0026rsquo;t add anything new, there was always only one thing to do next.\nWe didn\u0026rsquo;t even specifically aim to construct a finite subcover of \\(\\bigcup_i U_i\\) for most of the proof, we just \u0026ldquo;went with the flow\u0026rdquo;.\nThis is a feeling that is much more rare in e.g. real analysis, even for proofs that are similarly easy as the one above. With some experience, you might get enough intuition to discard all the wrong options immediately but they\u0026rsquo;ll still be there. You typically have to keep in mind what you want to prove and deliberately steer your proof in that direction, otherwise the number of possible paths you could take just explodes and you never get anywhere.\nThe importance of (lack of) structure The decisive difference between the point set topology example and real analysis is, I think, how much structure the spaces and objects we are working with have. By \u0026ldquo;structure\u0026rdquo;, I mean the same somewhat elusive concept I\u0026rsquo;ve previously talked about here. In short, a group is a set with some additional structure and a field adds even more structure. The way I use the word, a manifold also has more structure than a topological space (even though it doesn\u0026rsquo;t require any new choices).\nOne of the aspects of structure I talked about in the post I just linked is that objects with less structure admit fewer definitions and theorems. Applying theorems to the objects we\u0026rsquo;ve already constructed is how we make progress in our proofs. So having fewer theorems to work with leads to a proof tree with a lower branching factor: at each step of the proof, there are only a few things we can do. In an extreme case, we have a branching factor of one and can do the proof on autopilot, as in the topology example above.\nIf you are working on \\(\\mathbb{R}^n\\) on the other hand, you can use all the topological properties you had before, but you can also view \\(\\mathbb{R}^n\\) as a vector space, you can talk about lengths and angles and even about the Lebesgue measure of sets. This is possible because \\(\\mathbb{R}^n\\) has a lot of canonical structure, so you suddenly have many more tools at your disposal.\nThis explains why it can help to generalize a statement you are trying to prove: afterwards, you have less structure to work with. Assuming the statement is still true in its more general form, the tree of possible proofs has a much smaller branching factor and becomes easier to explore.\nPropositions as types One last thing to mention is that all of this is closely connected to the \u0026ldquo;propositions as types\u0026rdquo; interpretation: mathematical propositions can be interpreted as types, with proving a proposition corresponding to constructing a term of that type. I already talked about constructing new objects using the available objects and theorems and this is exactly the same idea but the language of type theory formalizes this. If you want to see an example like the topology proof I gave but explicitly using the propositions as types view, check out section II. in this post. If you haven\u0026rsquo;t seen the correspondence between propositions and types before and want to learn more, this talk is very fun to watch.\n  Note the \u0026ldquo;simple\u0026rdquo; \u0026ndash; there are obviously really hard to prove statements in point set topology, as in any discipline\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1611733980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611733980,"objectID":"bcab6c1869f8f7c4c60505914764f58f","permalink":"https://ejenner.com/post/too-much-structure/","publishdate":"2021-01-27T08:53:00+01:00","relpermalink":"/post/too-much-structure/","section":"post","summary":"  Proving things for object that have a lot of structure can be harder\n  than for object with less structure, simply because the tree of possible\n  proofs is much wider. This is probably why trying to prove a more general\n  case is sometimes a helpful strategy.\n  ","tags":["Structure","Math"],"title":"Too much structure","type":"post"},{"authors":null,"categories":null,"content":"Epistemic status: thinking out loud, not an expert on physics\nIn physics, there appears to be a deep duality between position and momentum, in the sense that they are largely equivalent perspectives on viewing the same system. In classical mechanics, \\(x \\mapsto p\\) and \\(p \\mapsto -x\\) is a canonical transformation, which means that treating momentum as position and position as negative momentum results in unchanged dynamics. In quantum mechanics, the roles of position and momentum can be similarly switched with a change of basis.\nSo mathematically speaking, it would appear that there is nothing special about either position or momentum, both yield similar and equally good descriptions. And yet, human cognition treats position and momentum very differently, they don\u0026rsquo;t feel like dual descriptions of reality. To us, there is a big difference between a car that is close to us and moving with a high relative velocity (distant in momentum space) and one that is far away and more or less stationary with respect to us.\nBut human cognition runs on brains, which run on physics, which seems to treat position and momentum equivalently. So how can this be? How does the cognitive asymmetry arise from what seems to be symmetry on the fundamental physical level?\nThe motivation for this post is mostly to point out the question. I\u0026rsquo;m not sure myself what the answer is but I\u0026rsquo;ll at least give my guesses below.\nFalse assumptions I\u0026rsquo;ve said that human cognition \u0026ldquo;runs on brains\u0026rdquo;, which \u0026ldquo;run on physics\u0026rdquo; and the argument loses a lot of its punch if this assumption is false. Cognition not running on physics could mean something like a fundamental Cartesian distinction between body and mind. That doesn\u0026rsquo;t answer why humans perceive two things differently that appear to be equivalent in physics but at least that fact doesn\u0026rsquo;t seem as paradoxical anymore.\nThere\u0026rsquo;s also the possibility that cognition does run on physics but uses physics we don\u0026rsquo;t know of yet, and in which there is a fundamental difference between position and momentum that our cognition exploits.\nI think that neither of these cases is very likely. If we didn\u0026rsquo;t find any other explanations for the cognitive difference between position and momentum, then this difference might be strong evidence for a Cartesian view or for new physics playing a role in our cognition. But I think there are other, more promising explanations, based on the observation that while the fundamental physical laws treat position and momentum the same, the Hamiltonian that happens to govern our universe does not. That\u0026rsquo;s what I\u0026rsquo;ll get to next.\nHamiltonian part I: Locality This explanation is specific for quantum mechanics. So if it turns out to be the reason for the asymmetry between position and momentum, this would mean that this feature of our cognition is inherently quantum mechanical and would not appear in a classical universe.\nThe Schrödinger equation, which determines the time evolution of a system, can be written in terms of position as follows: \\[i\\hbar \\frac{\\partial}{\\partial t}\\psi(x, t) = \\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x)\\right)\\psi(x, t)\\] This time evolution is local in the following sense: to calculate \\(\\frac{\\partial}{\\partial t} \\psi(x, t)\\), we only need to know the wave function \\(\\psi\\) in an arbitrarily small neighborhood of \\(x\\) (so that we can calculate its second spatial derivative).\nWe can also write the Schrödinger equation in terms of momentum: \\[i\\hbar \\frac{\\partial}{\\partial t}\\psi(p, t) = \\left(\\frac{p^2}{2m} + V\\left(i\\hbar\\frac{\\partial}{\\partial p}\\right)\\right)\\psi(p, t)\\] What does it mean to plug a derivative into the potential \\(V\\)? We\u0026rsquo;ll assume that \\(V\\) is analytic, which means that it can locally be written as a power series. Then \\(V\\left(i\\hbar \\frac{\\partial}{\\partial p}\\right)\\) is defined by plugging in \\(i \\hbar \\frac{\\partial}{\\partial p}\\) into that power series.\nIf \\(V\\) happens to be a polynomial, this is just a sum of normal differential operators and the time evolution is local in exactly the same sense as for position. But in general, \\(V\\) can be an infinite power series, and we will take arbitrarily high derivatives of \\(\\psi\\). This means that locality can be violated \u0026ndash; this power series of derivatives may depend on points that are far away in momentum space1. The most famous example for a power series of differential operators being non-local is probably the fact that \\(\\exp\\left(a \\frac{\\partial}{\\partial x}\\right) f(x) = f(x - a)\\) (see e.g. this stackexchange post). \\(f(x - a)\\) depends on the value of \\(f\\) outside a small enough neighborhood (if \\(a \\neq 0\\)), so in such cases, the time evolution in terms of position is not local in the sense described above.\nThis raises the question: where does the asymmetry between these two formulations of the Schrödinger equation come from? The answer is that it\u0026rsquo;s all the Hamiltonian\u0026rsquo;s fault. The Schrödinger equation can be written in basis independent form as \\[i \\hbar \\frac{\\partial}{\\partial t} \\psi = \\hat{H}\\psi\\] where \\(\\hat{H}\\) is the Hamiltonian operator. This Hamiltonian usually has the form \\[\\hat{H} = \\frac{\\hat{p}^2}{2m} + V(\\hat{x})\\] So the asymmetry on the level of the Hamiltonian is that the momentum operator appears as a second power, whereas the position operator is plugged into the potential, which may be an infinite power series.\nIn the position basis, \\(\\hat{p}\\) turns into a derivative whereas in the momentum basis, \\(\\hat{x}\\) becomes a derivative. This leads our observation that time evolution is local in the position formulation in a sense that does not hold for momentum.\nHamiltonian part II: \u0026ldquo;Weak\u0026rdquo; locality In the previous section, we considered only a single particle (though the same asymmetry applies to multiple particles \u0026ndash; having only a single particle is the weaker assumption). If we have multiple interacting particles, we get a different sense of locality that doesn\u0026rsquo;t require QM anymore.\nAt the beginning I mentioned the difference in our cognition between a distant stationary car and a nearby car that\u0026rsquo;s moving fast. It\u0026rsquo;s very reasonable that we think about these situations differently: if a car is very far away, it can\u0026rsquo;t interact with us, i.e. hit us. The same is not true for momentum: if a car is moving very fast, it can still hit us, even though it is far away in momentum space.\nWe might call the fact that spatially distant objects tend to interact less \u0026ldquo;weak locality\u0026rdquo;. \u0026ldquo;Weak\u0026rdquo; because they can still interact, just typically not as much. So position satisfies weak locality while momentum apparently doesn\u0026rsquo;t.\nThe reason for that can again be found in the Hamiltonian. For multiple particles \\(i = 1, \\ldots, n\\), the Hamiltonian usually has the form \\[H = \\sum_{i = 1}^n H_i(x_i, p_i) + \\sum_{i \\neq j} V(|x_i - x_j|)\\] Here, \\(x_i, p_i\\) are the position and momentum of particle \\(i\\). \\(H_i\\) is the Hamiltonian for a single particle, which only depends on the position and momentum of that particle. This includes the kinetic energy and any potentials that are not caused by particle interactions.\nThe second sum in the Hamiltonian describes the interactions between particles. The way I wrote it, it can model any pairwise interaction that depends only on the distance between particles. It so happens that for the forces that actually occur in our universe, the interaction potential \\(V\\) diminishes as the distance between the interacting particles increases. This is what leads to the weak locality in position space. Since the interaction does not depend on the momenta of the particles, there is no analogous weak locality for momentum.\nAs in the previous section, the asymmetry again boils down to the Hamiltonian being asymmetric in position and momentum. This fits rather well with my own intuition. For example, in a harmonic oscillator, both position and momentum appear as a second power in the Hamiltonian, and they really do seem much \u0026ldquo;more equivalent\u0026rdquo; there than in other systems.\nBut it raises the question why the Hamiltonian has such a form. Classical mechanics or QM themselves don\u0026rsquo;t have an answer; after all, symmetric Hamiltonians such as the harmonic oscillator work completely fine in principle, it\u0026rsquo;s just that our universe isn\u0026rsquo;t a harmonic oscillator. I\u0026rsquo;m not sure whether QFT can shed light onto this question, otherwise maybe theories of quantum gravity can. This would likely mean a more fundamental difference between position and momentum, which in turn leads to the asymmetry in the Hamiltonian.\nAnother approach is to say that most possible Hamiltonians aren\u0026rsquo;t symmetric in position and momentum, so it\u0026rsquo;s not surprising at all that ours isn\u0026rsquo;t. This doesn\u0026rsquo;t feel quite as satisfying and whether you buy into that argument at all depends on how you think about the \u0026ldquo;probability\u0026rdquo; of physical laws being a certain way. In a similar vein, one could appeal to the anthropic principle: we can only observe Hamiltonians that permit observers to exist in the universe they describe. A harmonic oscillator is presumably too simple for that and maybe the same is true for any Hamiltonian which treats position and momentum exactly equivalently.\n  I saw this point made in this comment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1611049920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611049920,"objectID":"e2137f36d62bfec06b6574e7f4bcb7a4","permalink":"https://ejenner.com/post/position-momentum-asymmetry/","publishdate":"2021-01-19T10:52:00+01:00","relpermalink":"/post/position-momentum-asymmetry/","section":"post","summary":"  In both classical mechanics and QM, there are transformations between position-based\n  and momentum-based representations that preserve the dynamical laws. So from\n  a mathematical perspective, position and momentum seem to play equivalent roles\n  in physics. But they don't play equivalent roles in our cognition, which is part of\n  the physical universe -- seemingly a paradox.\n  ","tags":["Physics"],"title":"Asymmetry between position and momentum in physics","type":"post"},{"authors":null,"categories":null,"content":"Reinforcement Learning consists of atomic building blocks that can be combined to create many of the well-known algorithms. This is not a secret but it can sometimes be obscured when learning about different methods one after another, never getting the big picture view. So this is my attempt at the kind of overview I would have like when I first got into RL. How helpful it is to you probably depends a lot on how similar your learning style is to mine.\nThis is part 1 of of a planned three-part series. Part 2 will be about policy optimization and part 3 about model-based RL.\nMotivation Why consider the building blocks of RL individually at all? There are at least two good reasons:\n It makes RL methods easier to memorize. This is for two reasons: first, memorization becomes easier when the material is split into small chunks. Second, many of the building blocks are shared by several methods, so we can avoid duplicate effort more effectively by explicitly considering these building blocks. More importantly, it gives a better understanding of the landscape of RL methods. A very naive view of RL methods would just consider them as a very long list of possibilities. But in reality, they are more of a very high-dimensional table, with different options to choose from for different aspects of the algorithm.  Target audience and what this is not On its own, this is not an introduction to Reinforcement Learning; I assume that you already know most of the definitions and algorithms and mainly describe how they fit into one common framework. That said, it might be helpful to read this series in parallel to learning about the algorithms it covers. Or you can use it as a review, or to deepen your big picture of RL. If you\u0026rsquo;re already very familiar with RL theory, you probably won\u0026rsquo;t find anything new.\nThis is also not a guide on which method to choose for which problem. It might help with that but I don\u0026rsquo;t focus on the various advantages and disadvantages.\nFinally, this overview is far from exhaustive. My main goal is to present the framework and give enough examples to provide intuition for how concrete algorithms fit in. In particular, I focus on a tabular setting (for Part 1) and cover Deep RL only briefly towards the end. All of the things I discuss for a tabular setting are still relevant for Deep RL, so it should still be useful even if you\u0026rsquo;re not interested in tabular RL for its own sake. But if you\u0026rsquo;re looking for an overview of the parts that are specific to Deep RL, this is not it.\nNotation  \\(A_t\\) is the action taken at time \\(t\\) while the agent is in state \\(S_t\\). Afterwards, the environment returns a reward \\(R_{t + 1}\\) and a new state \\(S_{t + 1}\\) The return \\(G_t\\) is the discounted sum of rewards from time \\(t\\) onwards \\[G_t = \\sum_{k = 1}^\\infty \\gamma^{k - 1} R_{t + k}\\]  Value functions This post is about value-based methods, which means the model explicitly learns and represents a value function and uses that value function to compute the policy (I will cover actor-critic methods when we talk about policy optimization in part 2).\nThere are two types of value functions: state-value functions or V-functions assign a value to every state \\(s\\). We write \\(v_\\pi(s)\\) for such a value function. Q-functions assign a value to every state-action pair \\((s, a)\\), i.e. to taking action \\(a\\) in state \\(s\\), and we write them as \\(q_\\pi(s, a)\\). Many algorithms work essentially the same for both kinds of functions but there will be a few cases where we need to make a distinction.\nFinding these value functions \\(v_\\pi\\) or \\(q_\\pi\\) for a given policy \\(\\pi\\) is called policy evaluation. Of course just evaluating a policy is not that useful by itself. After all, the goal of reinforcement learning is to find a good policy. We do this using generalized policy iteration (GPI), which we will talk about more later. For now you only need to know that GPI is a method (or rather collection of methods) for finding an optimal policy, which needs to evaluate a policy as one of its substeps. So we will start by only discussing policy evaluation, keeping in mind that this will later help us with finding good policies as well.\nGeneral shape of the update We will start in a tabular setting meaning there are only finitely many states and the value function is a simple lookup table. All the value-based methods in this setting have the same general shape: we have some observations \\(S_t, A_t, R_{t + 1}, S_{t + 1}, A_{t + 1}, \\ldots\\), which we got from running policy \\(\\pi\\) on the environment (or on an environment model, more on that in part 3). We keep an estimate \\(V\\) of the true value function \\(v_\\pi\\), which is updated for each observed state \\(S_t\\) as follows: \\[V(S_t) \\gets V(S_t) + \\alpha(\\text{target} - V(S_t))\\] or analogously \\[Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha(\\text{target} - Q(S_t, A_t))\\] for an estimate \\(Q\\) of \\(q_\\pi\\). \\(\\alpha\\) is a learning rate which may or may not be constant. \\(\\text{target}\\) is the key piece that distinguishes all the algorithms we\u0026rsquo;ll look at from one another. It should be something that is, in expectation, a better value estimate than the old \\(V(S_t)\\). In some cases it will depend on \\(V\\), those are called bootstrapping.\nTargets for policy evaluation Now that we have described the general shape of the update, we can define all the most popular methods in the tabular setting by just giving the target:\n Monte Carlo the target is simply the return \\(G_t\\). The way we described the general method in the previous section, we get every-visit MC. There is also first-visit MC, which updates the estimate for each state only once per episode (the first time it occurs). TD(0) the target is \\(R_{t + 1} + \\gamma V(S_t)\\) or \\(R_{t + 1} + \\gamma Q(S_t, A_t)\\) This is also the 1-step return \\(G_{t:t+1}\\). If we\u0026rsquo;re learning the Q-function, this is called Sarsa. Expected Sarsa Like Sarsa, but with an expectation over the next action, rather than the actually sampled action: \\(R_{t + 1} + \\gamma \\mathbb{E}_{a \\sim \\pi} Q(S_t, a)\\) This only works for Q-functions, since for V-functions, we would need to know the environment dynamics to calculate the expected value. n-step TD The target is the \\(n\\)-step return \\(G_{t:t+n}\\). This generalizes MC and TD(0): with \\(n = 1\\), we get TD(0) and with \\(n = \\infty\\), we get MC. n-step expected Sarsa Uses a variation of the n-step return as the target, where the value of the \\(n\\)-th state is estimated not by the value function but by an expected value over actions: \\[G_{t:t+n} - \\gamma^n Q(S_{t + n}, A_{t + n}) + \\gamma^n \\mathbb{E}_{a \\sim \\pi} Q(S_{t + n}, a)\\] This generalizes expected Sarsa and again only works for Q-functions. TD(\\(\\lambda\\)) uses \\(\\lambda\\)-returns as the target (an exponentially weighted average for \\(n\\)-step returns for all values of \\(n\\)  For a complete algorithm, we also need to specify a learning rate. If we decay the learning rate at the right pace, all these methods are guaranteed to converge to the true \\(v_\\pi\\) (and this decay schedule is the same for all methods). But of course a constant learning rate can also work well.\nThere is also dynamic programming, though it is a bit of a degenerate case: if we know the true environment transition probabilities, there is no need to sample episodes. Instead, we can use \\[\\text{target} = \\mathbb{E}_{A_t \\sim \\pi, S_{t + 1}, R_{t + 1} \\sim \\text{env}} \\left[R_{t + 1} + \\gamma V(S_{t + 1})\\right]\\]\nGPI, control and Q-learning All the methods from the previous section are policy evaluation methods: if the policy \\(\\pi\\) from which we sample trajectories is fixed, they converge to \\(v_\\pi\\) or \\(q_\\pi\\), not to the optimal value functions.\nAs promised, we can use a policy evaluation inside a larger algorithm to find optimal policies. This works as follows:\n Start with a random policy \\(\\pi\\) and value function \\(V\\) or \\(Q\\) Iterate until convergence:  Run one of the policy evaluation algorithms above for one or several steps to make the value estimate closer to the true \\(v_\\pi\\) or \\(q_\\pi\\) Improve the policy \\(\\pi\\), for example by making it \\(\\varepsilon\\)-greedy with respect to the current value estimate    This is called generalized policy iteration or GPI.\nThe second step in the loop, where we improve the policy, is easy for Q-functions. The greedy policy is then simply given by \\[\\pi'(s) = \\operatorname*{argmax}_a Q(s, a)\\] and the \\(\\varepsilon\\)-greedy policy just means following \\(\\pi'\\) with probability \\(1 - \\varepsilon\\) and choosing randomly with probability \\(\\varepsilon\\).\nWith V-functions on the other hand, we would need access to the environment dynamics to compute the greedy policy. Because we usually don\u0026rsquo;t have that, we use Q-functions if we want to do value-based control. Nevertheless, V-functions have other important uses (we\u0026rsquo;ll see them again for Actor-Critic methods in Part 2).\nWe now add one more target to our growing collection: \\[\\text{target} = R_{t + 1} + \\gamma \\max_{a} Q(S_t, a)\\] This results in Q-learning, which in contrast to all the previous targets learns the optimal policy directly. So it solves a different problem than policy evaluation and doesn\u0026rsquo;t need to be combined with GPI.\nHowever, we can also fit Q-learning into the GPI framework: it is equivalent to using Sarsa1 and making the policy greedy after each Q-update. Combining this into a single target that directly learns the optimal Q-function just simplifies things.\nOff-policy learning (Here and in the next section, I will write the equations only for V-functions to make it more readable but they all work exactly the same for Q-functions)\nSo far, we have used policy evaluation only to learn the policy \\(\\pi\\) that was used to sample actions. Off-policy learning means that actions are sampled by the behavior policy \\(b\\) but we still want to learn the value function for some other specified policy \\(\\pi\\).\nConsider our general update rule: \\[V(S_t) \\gets V(S_t) + \\alpha(\\text{target} - V(S_t))\\] The expected update at each step is \\[\\mathbb{E}_{A_t \\sim \\pi} \\left[\\alpha(\\text{target} - V(S_t))\\right]\\] i.e. the amount by which \\(V(S_t)\\) changes on average on one update. We want to tweak the update rule in such a way that we get this expected update even though we are using samples from \\(b\\) rather than \\(\\pi\\).\nThere is a general method for estimating an expected value with respect to one probability distribution \\(p\\) using samples from a different distribution \\(q\\). It\u0026rsquo;s called importance sampling and is simply the observation that \\[\\mathbb{E}_{x \\sim p} f(x) = \\mathbb{E}_{x \\sim q} \\frac{p(x)}{q(x)} f(x)\\] So when we can only sample from \\(q\\), we multiply each outcome by the importance sampling ratio \\(\\frac{p(x)}{q(x)}\\) to adjust. This has nothing to do with reinforcement learning, it\u0026rsquo;s a much more general method.\nSo we can use importance sampling for off-policy learning. For a 1-step method such as TD(0), our new update rule becomes \\[V(S_t) \\gets V(S_t) + \\alpha\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}(\\text{target} - V(S_t))\\] Note that this is a strict generalization: if \\(b = \\pi\\), which is the on-policy case we had before, the importance sampling ratio is one. Also note that we didn\u0026rsquo;t have to modify the target, so this can be applied the same way to all 1-step methods.\nWhy did I say \u0026ldquo;for a 1-step method\u0026rdquo;? If the target (even implicitly) depends on more future actions, i.e. \\(A_{t + 1}, A_{t + 2}, \\ldots\\), then these need to be included in the importance sampling ratio. So in general, we can define \\[\\rho_{t:t+n} := \\prod_{\\tau = t}^{t + n} \\frac{\\pi(A_\\tau|S_\\tau)}{b(A_\\tau|S_\\tau)}\\] and then use the update rule \\[V(S_t) \\gets V(S_t) + \\alpha\\rho_{t:t+n}{b(A_t|S_t)}(\\text{target} - V(S_t))\\] where \\(k\\) is the number of future actions the target depends on. For example, Monte Carlo has \\(n = \\infty\\) (meaning until the end of the episode) and Sarsa has \\(n = 1\\).\nThis means that there is a slight dependency between the target and the importance sampling ratio, namely the number \\(n\\) of future steps that are considered. But other than that, importance sampling works the same for all of our targets.\nNow you may have heard that some methods like expected Sarsa are \u0026ldquo;off-policy methods\u0026rdquo; while others are on-policy. This seems to clash with our observation that importance sampling has almost nothing to do with the update target, so what\u0026rsquo;s going on?\nFirst, we can always use importance sampling and get a method that works in an off-policy setting. So when we say that Sarsa is on-policy, that just means that we need importance sampling to use it for off-policy learning.\nSecond, some methods are off-policy methods in the sense that they already work in an off-policy setting without importance sampling. This is typically the case because the target doesn\u0026rsquo;t depend on the sampled action at all. For example in the target for expected Sarsa, we already take an expectation over the action, so the target itself is independent of the sampled action. Therefore, it doesn\u0026rsquo;t matter which policy we use for sampling, only which one we use for taking the expectation inside the target.\nFor such off-policy methods, the expected update is the correct one no matter which behavior policy we use. If we use importance sampling, we still get the same expected value, since the importance sampling ratio has an expected value of one. But for those methods, there is no reason to use it, and since it increases the variance, it would even hurt.\nAs a final note, what I described is more specifically called ordinary importance sampling. There is also weighted importance sampling which lowers the variance at the cost of introducing some bias. Which of those you use is in principle an orthogonal choice to your update target.\nWhat about function approximation? So far, we only considered a tabular setting, meaning that the value function estimate is a lookup table that assigns a value to each state. Our update equation reflects this: the \\(\\gets\\) in \\[V(S_t) \\gets V(S_t) + \\alpha(\\text{target} - V(S_t))\\] only makes sense if we can assign any value to any state.\nIf the state space is too large or even infinite, this won\u0026rsquo;t work. Instead, we need to limit ourselves to some family of functions and want to pick one among those that approximates the true \\(v_\\pi\\) as well as possible. We can then write the value estimate as a function \\(\\hat{v}(s, w)\\) of the state \\(s\\) and a parameter \\(w\\). We can\u0026rsquo;t set \\(\\hat{v}\\) itself anymore, only \\(w\\). I\u0026rsquo;m switching from \\(V\\) to \\(\\hat{v}\\) only to avoid confusion between tabular and non-tabular value functions, there\u0026rsquo;s no other difference.\nI won\u0026rsquo;t cover many of the theoretical aspects that arise in this setting, such as convergence guarantees, because that\u0026rsquo;s a big topic in itself. But as long as we focus on just describing the various methods, rather than on their theoretical properties, function approximation doesn\u0026rsquo;t require many changes to our framework.\nThe ideal update would still be \\[\\hat{v}(s, w) \\gets \\hat{v}(s, w) + \\alpha(\\text{target} - \\hat{v}(s, w))\\] but that doesn\u0026rsquo;t work anymore because we can only choose \\(w\\) directly. Instead, we introduce a new update rule that works on \\(w\\) instead of on the value function itself: \\[w \\gets w + \\alpha(\\text{target} - \\hat{v}(s, w))\\nabla_w \\hat{v}(s, w)\\] It contains the gradient of the value function, which we can think of as being needed for converting between the thing we want to change (\\(\\hat{v}\\)) and the thing we can directly change (\\(w\\)). But other than that, the update is very similar. In particular, we have the same choices to make: a learning rate (and how it changes) and the update target. If we want to use importance sampling, we simply multiply the update by \\(\\rho_{t:t+n}\\) just as before. So we only need to change the update rule and can then plug in all the same targets as before. For example, if we use the Q-learning target with this update rule, we get the basic algorithm underlying DQN.\nThat isn\u0026rsquo;t to say that there aren\u0026rsquo;t any other choices that need to be made when using function approximation. To name just a few important ones:\n We need to choose the parameterized family of functions that we use for \\(\\hat{v}\\). In Deep RL, this is a neural network, which means that there are many, many options to choose from. The update rule for \\(w\\) above isn\u0026rsquo;t the only one we can use. Chapter 11 of Sutton\u0026rsquo;s and Barto\u0026rsquo;s book on RL contains details on the various choices and the issues associated with them but that\u0026rsquo;s beyond the scope of this post. Besides what\u0026rsquo;s listed there, we can also choose more complex optimizers. Our update rule can be interpreted as SGD on the squared value error, so you could instead use SGD with momentum, Adam, or whatever your favorite optimizer is. We haven\u0026rsquo;t really talked about where the samples that we\u0026rsquo;re using to update are coming from. The theoretically simplest case is to always sample new actions after the policy is updated. But in practice, you might for example want to use a replay buffer instead.  These are the kinds of things that get us from an update rule plus a target to a full practical method such as DQN. We could try to incorporate as many of them as possible into our framework but I\u0026rsquo;m not sure how useful that would be. In any case, they are arguably not really building blocks of reinforcement learning in particular; most of them are more generally about designing and optimizing deep neural networks.\nSo when we go from tabular RL to function approximation, we get many new choices on top of the ones we already need to make in a tabular setting. But the building blocks we\u0026rsquo;ve seen for tabular methods, such as update targets or importance sampling, persist essentially unchanged.\nSummary: building blocks for value-based methods To summarize, these are the main building blocks for (tabular) value-based methods:\n The target for the update: this is something that should be a better estimate of the true value function \\(v_\\pi\\) than the current estimate. Examples include TD(0) (including Sarsa), Monte Carlo, expected Sarsa, Q-learning and n-step TD targets Importance sampling: there isn\u0026rsquo;t too much choice here. If the target depends on the taken action(s) and the behavior policy differs from the target policy, you need importance sampling. Otherwise there\u0026rsquo;s no reason to use it. But as mentioned, you can at least choose between ordinary and weighted importance sampling. The learning rate: could just be a fixed learning rate but might also decay over time How to improve the policy: remember that GPI consists of a policy evaluation step where we try to find \\(q_\\pi\\), and a policy improvement step where we use our estimate of the value function to update the policy. This improved policy might for example be \\(\\varepsilon\\)-greedy with respect to our estimate but there are other options  All of these are in principle independent choices: some combinations might work together better than others but choosing one option in each of these dimensions does give a valid RL algorithm.\nIn a function approximation setting, and in Deep RL in particular, we also need to make many \u0026ldquo;engineering choices\u0026rdquo;. These are certainly important and can determine whether an algorithm works really well or doesn\u0026rsquo;t even converge. But what I have hopefully convinced you of is that all the core building blocks from tabular RL appear in essentially the same way in deep RL and really are fundamental \u0026ldquo;building blocks\u0026rdquo;.\nNext up: Part 2, where we will apply a similar breakdown to policy optimization methods.\n  or expected Sarsa, they are the same for a deterministic policy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1610553480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610553480,"objectID":"92594d17778954ada1ca960096b63503","permalink":"https://ejenner.com/post/rl-building-blocks-1/","publishdate":"2021-01-13T16:58:00+01:00","relpermalink":"/post/rl-building-blocks-1/","section":"post","summary":"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 1\n  of a series with such an overview, covering value-based methods (mainly in a tabular\n  setting).\n  ","tags":["Reinforcement learning"],"title":"Building Blocks of RL Part I: Value-based methods","type":"post"},{"authors":null,"categories":null,"content":"\\( \\DeclareMathOperator*{\\argmax}{argmax} \\DeclareMathOperator*{\\argmin}{argmin} \\DeclareMathOperator*{\\E}{\\mathbb{E}} % expected value \\newcommand{\\R}{\\mathbb{R}} \\) This is my attempt to tell a story1 about how you might invent variational autoencoders (VAEs). There are already great introductory posts doing this and if you haven\u0026rsquo;t seen VAEs before, I would strongly recommend you start with one of those. These introductions often start with autoencoders and then extend them to VAEs. In contrast, we will start by asking ourselves how to generate new data that matches a training distribution and then motivate VAEs from there. We won\u0026rsquo;t assume an autoencoder-like architecture a priori, instead it will arise naturally from this motivation.\nOf course just this motivation of generating new samples given a training distribution won\u0026rsquo;t uniquely lead to VAEs \u0026ndash; after all, there are other good options for generative models. So at some points we will need to make design decisions but hopefully they won\u0026rsquo;t come out of the blue.\nOne pedagogical note before we start: if this derivation of VAEs seems unnecessarily long and convoluted, that\u0026rsquo;s because it is. The goal is not to arrive at the VAE framework as quickly as possible, but rather to make each step seem natural and to avoid any unmotivated \u0026ldquo;magical\u0026rdquo; jumps. It\u0026rsquo;s probably best if you forget for a moment what you know about VAEs, in particular that they consist of an encoder and a decoder. We will get there at the very end but initially this preconception might just be confusing.\nGenerative models The goal in generative modeling is the following: we have some family of probability distributions \\(\\mathcal{P}\\). Given a set of training examples \\(\\mathcal{D}\\) (assumed to be i.i.d.), we now want to pick the distribution \\(p \\in \\mathcal{P}\\) from our family that maximizes the likelihood \\(\\prod_{x \\in \\mathcal{D}} p(x)\\). Equivalently, we can maximize the log-likelihood: \\[\\argmax_{p \\in \\mathcal{P}} \\sum_{x \\in \\mathcal{D}} \\log p(x)\\] For now, we will consider the simper special case where we only have a single datapoint \\(x\\) and want to maximize \\(\\log p(x)\\) (we will get back to the general case at the end).\nOptimizing over a family of probability distributions is very abstract. To turn this into a problem we can actually solve numerically, we will use a parameterized family \\(p_\\theta(x)\\) and optimize over the parameter \\(\\theta \\in \\R^p\\). \\(p_\\theta(x)\\) should be differentiable with respect to \\(\\theta\\), then we can at least find a local optimum for our problem using gradient ascent.\nThis still leaves the question which parameterized family we should use. This is the largest crossroads we\u0026rsquo;ll face in this post: there are many good options to choose from. The challenge we face is to find a good trade-off between having a flexible family of distributions and keeping the number of parameters manageable. For example, if \\(x\\) takes on discrete values, we could in principle use the full categorical distribution over all possible values of \\(x\\). This would be as flexible as possible but the number of parameters might be huge. If \\(x\\) describes a \\(28\\times 28\\) binary image, there are already \\(2^{28 \\cdot 28} = 2^{784} \\approx 10^{236}\\) possible values that \\(x\\) can take, meaning we\u0026rsquo;d need about that many parameters.\nThe way we will deal with this problem is to use a continuous mixture of simple distributions. We will introduce a new latent variable \\(z \\in \\mathbb{R}^k\\) on which we define a very simple distribution \\(p(z)\\), for example a unit normal, \\(z \\sim \\mathcal{N}(0; I)\\). Then we parameterize a distribution \\(p_\\theta(x|z)\\), which gives us \\[p_\\theta(x) = \\int p(z) p_\\theta(x|z) dz\\] The important point is that for a fixed \\(z\\), \\(p_\\theta(\\cdot|z)\\) may be an extremely simple distribution. In the example above, we could use an independent Bernoulli for each of the 784 pixels, which requires only 784 parameters. But because we additionally have a dependency on \\(z\\), the marginal distribution \\(p_\\theta\\) can be much more complex (in particular, the pixels are typically not independent). Of course the dependency on \\(z\\) will require some additional parameters but this could just be a reasonably sized neural network, which gives us far fewer parameters than the \\(2^{784}\\) that a full categorical distribution would require.\nThis already describes our model. Sampling from this model is easy: we sample \\(z \\sim p(z)\\) and then for this \\(z\\) sample \\(x \\sim p_\\theta(x|z)\\). By assumption, both of those distributions are very simple (and we can also choose them to be easy to sample from).\nBut evaluating the likelihood \\(p_\\theta(x)\\) of a datapoint is intractable for most models \\(p_\\theta(x|z)\\) and \\(p(z)\\) because it requires calculating a complicated integral. Even if we only care about generating samples, this is a problem: to train the model, we want to maximize \\(\\log p_\\theta(x)\\), but we can\u0026rsquo;t even evaluate it (nor its gradient, for the same reason).\nThe cleverness of VAEs lies in using the right approximations to make this optimization problem tractable, and that is what the remainder of this post is about.\nVariational inference First, we expand the log-likelihood a bit. For any value of \\(z\\), we have \\[\\log p(x) = \\log p(z) p(x|z) - \\log p(z|x)\\] (not writing our the dependency on \\(\\theta\\) for now). The first term is easy to evaluate. So if we could evaluate the second term, our problem would be solved (sidenote on motivation2).\nThis is where variational inference comes into play (here is a tutorial if you want to dive a bit deeper but that\u0026rsquo;s not necessary for this post). The idea of variational inference is that you have some distribution \\(p\\) that you care about, but which is intractable to work with. So you define a family \\(\\mathcal{Q}\\) of simpler distributions and then find \\[q^* := \\argmin_{q \\in \\mathcal{Q}} D(q\\Vert p)\\] where \\(D(\\cdot \\Vert \\cdot)\\) is the Kullback-Leibler divergence (which measures \u0026ldquo;distances\u0026rdquo; between probability distributions, though it is not a metric in the mathematical sense3). We can then use \\(q^*\\) in place of \\(p\\) whenever we need to evaluate it.\nThis may sound like an enourmous amount of computational overhead: to just evaluate our objective, we will have to solve an entire optimization problem each time! We will later find a way to alleviate this issue but for now, let\u0026rsquo;s just ignore it and understand how we would solve the problem naively.\nTo apply this to our problem, we will approximate \\(p(z|x)\\) with a simpler distribution \\(q_\\lambda(z)\\), parameterized by a new parameter \\(\\lambda\\). For example, \\(q_\\lambda\\) could be a Gaussian and \\(\\lambda\\) would be its mean and covariance matrix. Note that while \\(q_\\lambda(z)\\) does not explicitly depend on \\(x\\), the optimal parameter \\(\\lambda^*\\) does depend on \\(x\\) because we minimize the Kullback-Leibler divergence between \\(q_\\lambda\\) and \\(p(\\cdot | x)\\).\nThe variational inference problem is now minimizing \\[D(q_\\lambda\\Vert p(\\cdot | x)) = \\mathbb{E}_{z \\sim q_\\lambda}\\left[\\log q_\\lambda(z) - \\log p(z|x)\\right]\\] This still contains the \\(p(z|x)\\) term that we can\u0026rsquo;t evaluate. But we can get rid of that by writing \\[\\mathbb{E}_{z \\sim q_\\lambda}\\left[\\log q_\\lambda(z) - \\log p(z|x)\\right] = \\mathbb{E}_{z \\sim q_\\lambda}\\left[\\log q_\\lambda(z) - \\log p(x, z)\\right] + \\log p(x)\\] We have reintroduced \\(\\log p(x)\\), which is intractable, but crucially, it doesn\u0026rsquo;t depend on \\(\\lambda\\). So to solve the minimization problem above, we can also minimize the expected value on the right. Usually, we instead maximize the negative of that: \\[\\argmax_\\lambda \\E_{z \\sim q_\\lambda}\\left[\\log p(x, z) - \\log q_\\lambda(z)\\right]\\] The objective \\[\\E_{z \\sim q_\\lambda}\\left[\\log p(x, z) - \\log q_\\lambda(z)\\right]\\] is called the evidence lower bound (ELBO) because it is a lower bound on the log evidence \\(\\log p(x)\\): \\[\\E_{z \\sim q_\\lambda}\\left[\\log p(x, z) - \\log q_\\lambda(z)\\right] = \\log p(x) - D(q_\\lambda\\Vert p(\\cdot|x)) \\leq \\log p(x)\\] For now this fact isn\u0026rsquo;t really interesting, but it will become relevant later.\nMaximizing the ELBO is finally a tractable problem: we can write \\[\\log p(x, z) = \\log p(z) + \\log p_\\theta(x|z)\\] which is something we can easily evaluate. The expectation is over \\(q_\\lambda\\) which also doesn\u0026rsquo;t pose a problem4.\nCombining the optimization problems Let\u0026rsquo;s briefly recap our progress so far. We originally wanted to find \\[\\argmax_\\theta \\log p_\\theta(x)\\] which we rewrote as \\[\\argmax_\\theta \\log p_\\theta(x, z) - \\log p_\\theta(z|x)\\] for an arbitrarily chosen \\(z\\). We then used variational inference to approximate the intractable term as \\[\\log p_\\theta(z|x) \\approx \\log q_{\\lambda^*}(z)\\] where \\(\\lambda^*\\) is the solution to the variational problem: \\[\\lambda^*(\\theta) = \\argmax_\\lambda \\E_{z \\sim q_\\lambda}\\left[\\log p_\\theta(x, z) - \\log q_\\lambda(z)\\right]\\]\nSo we could now in principle plug in this approximation and solve \\[\\argmax_\\theta \\log p_\\theta(x) \\approx \\log p_\\theta(x, z) - \\log q_{\\lambda^*}(z)\\] but there are problems with this. First, note that \\(\\lambda^*\\) depends on \\(\\theta\\). If we for example use gradient ascent to optimize over \\(\\theta\\), we would need to find the new optimal \\(\\lambda\\) after each gradient step. Second, using an arbitrarily chosen \\(z\\) is kind of silly: we optimized \\(q_\\lambda\\) such that the entire distribution approximates \\(p(z|x)\\) well, we should make use of this entire distribution.\nSo let\u0026rsquo;s go back. We know that the solution to \\[\\argmax_\\theta \\log p_\\theta(x) = \\argmax_\\theta \\log p_\\theta(x, z) - \\log p_\\theta(z|x)\\] is the same for any \\(z\\). So we can also maximize \\[\\E_{z \\sim q}\\left[\\log p_\\theta(x, z) - \\log p_\\theta(z|x)\\right]\\] instead, for an arbitrary distribution \\(q\\). Plugging in our approximation, we get \\[\\E_{z \\sim q}\\left[\\log p_\\theta(x, z) - \\log q_{\\lambda^*}(z)\\right]\\]\nThe question now is which distribution \\(q\\) to use. But note that by using \\(q = q_{\\lambda^*}\\), we again get the ELBO, this time as the objective for our original optimization problem. This is a good choice for two reasons:\n The ELBO is a lower bound on the evidence, \\(\\text{ELBO} \\leq \\log p(x)\\). If we used another distribution \\(q\\), we wouldn\u0026rsquo;t have any guarentee that we\u0026rsquo;re optimizing for the right thing if the approximation \\(p(z|x) \\approx q_{\\lambda^*}(z)\\) became bad enough. This way, we\u0026rsquo;re at least optimizing a lower bound on what we really care about. We saw above that we need to find the new \\(\\lambda^*(\\theta)\\) after each update to \\(\\theta\\), which is very inefficient. But the ELBO is already our objective for \\(\\lambda\\), so now we have the same optimization objective for both parameters and can optimize them jointly.  With this choice of \\(q = q_{\\lambda^*}\\), the joint optimization problem becomes \\[\\argmax_{\\theta, \\lambda} \\E_{z \\sim q_\\lambda}\\left[\\log p_\\theta(x, z) - \\log q_\\lambda(z)\\right]\\] We can use the reparameterization trick / pathwise gradients to optimize this efficiently with gradient ascent.\nUsing an encoder For a single datapoint \\(x\\), we now have an efficiently solvable problem. But now we get back to the more interesting setting of an entire dataset \\(\\mathcal{D}\\). We then want to optimize the likelihood of the entire dataset: \\[\\sum_{x \\in \\mathcal{D}} \\log p_\\theta(x)\\] The problem is that \\(q_\\lambda(z)\\) is supposed to approximate \\(p(z|x)\\), so the optimal \\(\\lambda\\) is different for each datapoint \\(x\\). Full variational inference would mean using a separate parameter \\(\\lambda\\) for each datapoint. The optimization would then be over \\(\\theta, \\lambda_1, \\ldots, \\lambda_n\\), where \\(\\lambda_i\\) is the parameter for the \\(i\\)-th datapoint. This again gets us into the realm of a huge number of parameters and computational infeasibility.\nSo instead, we use amortized variational inference. This means that instead of optimizing parameters for \\(q_\\lambda\\) for each \\(x\\), we learn a function \\(x \\mapsto \\lambda\\). This function is trained to approximate the optimal solution \\(\\lambda^*(x)\\). The downside is that we\u0026rsquo;re introducing yet another approximation, which can only worsen how well we maximize the likelihood \\(p(x)\\). But the big advantage is that evaluating it is much cheaper than solving an entire optimization problem.\nIn practice, this means we train a neural network \\(f_\\varphi(x)\\) to find the best \\(\\lambda\\) (in terms of the objective above) for a given \\(x\\). We then use \\(q_{f_\\varphi(x)}(z)\\) in place of \\(q_{\\lambda^*}(z)\\). To make the notation a bit nicer, we write this as \\[q_\\varphi(z|x) := q_{f_\\varphi(x)}(z)\\]\nThen we finally get the VAE objective: \\[\\argmax_{\\theta, \\varphi} \\E_{z \\sim q_\\varphi}\\left[\\log p(z)p_\\theta(x|z) - \\log q_\\varphi(z|x)\\right]\\]\nConnection to VAEs in practice As you\u0026rsquo;ve probably guessed by now, \\(p_\\theta(x|z)\\) is the decoder of a VAE and \\(q_\\varphi(z|x)\\) is the encoder. The ELBO can be rewritten as \\[\\begin{aligned}\\E_{z \\sim q(\\cdot | x)} \\left[\\log p(z)p(x|z) - \\log q(z|x)\\right] \u0026amp;= \\E_{z \\sim q(\\cdot | x)} \\log p(x|z) - \\E_{z \\sim q(\\cdot | x)} \\log \\frac{q(z|x)}{p(z)}\\\\\\\n\u0026amp;= \\E_{z \\sim q(\\cdot | x)}\\log p(x | z) - D(q(\\cdot|x)\\Vert p)\\end{aligned}\\] which gives us the interpretation as \u0026ldquo;reconstruction + regularization loss\u0026rdquo; that you may have encountered elsewhere (to treat this as a loss that is minimized, you would multiply everything by \\(-1\\)).\n\\(q_\\varphi\\) is typically chosen as a normal distribution, because that makes the KL divergence in the ELBO easy to calculate if \\(p(z)\\) is chosen as a unit normal. The choice of \\(p_\\theta\\) depends on the type of data. As mentioned, for binary images we might use independent Bernoulli distributions for each pixel. For continuous output, a normal distribution is a common choice.\nConclusion We saw how to arrive at VAEs starting from a purely generative motivation, without assuming an autoencoder architecture a priori. Interestingly, this gives a very different impression than the \u0026ldquo;autoencoder perspective\u0026rdquo;: what we really care about is the decoder, whereas the encoder is just a useful trick to be able to train the decoder efficiently.\nThis doesn\u0026rsquo;t mean that the autoencoder perspective is wrong of course. Having an encoder can be intrinsically useful for some applications, and this is something which is missing in this post. But I think the perspective we took here demonstrates that the VAE architecture is far less arbitrary than it may seem when starting from autoencoders.\nFurther reading  The CS 228 lecture notes on VAEs take a somewhat similar approach to this post in terms of emphasizing the variational inference perspective. They Also contain details on some points that I basically ignored, for example on the reparameterization trick Carl Doersch\u0026rsquo;s tutorial on VAEs contains much more detail and also has a different motivation for why we want to approximate \\(p(z|x)\\) (namely to use that to estimate the integral by sampling values of \\(z\\) that contribute the most) There is also a tutorial by Kingma and Welling, the authors who introduced VAEs. You could also look at their original paper but that\u0026rsquo;s a lot terser    Michael Nielsen calls this \u0026ldquo;discovery fiction\u0026rdquo;, mentioned for example here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you intrinsically care about \\(p(z|x)\\), for example because you hope the latent variables will have an interesting meaning, this and parts of the remaining post are unnecessary, you\u0026rsquo;ll get to VAEs more directly. But my point is that you don\u0026rsquo;t need that motivation \u0026ndash; VAEs arise pretty naturally even if you only care about finding a good model \\(p_\\theta(x)\\) of the training data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In particular, the Kullback-Leibler divergence is not symmetric, which raises the question why we use \\(D(q\\Vert p)\\) and not \\(D(p\\Vert q)\\). The reason is that the latter would itself lead to an intractable optimization problem and so we wouldn\u0026rsquo;t have made any progress.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you\u0026rsquo;ve read the previous footnote: this is the point where using the other KL divergence would mean we\u0026rsquo;re stuck because we have an expectation with respect to \\(p(z|x)\\)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1609940700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609940700,"objectID":"26703bdafec1ddf8497c02555dfdf1ba","permalink":"https://ejenner.com/post/vae-generative/","publishdate":"2021-01-06T14:45:00+01:00","relpermalink":"/post/vae-generative/","section":"post","summary":"  Variational autoencoders are usually introduced as a probabilistic extension of autoencoders\n  with regularization. An alternative view is that the encoder arises naturally as a tool\n  for efficiently training the decoder. This is the perspective I take in this post, deriving\n  VAEs without assuming an autoencoder architecture a priori.\n  ","tags":["Deep learning"],"title":"VAEs from a generative perspective","type":"post"},{"authors":null,"categories":null,"content":"Most of the objects that appear in mathematics can be thought of as sets with additional \u0026ldquo;structure\u0026rdquo;. For example, a group is a set \\(G\\) with an operation \\(G \\times G \\to G\\) fulfilling certain axioms. This operation is what makes a group feel more \u0026ldquo;structured\u0026rdquo; than a simple set of elements. A topological space is a set equipped with a topology and there is a myriad of other examples (graphs, ordered sets, vector spaces, metric spaces and measure spaces to name a few).\nBut \u0026ldquo;structure\u0026rdquo; in this sense is a somewhat elusive concept. We know it when we see it but it\u0026rsquo;s hard to describe explicitly \u0026ndash; which is why I just gave some examples and hoped you knew what I meant. (Sidenote: there is also a more formal notion of structure in mathematical logic but that\u0026rsquo;s not the topic of this post)\nThe goal of this post is not to give a formal definition of structure \u0026ndash; I\u0026rsquo;m not sure how helpful that would even be \u0026ndash; but rather to describe different perspectives that may be useful when thinking about it. To guide us, we will consider one particular question: what does it mean to say that object A has \u0026ldquo;more structure\u0026rdquo; than object B? For example, a vector space has more structure than a group, which has more structure than a simple set. We will start with more formal (but also more boring) perspectives and then work our way towards more speculative and fuzzy ones.\nNotation We\u0026rsquo;ll fix a set \\(X\\) and consider the different possible structures that can be imposed on \\(X\\). Calligraphy letters like \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) refer to the set of all possible structures of some type, for example the set of all groups on \\(X\\). Particular instances are written as \\(A \\in \\mathcal{A}\\) (e.g. a particular group on \\(X\\)).\nWe will write \\(\\mathcal{A} \\prec \\mathcal{B}\\) for the informal notion that \\(\\mathcal{A}\\) has more structure than \\(\\mathcal{B}\\), for example \\(\\text{fields} \\prec \\text{groups}\\). An alternative way to think about this (which hopefully explains why the \\(\\prec\\) sign points the way it does) is that fields are a special case of groups (each field is also a group), which means that the set of fields is in some sense a subset of the set of groups. This leads us right into the first perspective on structure.\nStructure can be canonically removed If \\(\\mathcal{A} \\prec \\mathcal{B}\\) (\\(\\mathcal{A}\\) has more structure than \\(\\mathcal{B}\\)), then there is a canonical way to turn any instance \\(A \\in \\mathcal{A}\\) into an instance \\(B \\in \\mathcal{B}\\). As an example, a vector space can be canonically turned into a group by just using vector addition as the group operation and ignoring scalar multiplication. Or any metric space can be treated as just a topological space by using the topology induced by the metric and ignoring the metric itself (category theory footnote1).\nStructure leads to smaller symmetry groups If \\(\\mathcal{A} \\prec \\mathcal{B}\\), then the automorphism group2 of \\(A \\in \\mathcal{A}\\) is smaller than the group of the corresponding \\(B \\in \\mathcal{B}\\) (where \u0026ldquo;corresponding\u0026rdquo; means that \\(B\\) is just \\(A\\) with parts of the structure removed as described in the previous section).\nFor example, we can treat the real numbers as a metric space or as a topological space. For a metric space, the automorphism group consists of only isometries (i.e. maps that don\u0026rsquo;t change distances between points), which for the real numbers are only translations. If we treat them as a topological space though (which has a lot less structure), then the automorphisms are all the homeomorphisms of the real number line, which form a much larger group.\nMore structure leads to fewer structure-preserving maps If we consider two sets \\(X\\) and \\(Y\\), there are \\(|Y|^{|X|}\\) maps from \\(X\\) to \\(Y\\). If we now introduce a group structure, most of those maps are typically not homomorphisms, i.e. not structure-preserving. If we then turn the groups into rings, even fewer maps will additionally be compatible with the ring multiplication. So adding structure reduces the number of maps which preserve all of that structure (which is pretty obvious when put like that).\nThe previous perspective is a special case of this, where \\(Y = X\\) and we only consider automorphisms rather than any structure-preserving maps, so it shouldn\u0026rsquo;t be surprising that we also got fewer automorphisms if we had more structure. But I think it\u0026rsquo;s a very important special case that deserves to be treated seperately because the interpretation via symmetries makes it much more intuitive than this general version.\nStructure allows more definitions and theorems Now we start getting into slightly more hand-wavy territory. If \\(\\mathcal{A} \\prec \\mathcal{B}\\), then there are more concepts we can define for objects with structure \\(\\mathcal{A}\\) than for objects with structure \\(\\mathcal{B}\\). We can also prove more and stronger theorems about objects with structure \\(\\mathcal{A}\\) then about objects with structure \\(\\mathcal{B}\\). This is related to the previous observation that any object with structure \\(\\mathcal{A}\\) can be canonically turned into one with structure \\(\\mathcal{B}\\). The important observation here is that this process \u0026ldquo;is compatible with definitions and theorems\u0026rdquo; (I told you it was getting hand-wavy). What I mean by that is that if some property holds for \\(B\\), then it also holds for any object \\(A\\) which can be turned into \\(B\\) by forgetting parts of its structure.\nSome examples:\n On a Riemannian manifold, we can do things like measure the angle at which two curves intersect, which is simply not a concept that makes sense for a manifold without a metric Rings allow us to talk about divisibility, which does not have an analogon if only a group structure is avaliable All vector spaces have a basis but the same is not true for all modules (which have less structure than vector spaces)  While adding structure \u0026ldquo;preserves definitions and theorems\u0026rdquo;, it can sometimes make definitions trivial or collapse certain distinct concepts into one. For example, divisibility becomes very boring on fields because every element is divisible by every other element (except 0).\nAlgorithmic complexity Now it\u0026rsquo;s getting really hand-wavy, so activate your lack-of-rigor-deflectors.\nLoosely speaking, algorithmic complexity (or Kolmogorov complexity) measures how long the shortest possible description of some object is. This can be formally defined for bit sequences but I will appeal to your intuition to also apply it to other things like mathematical structures, without explicitly specifying how to encode those as bit sequences.\nOne connection between structure and complexity is quite obvious: if \\(\\mathcal{A} \\prec \\mathcal{B}\\), then describing \\(\\mathcal{A}\\) is more complex. For example, describing what a field is takes slightly longer than just describing what a group is because there are more axioms that need to be specified. Similarly, defining a Riemannian manifold is more complex than just defining what a topological space is. Note that I switched from talking about a description of \\(\\mathcal{A}\\), e.g. the set of all groups, to talking about a definition of what a group is. But in terms of descriptive complexity those are essentially the same since the shortest description of the set of all groups on \\(X\\) consists of a definition of what a group is and then saying \u0026ldquo;all groups on \\(X\\)\u0026rdquo;.\nThere are some cases where this complexity perspective becomes a bit of a stretch. For example, it\u0026rsquo;s not obvious that defining a metric space is more complex than defining a topological space (unlike in the case of fields and groups, where the hierarchy is clear). I\u0026rsquo;d argue that it is in fact more complex because you need concepts like the real numbers which are pretty complicated compared to topological spaces. But there might be other examples where there really is a very short description of something which nevertheless has a lot of structure in terms of the other perspectives above. This is fine: our goal here is not to give a formal definition of structure but rather to list some of the properties that are typically associated with it.\nThere is another, more interesting way in which complexity comes into play when talking about structure: how long is an average description of a particular element \\(A \\in \\mathcal{A}\\) (given a description of \\(\\mathcal{A}\\))? Some examples to build intuition about this:\n Specifying a topological space can be extremely complex. Because there is such a large number of possible topologies on a fixed set, most of them need to have very long desriptions. Also note that those topological spaces with very simple descriptions are often those that have a natural additional structure. For example, to define the Euclidean topology on \\(\\mathbb{R}^n\\), we usually first define its vector space structure, use that to define a metric and then use that to define a topology Specifying a field on a finite set is very easy: there is at most one anyways (up to isomorphism) If the cardinality of \\(X\\) is prime, there is also only one group on \\(X\\). Otherwise, there might be more but still far fewer than there are topologies[I think, citation needed]  This seems to point towards more structure making it easier to specify a particular instance. But this is not always the case. For example, a Riemannian smooth manifold has more structure than just a smooth manifold (according to all the previous perspectives). But since every smooth manifold can be equipped with a Riemannian metric but that metric is not uniquely determined by the manifold, describing a Riemannian manifold usually takes longer than just describing a smooth manifold without a metric, because the choice of metric needs to be specified.\nIn general, adding structure means that there might be additional choices that need to be specified (such as a Riemannian metric) but it can also impose restrictions (for example, many topological spaces can\u0026rsquo;t be turned into metric spaces). These two factors pull the descriptive complexity of individual instances in opposite directions.\nInherent structure of objects This is not a new perspective for thinking about structure. Instead, I will give an example for a possible \u0026ldquo;application\u0026rdquo; of the complexity-based perspective. Hopefully that will illustrate how these perspectives can be useful to have in your mental toolkit.\nThere are interesting connections between what we discussed in the previous section and Kolmogorov sufficient statistics. Intuitively speaking, the Kolmogorov sufficient statistic of a bit string is the part of that string that has \u0026ldquo;structure\u0026rdquo; in the sense of not being algorithmically random. Any bit string can be efficiently described by first describing its Kolmogorov sufficient statistic (which is a list of bit strings with the same \u0026ldquo;structure\u0026rdquo;) and then specifying its algorithmically random component (by giving its index in that list).\nThis is exactly analogous to describing e.g. a group on \\(X\\) by first defining what a group is (or rather defining an enumeration of all groups on \\(X\\)) and then saying \u0026ldquo;the 14th object in that enumeration\u0026rdquo;. The important property of the Kolmogorov sufficient statistic is that this description in two parts is efficient (there is no shorter description using some other scheme, up to an additive constant). As an example where this is not the case, we could also specify a group by first defining an enumeration of monoids and then saying \u0026ldquo;the 247th object in that enumeration\u0026rdquo;. But because there are many more monoids than group, this description would probably be inefficient in most cases: we save ourselves the specification of a single axiom but we pay by needing to specify a much higher index.\nPerhaps this idea can be used to define the \u0026ldquo;true inherent structure\u0026rdquo; on an object as its Kolmogorov sufficient statistic. But fleshing that out is a topic for another post.\nConclusion In summary, here are all the perspectives we talked about: If an object has more structure, \u0026hellip;\n this structure can always be canonically removed it has fewer symmetries there are fewer maps between it and other objects that preserve all the structure more concepts can be defined and more theorems proven specifying the class of objects with that structure tends to be more complex specifying that particular object is often easier because the structure restricts the space of options, but there are exceptions    In category theory these are forgetful functors but I\u0026rsquo;m just interested in intuition here, not formalism. In this example, there is also a natural way to turn any abelian group into a \\(k\\)-vector space, for a given field \\(k\\), by tensoring with \\(k\\). But that vector space won\u0026rsquo;t be over \\(X\\) anymore and in many other cases there is no canonical way to add structure at all.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The automorphism group is the set of all isomorphisms from an object to itself (which becomes a group via composition as the group operation)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1609246980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609246980,"objectID":"69ba7c2506c90c124be77a33189948aa","permalink":"https://ejenner.com/post/perspectives-on-structure/","publishdate":"2020-12-29T14:03:00+01:00","relpermalink":"/post/perspectives-on-structure/","section":"post","summary":"  \"Structure\" is a concept that keeps popping up when thinking about mathematics\n  but it's hard to pin down what it is exactly. I discuss several different perspectives\n  for thinking about it.\n  ","tags":["Structure","Math"],"title":"Ways to think about structure in mathematics","type":"post"},{"authors":null,"categories":null,"content":"(Note: this is an analysis of one aspect of the Karger-Stein algorithm, it\u0026rsquo;s not meant to be a beginner-friendly introduction)\nKarger\u0026rsquo;s algorithm randomly contracts a graph and surprisingly, this can be used to find a minimum cut with probability \\(\\mathcal{O}(n^{-2})\\), where \\(n\\) is the number of vertices of the graph. This is a much, much higher probability than sampling a graph cut uniformly at random would give! But it still means we need to run the algorithm \\(\\mathcal{O}(n^2\\log n)\\) times to get a high success probability. Karger\u0026rsquo;s algorithm can be implemented in \\(\\mathcal{O}(n^2)\\) time1, which gives an overall runtime of \\(\\mathcal{O}(n^4 \\log n)\\) \u0026ndash; not great, there are much faster deterministic algorithms.\nTo understand how the Karger-Stein algorithm improves upon that, we need the following key result that forms the foundation for Karger\u0026rsquo;s algorithm:\nTheorem2: When Karger\u0026rsquo;s algorithm contracts a graph from \\(n\\) to \\(r\\) vertices, any given mincut survives with probability \\(\\geq {r \\choose 2}/{n \\choose 2} = \\frac{r (r - 1)}{n (n - 1)}\\).\nIn particular, for \\(r = 2\\), we get the \\(\\mathcal{O}(n^{-2})\\) probability mentioned above.\nBut note the following: if we make only a few contractions, \\(r \\lesssim n\\), then mincuts are almost guaranteed to survive! This is the key insight that allows us to improve the runtime of Karger\u0026rsquo;s algorithm, leading to the improved Karger-Stein algorithm.\nThe idea is the following: first we contract the graph down to roughly \\(\\frac{n}{b}\\) vertices, where \\(b\\) is small enough that mincuts are very likely to survive. Then we branch: we again contract the graph down by a factor of \\(b\\), but we do so \\(a\\) times independently from one another. \\(a\\) needs to be chosen high enough that mincuts are very likely to survive in at least one of the branches. We repeat this process until we have contracted the graph down to just 2 vertices3. If we chose \\(a\\) and \\(b\\) right, at least one of the final leaves of our computational tree will likely contain a mincut. So we return the best cut we\u0026rsquo;ve found among all the leaves.\nThe Karger-Stein algorithm as it was originally described and as it is usually presented uses \\(a = 2\\) and \\(b = \\sqrt{2}\\). So we always split the computation into two branches and reduce the number of vertices by a factor of \\(\\sqrt{2}\\) before branching again. But in this post, I would like to motivate where these numbers come from, as well as show that they\u0026rsquo;re not the only ones that work. So in the following, we\u0026rsquo;re going to analyze the \u0026ldquo;generalized Karger-Stein algorithm\u0026rdquo; with arbitrary \\(a\\) and \\(b\\).\nSuccess probability As mentioned above, any minimum cut survives a contraction from \\(n\\) to \\(r\\) vertices with probability \\(\\geq {r \\choose 2}/{n \\choose 2} = \\frac{r (r - 1)}{n (n - 1)}\\). I said we first contract to \u0026ldquo;roughly\u0026rdquo; \\(\\frac{n}{b}\\) vertices \u0026ndash; to be precise we contract until \\(\\lceil \\frac{n}{b} + 1\\rceil\\) vertices are left, this will give us a nice bound. The probability that a mincut survives this contraction is4:\n\\begin{equation} \\begin{split} p \u0026amp;\\geq \\frac{\\lceil \\frac{n}{b} + 1 \\rceil \\lceil \\frac{n}{b} \\rceil}{n (n - 1)}\\\\\\\n\u0026amp;\\geq \\frac{(\\frac{n}{b} + 1) \\cdot \\frac{n}{b}}{n (n - 1)}\\\\\\\n\u0026amp;\\geq \\frac{1}{b^2} \\end{split} \\end{equation}\nWe can now apply this bound recursively: After we have contracted to \\(\\lceil \\frac{n}{b} + 1 \\rceil\\) vertices, we can forget that this is a partially contracted graph, and just treat this number as the \u0026ldquo;new \\(n\\)\u0026rdquo;.\nWe will write \\(p_k\\) for the survival probability if there are \\(k\\) levels of recursion left before we reach the leaves of the tree. So \\(p_0\\) will be the probability in the leaves of the recursion tree. Depending on when precisely we stop the recursion and what method we use to finish the contraction, \\(p_0\\) might take different values, but all that matters for us is that it is some constant.\nUsing the bound we found above, we get the following recurrence: \\[p_{k + 1} \\geq 1 - \\left( 1 - \\frac{p_k}{b^2} \\right)^a\\] What\u0026rsquo;s going on here? \\(\\frac{p_k}{b^2}\\) is a lower bound on the probability that any given mincut survives in one particular branch. So \\(\\left(1 - \\frac{p_k}{b^2}\\right)^a\\) is an upper bound on the probability that the mincut survives in none of the \\(a\\) branches, and consequently \\(1 - \\left( 1 - \\frac{p_k}{b^2} \\right)^a\\) is a lower bound on the probability that it survives in at least one.\nThis recurrence doesn\u0026rsquo;t have an obious solution we can just read off but with some rewriting, we can get something that\u0026rsquo;s good enough for our purposes. Substituting \\(z_k := \\frac{b^2}{p_k} - 1\\), we get\n\\begin{equation} \\begin{split} z_{k + 1} \u0026amp;= \\frac{b^2}{p_{k + 1}} - 1 \\\\\\\n\u0026amp;\\leq \\frac{b^2}{1 - \\left(1 - \\frac{1}{z_k + 1}\\right)^a} - 1\\\\\\\n\u0026amp;= \\frac{b^2 \\left(z_k + 1\\right)^a}{\\left(z_k + 1\\right)^a - z_k^a} - 1\\\\\\\n\u0026amp;\\leq \\frac{b^2 \\left(z_k + 1 \\right)^a}{a z_k^{a - 1}} - 1\\\\\\\n\u0026amp;\\leq \\frac{b^2}{a} z_k + \\text{const} \\end{split} \\end{equation}\nwhere we used \\(z_k \\geq 1\\) in the last step. The constant term may depend on \\(a\\) and \\(b\\) but not on \\(z_k\\).\nIf \\(a \\geq b^2\\), then \\(z_k \\in \\mathcal{O}(k)\\) which means that \\(p_k \\in \\Omega\\left(\\frac{1}{k}\\right)\\). The depth of recursion for a graph with \\(n\\) vertices is \\(\\Theta(\\log n)\\), so the overall success probability is \\(\\Omega\\left(\\frac{1}{\\log n}\\right)\\).\nWhat this means in words: if we create enough branches (at least \\(b^2\\)) compared to how long we contract before branching again, then we get quite a high success probability \u0026ndash; \\(\\Omega\\left(\\frac{1}{\\log n}\\right)\\) means that \\(\\log^2 n\\) runs are enough to get an overall success probability that approaches 1 as \\(n \\to \\infty\\).\nBut what if \\(a \u0026lt; b^2\\), i.e. if we don\u0026rsquo;t have enough branches at each stage? Then the inequality derived above only yields \\(z_k \\in \\mathcal{O}\\left(\\left(\\frac{b^2}{a}\\right)^k\\right)\\) so the success probability \\(p_k\\) can be exponentially low in \\(k\\). This means we\u0026rsquo;d have to repeat the algorithm a potentially exponential number of times, which would make it useless.\nThat still leaves the question: why does the Karger-Stein algorithm use \\(a = b^2\\) in particular, when \\(a \u0026gt; b^2\\) would give a success probability at least as high? For that we need to turn to the runtime complexity.\nRuntime The runtime of the Karger-Stein algorithm can be described with the following recurrence: \\[T(n) = aT\\left(\\frac{n}{b}\\right) + \\mathcal{O}(n^2)\\] The \\(\\mathcal{O}(n^2)\\) term is for contracting down to roughly \\(\\frac{n}{b}\\) vertices. At that point, we solve \\(a\\) smaller version of the original problem, each of size \\(\\frac{n}{b}\\). That leads to the \\(aT\\left(\\frac{n}{b}\\right)\\) term.\nThis kind of recurrence is exactly what the Master theorem is for. In this case, if we choose \\(a = b^2\\), we get a runtime of \\(\\Theta(n^2 \\log n)\\) for a single run of the Karger-Stein algorithm. We already saw that with \\(a \u0026lt; b^2\\), we get an exponentially low success probability, so that choice isn\u0026rsquo;t interesting anyway. Finally, if \\(a \u0026gt; b^2\\), we get a high success probability, but the runtime becomes \\(\\Theta(n^c)\\), where \\(c := \\frac{\\log a}{\\log b} \u0026gt; 2\\).\nWe can summarize all our results (and a few I didn\u0026rsquo;t mention) in one table:\n   Condition Time for single run Success probability Total runtime Comment     \\(a \u0026lt; b^2\\) \\(\\Theta(n^2)\\) exponentially low in \\(n\\) exponential in \\(n\\) Too little branching   \\(a = b^2\\) \\(\\Theta(n^2 \\log n)\\) \\(\\Omega\\left(\\frac{1}{\\log n}\\right)\\) \\(\\mathcal{O}(n^2 \\log^3 n)\\) Just right   \\(a \u0026gt; b^2\\) \\(\\Theta(n^c)\\) with \\(c \u0026gt; 2\\) \\(\\Omega(1)\\) \\(\\Theta(n^c)\\) with \\(c \u0026gt; 2\\) Unnecessarily much branching    The \u0026ldquo;total runtime\u0026rdquo; column contains the runtime that is needed to achieve a high success probability by repeating the Karger-Stein algorithm often enough (at least if \\(n\\) is large enough). This is the complexity that we want to minimize in practice.\nWe can now see that combining the analysis of the success probability with the runtime analysis explains why the Karger-Stein algorithm uses \\(a = b^2\\): in the other cases, we are either very unlikely to succeed and therefore need too many runs, or we are taking unnecessarily long for a single run.\nBut also note that any choice of a \u0026ldquo;branching factor\u0026rdquo; \\(a\\) works, as long as we then choose \\(b = \\sqrt{a}\\). So splitting the computation up into just two subproblems is a reasonable and simple choice, but from a purely asymptotic perspective it is arbitrary.\n  There is also an implementation in \\(\\mathcal{O}(m)\\), where \\(m\\) is the number of edges, but for the Karger-Stein algorithm that won\u0026rsquo;t make a difference and we\u0026rsquo;ll ignore it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n David Karger: Global min-cuts in RNC, and other ramifications of a simple min-cut algorithm, SODA 1993\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Usually we stop a bit before 2 vertices and just compute the mincut from there using other methods but that doesn\u0026rsquo;t matter here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This and the following analysis is based on the paper by Karger and Stein: A new approach to the minimum cut problem, Journal of the ACM 1996. The difference is just that I consider arbitrary \\(a\\) and \\(b\\), rather than just \\(a = 2\\) and \\(b = \\sqrt{2}\\).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1607274000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607274000,"objectID":"7c59ca9ec8932905795ed1e2dcf7f1c8","permalink":"https://ejenner.com/post/karger-stein/","publishdate":"2020-12-06T18:00:00+01:00","relpermalink":"/post/karger-stein/","section":"post","summary":"  The Karger-Stein algorithm is an improvement over Karger's beautiful contraction\n  algorithm for minimum graph cuts. In this post, I show how it finds the perfect\n  tradeoff between finding a mincut with high probability and finding it quickly.\n  In the course of doing so, we will also understand where the somewhat opaque\n  factor of sqrt(2) comes from.\n  ","tags":["Graphs"],"title":"Trading off speed against the probability of success in the Karger-Stein Algorithm","type":"post"},{"authors":null,"categories":null,"content":"Karger\u0026rsquo;s algorithm is a randomized algorithm for finding global mincuts in graphs. For my Bachelor\u0026rsquo;s thesis, I aimed to find out if and how Karger\u0026rsquo;s algorithm can be generalized to other types of problems, for example $s$-$t$-mincuts or normalized cuts. It turns out that this is provably impossible in a formal sense. But in addition to this impossibility result, my research also led me to a new algorithm for seeded segmentation that uses Karger\u0026rsquo;s algorithm to sample many different cuts and then takes their \u0026ldquo;average\u0026rdquo;. A paper with these findings was accepted as an Oral at ICCV 2021.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"ff02108cefa365c06e91201459a09b07","permalink":"https://ejenner.com/project/karger/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/karger/","section":"project","summary":"Can Karger's algorithm be extended to different kinds of cut problems?","tags":["Graphs"],"title":"Extensions of Karger's algorithm","type":"project"},{"authors":null,"categories":null,"content":"As part of the course Fairness, Accountability, Confidentiality and Transparency in AI at the University of Amsterdam, I replicated the paper Fairness without Demographics through Adversarially Reweighted Learning (Lahoti et al., 2020) together with three other Master\u0026rsquo;s students. We also performed additional experiments to test whether the method transfers from tabular to image data. We wrote a report with our findings for the ML Reproducibility Challenge 2020.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"e053a307201d61e3602f6e59e630c303","permalink":"https://ejenner.com/project/fact-ai/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/fact-ai/","section":"project","summary":"Replication of an ML fairness paper together with other Master's students.","tags":["Deep Learning","Fairness"],"title":"ML Reproducibility Challenge","type":"project"},{"authors":null,"categories":null,"content":"Equivariant deep learning has many similarities to theoretical physics, which have already lead to ideas from physics being applied to the field. But they typically use convolutions, whereas physics uses differential operators. In an internship at QUVA Lab, I developed the theory of equivariant differential operators \u0026ndash; Steerable PDOs \u0026ndash; in order to close this gap. I also implemented steerable PDOs as an extension of the e2cnn library to allow others to easily apply them in their own work.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"a85ea2cc510516a95fcf6cfc67c4374f","permalink":"https://ejenner.com/project/steerable-pdos/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/steerable-pdos/","section":"project","summary":"Using differential operators for equivariant NNs.","tags":["Equivariance","Deep Learning"],"title":"Steerable PDOs","type":"project"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ve heard that when starting a new sketchbook, you should begin by drawing some silly doodles on the first page to break the paralysis that a fresh book full of beautiful blank pages can induce. So for my first blog post I chose the silliest topic that came to mind, namely the intersection of ethics, economics and special relativity.\nDiscounting is the idea that obtaining value \\(V\\) some time \\(\\Delta t\\) into the future is worth only \\(f(\\Delta t)V\\) now where \\(f(\\Delta t) \u0026lt; 1\\) is the discount factor. What exactly \u0026ldquo;value\u0026rdquo; means depends on the context. For now we will talk about money as an example but we will get back to this point later.\nThis definition of discounting raises an obvious problem: the distance in time to future events is not invariant under Lorentz boosts, so by discounting like this, your value assignments become dependent on your frame of reference. Now, as long as you never accelerate, your frame of reference will stay the same and this isn\u0026rsquo;t a practical problem (though you may still have objections on aesthetic grounds). But as soon as you change your state of motion, you\u0026rsquo;ll run into problems.\nImagine that you\u0026rsquo;re about to leave for your vacation in the Alpha Centauri system, taking the new Starline 90C moving at 90% the speed of light. Suddenly, Omega comes along and offers you a deal: it will pay you $90 right now but in return you will have to pay $100 once you arrive on Alpha Centauri in (format \u0026quot;%0.2f\u0026quot; (/ 4.34 0.9)) 4.82 years (as seen from your current frame of reference on earth). This sounds like a great deal to you: you discount at 3% per year, so the $100 you\u0026rsquo;ll have to pay are only worth (format \u0026quot;$%0.2f\u0026quot; (* (expt 0.97 4.82) 100)) $86.35 to you now.\nSo you accept the deal, board your spaceship and begin accelerating towards Alpha Centauri. But as you do, you feel your value assignments shifting \u0026ndash; or rather you realize that you will be on Alpha Centauri in only (format \u0026quot;%0.2f\u0026quot; (* 4.82 (sqrt (- 1 0.81)))) 2.10 years in your new reference frame because of time dilation. This means that you suddenly value the $100 you will have to pay on arrival at (format \u0026quot;$%0.2f\u0026quot; (* (expt 0.97 2.1) 100)) $93.80, just because you stepped into a spaceship and took off.\nSo clearly, improper discounting is an important financial hazard for space tourists. But what should you do instead, if you want to keep your normal discounting procedures while on earth?\nNow we need to get back to what we meant by \u0026ldquo;value\u0026rdquo;. If value refers to money, then discounting is closely related to the fact that you can invest money you already have now, so getting money at a later point in time is less valuable. The amount of time used for discounting calculations should then be the proper time of the money, so it depends on whether you were going to leave most of your money invested on earth (in which case you should discount with the 4.82 years in earth\u0026rsquo;s frame of reference) or whether you were going to invest it aboard the Starline 90C (in which case you should discount with the travel time of 2.10 years).\nBut what if you want to discount pure utilities? In that case the question is no longer one of economics but of ethics. We are looking for a discount function \\(f\\) that satisfies the following criteria:\n \\(f\\) assigns some discount factor to each point in your future light cone \u0026ndash; all of those are events that you might be able to influence and therefore need to take into account for utility calculations. On the world line of your current frame of reference, these factors coincide with the old discounting factor \\(f(t)\\) \u0026ndash; \u0026ldquo;It all adds up to normality\u0026rdquo;. \\(f\\) is invariant under Lorentz boosts in the sense that if your velocity suddenly changed and you recalculated all discount factors, they would remain the same. Essentially, your ethical judgements don\u0026rsquo;t change just because you take a flight to Alpha Centauri at relativistic speeds.  I think there is only one way of discountig that satisfies all of these desiderata1: use the spacetime interval instead of the time as measured in your current reference frame.\nThe spacetime interval between two points is\n\\begin{equation}\\label{eq:spacetime_interval} \\Delta s = \\sqrt{\\left(c\\Delta t\\right)^2 - \\left(\\Delta x\\right)^2 - \\left(\\Delta y\\right)^2 - \\left(\\Delta z\\right)^2} \\end{equation}\nwhere \\(\\Delta t\\) is their time difference (what we used for discounting before) and \\(\\Delta x, \\Delta y, \\Delta z\\) are the spatial distances. The nice thing about \\(\\Delta s\\) is that it is invariant under Lorentz transformations, so if instead of discounting with \\(f(\\Delta t)\\), you discount with \\(f(\\Delta s)\\), then your value assignments won\u0026rsquo;t change when you change frames of reference.\nWhat consequences does this have? For small spatial distances, not much changes. The \\(c\\) in equation \\eqref{eq:spacetime_interval} means that as long as you could reach an event while travelling much slower than the speed of light, \\(\\Delta s \\approx \\Delta t\\). On the other hand, events that are close to the edges of your future light cone have \\(\\Delta s\\) close to 0, meaning they are discounted only very weakly. So you\u0026rsquo;d care about things that happen in 4.3 years (earth frame) on Alpha Centauri almost as much as about what happens on earth right now \u0026ndash; and much more than about things that happen on on earth 4.3 years into the future.\nIf you think this is absurd, I completely agree. One way to get around this is to give up desideratum 3 above. Maybe if your velocity changes very suddenly, it\u0026rsquo;s justified for your value judgements to also change very suddenly? But there is of course another way to avoid these consequences: if \\(f(\\Delta s)\\) is independent of \\(\\Delta s\\), then there is no weird bias towards things that happen close to the edge of your light cone either. In other words: don\u0026rsquo;t discount pure utilities at all.\nFor me, the second option is much more appealing. There are already good reasons against discounting utilities, this simply adds yet another one. It\u0026rsquo;s by no means an argument showing that you can\u0026rsquo;t consistently discount events in the far future in a relativistic universe. But if you do, there will be some consequences that I find rather unintuitive. Either you care a lot about what happens on faraway star systems in the far future, or how much you value different things changes whenever you change your velocity. Take your pick.\n  because every point in your future light cone lies on the world line of some reference frame that you can reach by a Lorentz boost\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1592648700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592648700,"objectID":"b79ae52cf351c219e428d1da408c1bea","permalink":"https://ejenner.com/post/discounting-relativistic-universe/","publishdate":"2020-06-20T12:25:00+02:00","relpermalink":"/post/discounting-relativistic-universe/","section":"post","summary":"  For people who want to discount the future, special relativity creates\n  some challenges. There are different ways to handle those but none\n  seem completely satisfactory which may be yet another argument against\n  discounting pure utilities.\n  ","tags":["Physics"],"title":"Discounting in a relativistic universe","type":"post"},{"authors":["Erik Jenner","Maurice Weiler"],"categories":null,"content":"","date":1592438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592438400,"objectID":"882a731b93b5596e999fcdba1d2ac3a2","permalink":"https://ejenner.com/publication/steerable-pdos/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/publication/steerable-pdos/","section":"publication","summary":"We present a framework for equivariant partial differential operators, generalizing existing approaches and narrowing the gap between PDOs and convolutions.","tags":["Deep Learning","Equivariance"],"title":"Steerable Partial Differential Operators for Equivariant Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"I don’t use cookies, track your visits or collect any personal information about you. The website does remember whether you use the dark or light theme but this information stays on your computer.\nBecause this blog is hosted with Github Pages, Github may log visits, including IP adresses as per their privacy policy.\nYou can contact me at erik@ejenner.com.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://ejenner.com/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"I don’t use cookies, track your visits or collect any personal information about you. The website does remember whether you use the dark or light theme but this information stays on your computer.","tags":null,"title":"Privacy","type":"page"}]