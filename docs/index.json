
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m a Computer Science PhD student at UC Berkeley working on how to align powerful future AI systems with human values, to reduce the existential risk posed by AI. I am advised by Stuart Russell and part of the Center for Human-Compatible AI. I am grateful to be supported by fellowships from the Future of Life Institute and Open Philanthropy.\nSee Google Scholar for papers I’ve written, and the Alignment Forum for my blog posts about earlier-stage work.\n","date":1633392000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633392000,"objectID":"a4e464a1a97bf0184b407fe8db196761","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m a Computer Science PhD student at UC Berkeley working on how to align powerful future AI systems with human values, to reduce the existential risk posed by AI. I am advised by Stuart Russell and part of the Center for Human-Compatible AI.","tags":null,"title":"Erik Jenner","type":"authors"},{"authors":null,"categories":null,"content":"Improvements to the activation function in neural networks have hit diminishing returns over recent years. Whereas the choice between Sigmoids and ReLUs can make or break your training, the same cannot be said for the difference between Swish, Mish, GLish, and Phish. (I made up one of these, but I bet you don’t know which.)\nIn our recent paper “Swoosh: Rethinking Activation Functions”1, we argue that the problem is an overly narrow conception of what an activation function can be. Our key idea is the following observation:\nDifferent activation functions work best for different problems. A good activation function should thus have many hyperparameters.\nActivation functions with learnable parameters have been proposed before, but they suffer from two flaws:\nThe number of parameters is usually quite small. For example, Swish uses only a single parameter. That’s about 175 billion fewer parameters than even a tiny model like GPT-3! And it’s well known that more parameters = more powerful. They use parameters. As we will argue, using hyperparameters instead offers countless advantages. Introducing Swoosh The simplest version of the Swoosh activation function is $$\\operatorname{Swoosh}_{f}(x) := f(Wf(x) + b),$$ where $f$ is some simple non-linear function (such as $f(x) := \\max(x, 0)$), $W$ is a matrix and $b$ is a vector. The entries of $W$ and $b$ are hyperparameters of the Swoosh activation function.\nFor even better results, we recommend using $f = \\operatorname{Swoosh}_{g}$ for a simple non-linear function $g$, or even\n$$f = \\operatorname{Swoosh}_{\\operatorname{Swoosh}_g}$$\nand so on. Note that in this case, the hyperparameters for each instance of Swoosh should be tuned independently.\nAstonishingly, our experiments show that just a single hidden layer is enough for most tasks when using the Swoosh activation function. In slogan form: 2-layer networks are all you need!2 The full network is then simply $$\\operatorname{net}(x) = U \\operatorname{Swoosh}(Vx + a) + b.$$\nWhy hyperparameters? One of our key contributions is that we treat the entries of $W$ and $b$ in Swoosh as hyperparameters. This is a much better choice than using them as learnable parameters, like existing activation functions typically do. We’ll briefly sketch a few of the advantages:\n“Hyper” is greek for “above”, so the name should already tell us that hyperparameters are above (i.e. better than) parameters. Specifically for Swoosh, the network output is differentiable in these hyperparameters, which means you can still use SGD! All common opitimizers and backpropagation implementations work out of the box for optimizing Swoosh hyperparameters. Neural networks with higher parameter count tend to have higher memory requirements, longer training and inference times, higher compute costs, and a bigger environmental footprint. By using hyperparameters instead of parameters, we can keep the parameter count low. Models sometimes perform worse on the test dataset than on the training data.3 Using the well-known technique of choosing hyperparameters based on test performance, this is an issue of the past with Swoosh. Conclusion and future work We have shown how expanding our conception of what an activation function can be solves several key problems in deep learning, such as test set generalization and high compute requirements. We believe that this insight will not only revitalize the field of activation functions, but that it in fact heralds the end of the AI Winter we currently find ourselves in.\nTo make it as easy as possible for practitioners to adopt Swoosh, we have worked closely with the PyTorch team. We are excited to announce that Swoosh is available now in the PyTorch library! Tensorflow and JAX users can easily implement it in a few lines of code.\nIn the future, we are planning to improve the design of Swoosh in various ways. For example, we believe it could be promising to replace the matrix multiplication $Wx$ with a convolution, especially on inputs with translational symmetry such as images. There is a vast unexplored space of possible Swoosh architectures to discover, and we invite other researchers to join us in this exciting journey!\nWe will release the full paper upon acceptance of this blog post. ↩︎\nIn fact, in an upcoming paper we will show that for Swoosh activation functions, there are universal approximation theorems with bounded width and depth 1, improving on a result by Maiorov et al. ↩︎\nOr so I’ve heard, this has never happened to me personally. ↩︎\n","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"b7a60b720b410aaef06180ca79843d97","permalink":"https://ejenner.com/post/swoosh/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/post/swoosh/","section":"post","summary":"Introducing the new Swoosh activation function. Perfect test set generalization\nguaranteed.\n  ","tags":[""],"title":"Swoosh: Rethinking Activation Functions","type":"post"},{"authors":null,"categories":null,"content":"einsum is one of the most useful functions in Numpy/Pytorch/Tensorflow and yet many people don’t use it. It seems to have a reputation as being difficult to understand and use, which is completely backwards in my view: the reason einsum is great is precisely because it is easier to use and reason about than the alternatives. So this post tries to set the record straight and show how simple einsum really is.\nThe general syntax for einsum is\neinsum(\u0026#34;some string describing an operation\u0026#34;, tensor_1, tensor_2, ...) with an arbitrary number of tensors after the string. (I’ll be saying “tensors” but they could just as well be Numpy arrays.)\nLet’s look at an example. Say we have two matrices, A and B, with shapes such that we can multiply them as A @ B. Using einsum, we can write this matrix product as\neinsum(\u0026#34;ij,jk-\u0026gt;ik\u0026#34;, A, B) The interesting part is the string, \u0026#34;ij,jk-\u0026gt;ik\u0026#34;. These einsum strings always follow the same structure:\n\u0026#34;\u0026lt;input indices\u0026gt; -\u0026gt; \u0026lt;output indices\u0026gt;\u0026#34; In this example, the input indices are \u0026#34;ij,jk\u0026#34;. These define letters for indices into each input tensor. Different tensors are comma-separated, so ij refers to A and jk refers to B. ij means we call the first axis of A i and the second axis j, and similarly, jk defines names for the axes of B. The specific letters we use are arbitrary here, we could just as well write \u0026#34;ga,aw-\u0026gt;gw\u0026#34;. There has to be one index per axis of the input tensor—in our case, both A and B have two axes (they’re matrices), so both get two indices.\nWhat’s important is that we’re using the same letter, j, both for the second axis of A and for the first axis of B. That’s not just an arbitrary definition, it has an effect on the result! Think of it this way: the entire left-hand side, \u0026#34;ij,jk\u0026#34; defines a three-dimensional tensor, indexed by i, j, and k. We get its elements by multiplying the corresponding elements of A and B:\nproduct[i, j, k] = A[i, j] * B[j, k] So it matters that j appears twice—a string like \u0026#34;ij,lk\u0026#34; would define a four-dimensional tensor:\nproduct[i, j, l, k] = A[i, j] * B[l, k] (Don’t worry about the order of these indices into product—as we’ll see in a moment, the right-hand side of our string will explicitly specify the order we want).\nThe right side of the -\u0026gt; arrow describes how to get our final output from this product tensor. It’s very simple: any index that appears on the left (i.e. in the product tensor) but doesn’t appear on the right is summed over. So in our matrix multiplication example, since our output indices are ik, we sum over j. So the final result is\nout[i, k] = sum_j A[i, j] * B[j, k] Precisely a matrix multiply, as promised!\nAll of this generalizes in very nice and intuitive ways. On the left side of the -\u0026gt; arrow, we can have arbitrary patterns, and they’ll always describe a scheme for indexing into a product of the inputs. For example, the string \u0026#34;iij,kji,l\u0026#34; would define a four-dimensional tensor, given by\nproduct[i, j, k, l] = A[i, i, j] * B[k, j, i] * C[l] (for input tensors A, B, C). Note how much easier this is compared to a version without einsum:\nn = A.shape[0] product = A[t.arange(n), t.arange(n), :, None, None] \\ * B.permute(2, 1, 0)[:, :, :, None] \\ * C[None, None, None, :] Our output can now be any permutation of any subset of ijkl. For example, \u0026#34;iij,kji,l-\u0026gt;ki\u0026#34; would implicitly compute the product tensor above, then sum over j and l, and finally permute the result so the order of axes was ki. Contrast with how messy this would be without einsum:\n# With broadcasting and array indexing: n = A.shape[0] out = ( A[t.arange(n), t.arange(n), :, None, None] * B.permute(2, 1, 0)[:, :, :, None] * C[None, None, None, :] ).sum(1,3).T # With einsum: out = t.einsum(\u0026#34;iij,kji,l-\u0026gt;ki\u0026#34;, A, B, C) The main point is not that the einsum version is shorter—the point is that the other version took me 10 minutes to write and I’m still not sure it’s correct.\nThat concludes the description of einsum, but let’s look at some more examples to get a better intuition:\nSay you want to compute the transpose of the matrix product, (A @ B).T. What that means is just that you want the indices in the output flipped, so the string now becomes \u0026#34;ij,jk-\u0026gt;ki\u0026#34;. Sometimes you want to sum over all axes; in that case, you can just leave the right hand side empty. For example, \u0026#34;i,i-\u0026gt;\u0026#34; will compute the inner product of two input vectors. Just like you can have repeated indices in different input tensors, you can repeat indices within the same tensor. For example, \u0026#34;ii-\u0026gt;\u0026#34; computes the trace of a matrix. Or you could do \u0026#34;ii-\u0026gt;i\u0026#34; to get the diagonal as a vector. You can trivially add batch dimensions to any operation. For example, a batched inner product would be \u0026#34;bi,bi-\u0026gt;b\u0026#34;. A batched matrix multiply would be \u0026#34;bij,bjk-\u0026gt;bik\u0026#34;. If for some reason, your batch dimension is in the last position for the second batch of matrices, that’s no problem: \u0026#34;bij,jkb-\u0026gt;bik\u0026#34;. Batching also lets you take arbitrary diagonals of a tensor easily. For example, \u0026#34;ibi-\u0026gt;bi\u0026#34; will give you the diagonal along the first and …","date":1667606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667606400,"objectID":"0759702101df40b7da24fd834752c620","permalink":"https://ejenner.com/post/einsum/","publishdate":"2022-11-05T00:00:00Z","relpermalink":"/post/einsum/","section":"post","summary":"`einsum` is one of the most useful functions in Numpy/Pytorch/Tensorflow and yet many people don't use it. It seems to have a reputation as being difficult to understand and use, which is completely backwards in my view: the reason `einsum` is great is precisely because it is *easier* to use and reason about than the alternatives. So this post tries to set the record straight and show how simple `einsum` really is.\n  ","tags":[""],"title":"Einsum is easy and useful","type":"post"},{"authors":null,"categories":null,"content":"Last time, we introduced Schwartz distributions. Let’s briefly recap: a distribution is a function that maps certain types of functions to real numbers. We write $\\mathcal{D}(U)$ for the space of (compactly supported, smooth) functions on a space $U \\subseteq \\mathbb{R}^n$, called test functions. Distributions are continuous linear maps $T: \\mathcal{D}(U) \\to \\mathbb{R}$, and we write $\\mathcal{D}’(U)$ for the space of these distributions.\nAn important aspect of distributions is that they generalize functions $f: U \\to \\mathbb{R}$. This is not obvious at first—distributions have an entirely different type signature after all, so how can they be a generalization? What we mean by that is that there is a natural way to embed the space of (locally integrable) functions on $U$ into the space of distributions $\\mathcal{D}’(U)$. Namely, each function $f: U \\to \\mathbb{R}$ induces a distribution $T_f$ defined by $$\\langle T_f, \\varphi \\rangle := \\int_{U} f\\varphi d\\lambda^n.$$ Here, $\\langle T_f, \\varphi \\rangle$ is just a commonly used notation for $T_f(\\varphi)$, i.e. the distribution $T_f$ applied to the function $\\varphi$.\nFor “classical” functions, we can do things like add them, convolve them, take their derivative, and many other operations. Given that distribution generalize classical functions, it’s natural to ask whether we can also generalize these operations. It turns out that this is possible in a (to me) surprising number of cases, in a surprisingly simple way!\nAddition Let’s start with a warm-up: just like we can add classical functions, we can add two distributions $S$ and $T$. Specifically, we define $S + T$ as the distribution given by $$\\langle S + T, \\varphi \\rangle := \\langle S, \\varphi \\rangle + \\langle T, \\varphi\\rangle.$$ In general, this will be how we define distributions: we just say how to evaluate them on an arbitrary function $\\varphi$. If this were a math textbook, we’d also have to show that the distributions we define this way are indeed continuous and linear in $\\varphi$, but since this is a blog post, we’ll skip that part. The goal here is only to get a good understanding of the definitions.\nNow we get to an important theme for this post: we’ve just defined addition of distributions, but does this definition really generalize the definition for classical functions? In other words: say we have two functions $f$ and $g$, both from $U$ to $\\mathbb{R}$. There are now two things we could do:\nAdd $f$ and $g$ as functions, then turn the result into a distribution, i.e. $T_{f + g}$. Turn $f$ and $g$ into distributions, then add them as distributions, i.e. $T_f + T_g$. We really, really want 1. and 2. to be equivalent! This is a good example of how definitions in math can be good or bad: in principle, we’re free to define addition of distributions however we want—but if our definition doesn’t generalize the definition we already use for classical functions, then it will be really hard to work with and probably just not useful.\nLuckily, it’s easy to check that our definition is a good one: we have $$\\begin{aligned} \\langle T_{f + g}, \\varphi\\rangle \u0026amp;= \\int_U (f + g)\\varphi d\\lambda^n\\\\ \u0026amp;= \\int_U f\\varphi d\\lambda^n + \\int_U g\\varphi d\\lambda^n\\\\ \u0026amp;= \\langle T_f, \\varphi \\rangle + \\langle T_g, \\varphi\\rangle\\\\ \u0026amp;= \\langle T_f + T_g, \\varphi\\rangle. \\end{aligned}$$ This implies that $T_{f + g} = T_f + T_g$ (since distributions are themselves just functions, and if two functions are equal on all inputs, they’re the same).\nDerivatives Addition was pretty easy, but how are we supposed to define derivatives of distributions? This seems really hard at first. Do we need some kind of limit of finite differences, like for classical functions? But what are these finite differences supposed to look like?\nRecall that in Part I, we used the powerful tool of Wishful Thinking: just pretend that distributions behave like classical functions, calculate a bit, and then use the result as the definition.\nWe’re now in a better position to understand what is actually going on there. Our central demand of any new definition is that it should generalize the corresponding definition for classical functions. In the case of derivatives, this means we want $$\\partial_i T_f = T_{\\partial_i f}$$ for all functions $f$. ($\\partial_i := \\frac{\\partial}{\\partial x_i}$ is the $i$-th partial derivative). So let’s just consider the special case of distributions that are induced by classical functions for now (this is the “pretend that distributions behave like functions” step). We then have $$\\begin{aligned} \\langle \\partial_i T_f, \\varphi\\rangle \u0026amp;= \\langle T_{\\partial_i f}, \\varphi\\rangle\\\\ \u0026amp;= \\int_U (\\partial_i f) \\varphi d\\lambda^n\\\\ \u0026amp;= -\\int_U f (\\partial_i \\varphi) d\\lambda^n\\\\ \u0026amp;= -\\langle T_f, \\partial_i \\varphi\\rangle. \\end{aligned}$$ We used integration by parts here, and made use of the fact that $\\varphi$ is compactly supported and $U \\subseteq \\mathbb{R}^n$ is open, so the boundary term vanishes.\nAnd now comes the …","date":1646521200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646521200,"objectID":"d6fc065757139e0cf7f330187cbfbc79","permalink":"https://ejenner.com/post/distributions-operations/","publishdate":"2022-03-06T00:00:00+01:00","relpermalink":"/post/distributions-operations/","section":"post","summary":"  As promised in part I, we can do a lot of the same things with Schwartz\n  distributions as with classical functions. To see how, we'll cover\n  derivatives, convolutions, and Fourier transforms of distributions.\n  ","tags":["Math"],"title":"Distributions Part II: What can we do with distributions?","type":"post"},{"authors":null,"categories":null,"content":"The following figure1 is a good illustration of why random search typically works better than grid search for hyperparameter optimization: The idea here is that the performance depends on two hyperparameters, $x$ and $y$, and one of them ($x$ in this case) is much more important. More specifically, the performance can be written as $f(x) + g(y)$, where $f(x)$ is the green curve at the top and $g(y)$ the yellow curve on the side.\nBoth random and grid search test 9 different $(x, y)$ pairs. But random search also tests 9 different values of $x$, whereas grid search tests only 3. So since the $x$-dependency dominates overall performance, random search will very likely find a better parameter combination.\nI’d like to show another visualization of this idea, by stretching the space of hyperparameters to illustrate their relative importance.\nIf we have two hyperparameters that are equally important to overall performance, grid and random search look as follows: You should imagine that the best hyperparameter combination lies at some unknown point in these squares. The goal is to have tested at least one point relatively close to this optimum.2 Grid search is looking pretty good from that perspective.\nBut this figure is very misleading if one of the parameters is much more important than the other. In that case, we care a lot more about the distance along the important axis. We can visualize this by stretching the space of hyperparameters so that the more important hyperparameter has a longer axis: In this stretched figure, we can just look at distances without worrying about the different axes, because the geometry of the space already takes the relative importance of the two hyperparameters into account.\nSuddenly, grid search doesn’t look that great anymore, and random search is actually better at “uniformly” filling out the hyperparameter space under this new geometry.\nIf we knew the relative importance of the two hyperparameters, we could improve grid search by having more grid points along the more important axis—but often we don’t know that, at least not very precisely. And for random search, it luckily doesn’t matter.\nJames Bergstra and Yoshua Bengio: Random Search for Hyper-Parameter Optimization, https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf ↩︎\nThis is a different sort of simplifying assumption than we had above: now, the overall performance cannot necessarily be decomposed as $f(x) + g(y)$. Instead, we assume that performance depends monotonically on the weighted distance $a(x - x_{\\text{opt}})^2 + b(y - y_{\\text{opt}})^2$. Just like the previous assumption of additivity, this is pretty unrealistic, but I think it still serves as a good intuition. ↩︎\n","date":1640131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640131200,"objectID":"7aaffda2fc6f7bcc0962a7d7e4892d94","permalink":"https://ejenner.com/post/random-vs-grid-search/","publishdate":"2021-12-22T00:00:00Z","relpermalink":"/post/random-vs-grid-search/","section":"post","summary":"Random search usually works better than grid search\n  for hyperparameter optimization. This brief post suggests a way\n  to visualize the reason for this geometrically.\n  ","tags":["Machine learning"],"title":"Visualizing random vs grid search","type":"post"},{"authors":["Erik Jenner","Enrique Fita Sanmartín","Fred A. Hamprecht"],"categories":null,"content":"See our blog post or the ICCV presentation for a brief and simple overview.\n","date":1633392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633392000,"objectID":"f011d63ac54021d562d3a87a24e84dff","permalink":"https://ejenner.com/publication/karger/","publishdate":"2021-10-05T00:00:00Z","relpermalink":"/publication/karger/","section":"publication","summary":"We prove impossibility results showing that Karger's contraction algorithm cannot be extended to $s$-$t$-mincuts or normalized cuts. However, we show how extensions of Karger's algorithm can still be useful for seeded segmentation.","tags":["Graphs"],"title":"Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice","type":"publication"},{"authors":["Erik Jenner","Adam Gleave"],"categories":null,"content":"","date":1632528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632528000,"objectID":"28ddc09b7250ca14ff3c003c0bb520dd","permalink":"https://ejenner.com/publication/reward-preprocessing/","publishdate":"2021-09-25T00:00:00Z","relpermalink":"/publication/reward-preprocessing/","section":"publication","summary":"We present a method for simplifying a learned reward model before visualizing it and show that this can make the reward more interpretable.","tags":["Reinforcement learning"],"title":"Preprocessing Reward Functions for Interpretability","type":"publication"},{"authors":null,"categories":null,"content":"If you prefer videos, check out our ICCV presentation, which covers similar content as this blog post. For more details, see our paper.\nKarger’s contraction algorithm is a fast and very famous method for finding global minimum graph cuts. First published in 1993, it helped start a wave of other randomized algorithms for graph cut problems. And while many of these are asymptotically even faster, Karger’s algorithm remains important and fascinating in part because of its extreme simplicity – it’s description wouldn’t even require an entire postcard:\nContract a randomly chosen edge, i.e. merge the two nodes it connects into one node. Repeat until only two nodes are left.\nWe will discuss Karger’s algorithm in a bit more detail shortly. But first, this simplicity raises a question: can Karger’s algorithm be extended to help with other tasks than the global minimum cut problem it was originally meant to solve? That is the question we aim to answer in our new paper and which this blog post will focus on.\nOne extension that immediately suggests itself, and which we will discuss first, is that to \\(s\\)-\\(t\\)-mincuts. If you already know what \\(s\\)-\\(t\\)-mincuts are and why we’d want to find them, you can read on; if you’d rather have a brief refresher, there’s one in the expandable box below.\nWhat are \\(s\\)-\\(t\\)-mincuts and why care about them? \\(s\\)-\\(t\\)-mincuts (and the related max flows) play an enormously important role in computer science in general. But instead of giving a huge list, we’ll focus on just one application to computer vision: Say you have an image that you want to segment into fore- and background. You might have a neural network that can tell you how likely each pixel is to belong to either of these classes. But in the end, you don’t want all these numbers, you want a single “best guess” segmentation.\nYou could of course just assign each pixel to the more likely class: make a pixel part of the foreground whenever the network says it’s more likely to be foreground than background. But this would ignore dependencies between pixels: if there is a region where the network classifies most pixels as slightly more likely to be background but there are a few exceptions, then you probably don’t want these exceptions to be part of the foreground. We know that fore- and background are at least somewhat contiguous and we want to make use of this prior knowledge.\nOne approach to this is shown in the following figure:\nFigure 1: Example application of \\(s\\)-\\(t\\)-mincuts: we want to segment the top left image into foreground and background and have some degree of belief about the correct choice for each pixel. We combine all these degrees of belief into a graph (bottom left) and then find its \\(s\\)-\\(t\\)-mincut (bottom right), which corresponds to a segmentation of the original image (top right). Image from Xiao, Pengfeng et al. (2017), which in turn was adapted from Boykov, Yuri and Funka-Lea, Gareth (2006). We turn our image into a graph; each pixel becomes one of the gray nodes in the graph. Then we add two more nodes, \\(s\\) and \\(t\\) (in red and blue). They are “helper nodes” that represent the foreground and background. We add edges from each pixel node to both \\(s\\) and \\(t\\) and use the probabilities from our neural network to determine positive weights for these edges (for example, a probability of 0.5 would mean that the edge connecting that pixel to \\(s\\) has the same weight as the edge to \\(t\\)).\nWe also add edges between neighboring pixel nodes (the yellow edges in the figure). Their weights describe how similar two pixels are; they could just be calculated from brightness differences or they could come from an edge detector network.\nThe goal is now to cut this entire graph into two halves, one containing \\(s\\) and the other containing \\(t\\). And we want to do this in such a way that the total weight of all edges that are cut is as small as possible – strongly connected nodes should remain on the same side of the cut.\nFor each pixel, we will have to cut either its edge to \\(s\\) or to \\(t\\); this is where the probabilities predicted by the neural network come in. But we will also have to cut edges between pixels themselves, and this is what biases the segmentation towards one that doesn’t split up homogeneous areas of the image.\nGlobal mincuts and Karger’s algorithm Karger’s algorithm doesn’t find \\(s\\)-\\(t\\)-mincuts, it finds global minimum cuts. This means that we want to separate a graph into two parts while minimizing the total weight of edges that need to be cut – all without having seeds \\(s\\) and \\(t\\) that need to be separated.\nThe following figure illustrates how Karger’s algorithm works on a simple example graph: we first selects an edge at random, in this case the one between \\(b\\) and \\(c\\) (in red), and then contract the chosen edge. We then repeat this process until only two nodes are left (on the very right). These nodes define a cut of the original graph into the partitions …","date":1631266860,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631266860,"objectID":"939afb5f779383ac862f7ff0d677efea","permalink":"https://ejenner.com/post/karger-extensions/","publishdate":"2021-09-10T11:41:00+02:00","relpermalink":"/post/karger-extensions/","section":"post","summary":"If you prefer videos, check out our ICCV presentation, which covers similar content as this blog post. For more details, see our paper.\nKarger’s contraction algorithm is a fast and very famous method for finding global minimum graph cuts.","tags":null,"title":"Extensions of Karger's algorithm","type":"post"},{"authors":null,"categories":null,"content":"Schwartz distributions are a generalization of functions from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\): strictly speaking, they aren’t such functions themselves, but you can do a lot of the same stuff with them that you can do with normal functions, such as taking derivatives, computing convolutions, and even Fourier transforms (at least in certain cases). And in some ways, they even make life easier compared to functions. For example, every distribution is infinitely differentiable! But of course, we do have to give up some things: distributions can’t be evaluated at a single point and it’s in general impossible to multiply two distributions.\nIn this series, we’ll try to understand all of these properties of distributions and more. I will focus on intuition but still give formal definitions of all the concepts we look at. As a secondary purpose, studying distributions will also be an excellent opportunity to practice finding good definitions. We will introduce many different operations on distributions and in each case, we will try to understand how one could come up with the definition in a natural way.\nMotivation In electrostatics, charge densities are used to model the amount of electric charge in different places. Such a charge density is a function \\(\\rho: \\mathbb{R}^3 \\to \\mathbb{R}\\) that assigns an amount of charge per volume to every point \\(x \\in \\mathbb{R}^3\\). From an experimental standpoint, these densities are only useful abstractions; what we can measure is at best the total charge in some volume. This charge \\(Q\\) is given by the integral of the density over the volume: \\[Q(V) = \\int_V \\rho(x) dx\\] for any subset \\(V \\subseteq \\mathbb{R}^3\\). You can even think of this as the definition of the density \\(\\rho\\): the only thing we care about is that when we measure the charge \\(Q(V)\\) in any volume \\(V\\), we get \\(\\int_V \\rho(x) dx\\).\nNow assume we observe the following: \\(Q(V) = 1\\) for any volume \\(V\\) that contains the origin but \\(Q(V) = 0\\) if \\(V\\) does not contain the origin. Intuitively, we conclude that there is a point charge with value 1 in the origin and no charge anywhere else. But how can we model this using a density \\(\\rho\\)? If \\(\\rho\\) is any (integrable) function, as we originally assumed, then we must have \\(\\rho(x) = 0\\) for \\(x \\neq 0\\).1 But in that case, \\(\\int_V \\rho(x) dx = 0\\) for all volumes \\(V\\), which contradicts our first observation.\nFor now, let’s just “define this problem away”: we’ll say that \\(\\rho(x) = \\delta(x)\\), where \\(\\delta(x)\\) is an object such that \\[\\int_V \\delta(x) dx := 1 \\text{ if } 0 \\in V, \\text{ otherwise } 0.\\] The word “object” here is code for “we’re pretty confused and don’t know what this thing is but we’d like to have something that behaves this way”.\nWe’ll develop a formal definition of \\(\\delta\\) soon. But first, let’s extend the original example a bit: suppose instead of being interested only in the charge inside some volume, we now introduce a charged test particle and want to know the potential energy it has due to the charge density \\(\\rho\\). This potential is given by \\[\\Phi \\propto \\int_{\\mathbb{R}^3} \\frac{1}{|x_0 - x|} \\rho(x) dx\\] for a test particle at position \\(x_0\\). So what is the potential energy if we have the point charge from before, \\(\\rho(x) = \\delta(x)\\)? So far, we have only defined \\(\\int_V \\delta(x) dx\\), and if \\(\\delta(x)\\) appears anywhere else, we don’t really know what to do with it. Remember, \\(\\int_V \\delta(x) dx\\) is just a notation we introduced to mean “1 if \\(0 \\in V\\) and 0 otherwise”, it’s not actually an integral in any usual sense.\nSo we will apply a powerful technique – wishful thinking. We just assume that \\(\\delta(x)\\) behaves the way we would intuitively like it to, and then worry later about constructing something that actually does behave that way. Since for \\(\\rho(x) = \\delta(x)\\), there is no charge outside the origin, all parts of the integral above except for \\(x = 0\\) ought to vanish. So let’s just write \\[\\int_{\\mathbb{R}^3} \\frac{1}{|x_0 - x|}\\delta(x) dx = \\int_{\\{0\\}}\\frac{1}{|x_0 - x|}\\delta(x)dx.\\] Since we’re only integrating over \\(\\{0\\}\\) now, we can set \\(x = 0\\) in \\(|x_0 - x|\\). Then this part doesn’t depend on \\(x\\) anymore and we get \\[\\int_{\\{0\\}}\\frac{1}{|x_0 - x|}\\delta(x)dx = \\frac{1}{|x_0|}\\int_{\\{0\\}}\\delta(x)dx.\\] But we know what to do with that last part, it’s 1! So the potential should be \\(\\Phi \\propto \\frac{1}{|x_0|}\\).\nWe can apply the same argument more generally to \\(\\int \\varphi(x) \\delta(x)dx\\) for other functions \\(\\varphi\\). So let’s “wish” that \\[\\int_{\\mathbb{R}^3} \\varphi(x) \\delta(x) dx := \\varphi(0)\\] hold for all functions \\(\\varphi\\). This contains our original definition of \\(\\delta(x)\\) as a special case, namely for the indicator function \\(\\varphi = 1_V\\).\nSchwartz distributions The defining property of \\(\\delta(x)\\) that we would like to have is \\[\\int_{\\mathbb{R}^3} \\delta(x) \\varphi(x) dx := \\varphi(0)\\] for arbitrary functions \\(\\varphi\\). We …","date":1625573700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625573700,"objectID":"d4559bf76103b76afb837e21099513a8","permalink":"https://ejenner.com/post/distributions-intro/","publishdate":"2021-07-06T14:15:00+02:00","relpermalink":"/post/distributions-intro/","section":"post","summary":"  Did you always want to know kind of object this weird Dirac delta \"function\"\n  actually is? Well, it's a Schwartz distribution. If that doesn't help much,\n  then keep reading.\n  ","tags":["Math"],"title":"Distributions Part I: the Delta distribution","type":"post"},{"authors":["Erik Jenner","Maurice Weiler"],"categories":null,"content":"","date":1623974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623974400,"objectID":"882a731b93b5596e999fcdba1d2ac3a2","permalink":"https://ejenner.com/publication/steerable-pdos/","publishdate":"2021-06-18T00:00:00Z","relpermalink":"/publication/steerable-pdos/","section":"publication","summary":"We present a framework for equivariant partial differential operators, generalizing existing approaches and narrowing the gap between PDOs and convolutions.","tags":["Deep Learning","Equivariance"],"title":"Steerable Partial Differential Operators for Equivariant Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"This is just a short PSA: if you can code, you can write small scripts to support your habits and productivity routines. I’m not talking about automating long, tedious tasks. Rather, I mean automating tasks that take about five or ten seconds but that you do every day, or at least very often.\nThe point is not actually that you’ll save five seconds. Instead, such scripts can give you the right nudges at the right time, make your life slightly less annoying, or automate stuff so you can’t accidentially forget doing it. The philosophy is that even very small inconveniences matter, and if you can spend a few minutes to make every day that follows even a tiny bit more convenient, it’s probably worth doing.\nThis is best illustrated by some examples, so that’s what the rest of this post consists of. But I’m sure this is only scratching the surface, so take these as inspiration and not as a comprehensive list of possibilities.\nChecklists One of my most important routines is a “daily checklist” that I go through at the end of each day, where I reflect a bit but mainly plan the next day. I used to just have a list of all the steps in a text file and went through those, but as the checklist grew, this became ever so slightly annoying, and I was liable to skip steps sometimes. So I wrote a small script (inside Emacs), which takes me through the checklist. I press a keyboard shortcut to start the checklist and I’m prompted with the first item, then I press the same shortcut again and I’m prompted with the next item and so on. For me, this alone is already an improvement over a list in a textfile, because I’m less likely to press the shortcut without actually doing the current item than I was to just skip to the next one when reading a checklist. But even better, for many items my script can give me additional nudges to make the checklist less annoying. For example, I have several items where I look at my weekly goals, my scheduled TODO items for the next day etc. Instead of opening the right files by hand, the script does that automatically once the corresponding prompt comes up. It doesn’t sound like much, but it adds up and makes going through the checklist much less annoying, which means I’m more likely to do it diligently. Using a script for this checklist has other advantages, which I’ll talk about below.\nOf course, this is not specific about a daily checklist you go through every evening, it applies to any checklist you use regularly and which has a reasonably large nummber of items.\nAutomatically close distracting programs Cal Newport recommends ending the work day with a shutdown ritual and truly relaxing afterwards. This doesn’t really work if you still have programs such as Slack open that distract you with work-related notifications, so my daily checklist script closes distracting programs automatically at the end of the checklist.\nOne thing I haven’t yet looked into but that would be nice is to also close individual browser tabs automatically. For example, you could close distracting websites whenever you suspend your PC, or whenever they were idle for a specific time, or when you start a Pomodoro, etc. A website blocker can of course serve a similar function, but unless you always block those website, the slight nudge from closing tabs automatically could be useful on top of a blocker.\nWelcome screen When I start my computer in the morning, I’m greeted by a fullscreen wallpaper, with a nice quote and my top priority for the day. For example, it might look like this1: The top priority is another thing where my daily checklist script comes in handy: it prompts me to enter one for the next day and stores it; the welcome screen can then later read it from disk and display it.\nUse APIs for the apps you use This is of course very specific to the apps you use, but I’ll give an example. I use Complice to plan my day and Beeminder for accountability for my goals. Yet another thing my daily checklist script does is to fetch all goals from Beeminder for which some progress is due the next day, using the Beeminder API. It then adds corresponding TODOs to Complice, using the Complice API. So before I add TODOs manually, the list on Complice is already pre-populated with what I need to do to stay on track with my Beeminder goals.\nOf course this is only possible when the services you want to automate stuff for expose an API, but it’s worth checking whether that’s the case. If it is, it’s often surprisingly easy to use, as long as you only want to do a few simple things. What I just described as an example can be implemented as a bash script with just a few lines, using curl to access the API and jq to parse JSON.\nFinal notes In case you’re interested, here is the script that displays the welcome screen and here is the one for the Complice/Beeminder API. That repository also contains the other things I mentioned in this post.\nI’m interested to apply this idea of automating parts of my routines even more. If you have ideas …","date":1618422120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618422120,"objectID":"225a74febf302eee5917bde9972bc037","permalink":"https://ejenner.com/post/automation-productivity/","publishdate":"2021-04-14T19:42:00+02:00","relpermalink":"/post/automation-productivity/","section":"post","summary":"  If you can program, you can use that to support your habits and automate\n  some routines. This post gives a few examples.\n  ","tags":["Productivity"],"title":"Scripting for personal productivity","type":"post"},{"authors":null,"categories":null,"content":"(Last updated: 2021-04-22)\nArguably one of the most important concepts in machine learning, taught in any introductory course, is under- and overfitting. The story goes like this: if your model is too simplistic, you won’t be able to fit the data well and get a large error, you underfit. On the other hand, if your model is too complex, it will fit any noise that is present in the data, i.e. you overfit. Such a model won’t generalize to the test set, so you also get a large error. Somewhere in between those two is a sweet spot with minimal test error.\nThe choice of words, under- and overfitting, already implies that we believe a tradeoff exists: they are two ends of a scale, and we need to find the point in the middle where we’re neither under- nor overfitting too much.\nUnder- and overfitting can be formalized using the notions of bias and variance (there’ll be a short recap in the next section). Underfitting means that we have a high test error because of high bias, while overfitting means that high variance causes a high error. Phrased in those terms, the tradeoff between under- and overfitting becomes the bias-variance tradeoff: methods with low bias tend to have high variance and vice versa.\nThis idea is ubiquitous in machine learning. So when I recently wanted to look up some details, I expected to find troves of information. Tons of empirical evidence, a formal definition of what exactly we mean by “tradeoff”, and hopefully even theorems showing that this tradeoff exists. As you can tell from the setup, that’s not what happened. So in this post, I’m going to describe what I found (and what I did not find). My goal is to clear up some potential misconceptions, and hopefully convince you that the bias-variance tradeoff is less simple and more interesting than you thought.\nPrimer: Bias and variance Under- and overfitting can be explained in terms of bias and variance. I’m going to discuss everything in a supervised learning setting. So the setup is the following:\nThere is a true (unknown) function \\(f(x)\\), which we want to approximate We have a dataset \\(D = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}\\) of datapoints that we use to learn an approximation, where \\(y_i = f(x_i)\\) We have some training process that takes in this dataset \\(D\\) and produces a function \\(\\hat{f}(x; D)\\) that approximates \\(f(x)\\) Finally, we imagine there is some distribution over datasets \\(D\\). This distribution is unknown an it’s a somewhat elusive concept, but think of it like this: we created our dataset with some process, e.g. by taking lots of photos and then having people label them. The distribution over datasets describes how likely this process is to produce any particular dataset. Now we can define precisely what we mean by bias and variance:\nThe bias is the difference between the expected value of \\(\\hat{f}(x; D)\\) and the true value \\(f(x)\\), i.e. \\[\\mathbb{E}_D[\\hat{f}(x; D)] - f(x)\\] By “variance” we mean the variance of \\(\\hat{f}(x; D)\\) with respect to \\(D\\), i.e. \\[\\mathbb{E}_D \\left(\\mathbb{E}_D[\\hat{f}(x; D)] - \\hat{f}(x; D)\\right)^2\\] Ideally, we want both bias and variance to be small. The reason is the bias-variance decomposition of the expected squared error1: \\[\\mathbb{E}_D (\\hat{f}(x; D) - f(x))^2 = \\text{bias}^2 + \\text{variance}\\] Assuming we ultimately want to minimize this expected squared error, it’s clear that all else being equal, we prefer methods with low bias and variance.\nOk, now what about the tradeoff between bias and variance? The idea is that methods with low bias tend to have high variance, and those with low variance tend to have high bias. So we can’t get both low bias and low variance, instead we need to find a tradeoff between the two, such that the expected squared error is minimized. This is illustrated by the following figure (from this essay, which I recommend if you want to gain more intuition about bias, variance, and their tradeoff): If we use an overly simplistic method, we have a high bias because our model just can’t get close to the true function \\(f\\), no matter what we feed in as training data. With a more complex model, we can fit the true function better, but the trained model \\(\\hat{f}\\) also depends a lot more on the training data \\(D\\), so the variance increases.\nThis explanation sounds sort of intuitive, but I find it a bit unsatisfying. Why exactly do complex models vary more depending on the training data? When does this hold? Can I have a theorem, please? And what is this “model complexity” anyways?\nA non-answer Unfortunately, the bias-variance decomposition and the bias-variance tradeoff are often conflated somewhat, so let’s get one thing out of the way: the bias-variance decomposition is not an explanation for the tradeoff (even though some handy-wavy explanations suggest this with varying degrees of explicitness). The only way this could work is if the total error was constant; this would indeed imply a tradeoff between bias and variance. But it clearly isn’t …","date":1617812340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617812340,"objectID":"5d061a6cb27d83343f625cf39cd2364f","permalink":"https://ejenner.com/post/bias-variance-tradeoff/","publishdate":"2021-04-07T18:19:00+02:00","relpermalink":"/post/bias-variance-tradeoff/","section":"post","summary":"  The bias-variance tradeoff is a key idea in machine learning. But I'll\n  argue that we know surprisingly little about it: when does it hold?\n  How does it relate to the Double Descent phenomenon? And what do we\n  even formally mean when we talk about it?\n  ","tags":["Machine learning"],"title":"Troubles with the Bias-Variance tradeoff","type":"post"},{"authors":null,"categories":null,"content":"This is a list of tips for improving your experience working with your computer. I focus on things that are quick to implement (say, 5 minutes to half an hour). Spending 5 minutes on something becomes worthwhile as soon as it saves you 5 seconds per month over 5 years, and I think all the tips here easily clear that bar. I’m not mentioning some things that are extremely valuable but take more time to do, stuff like “learn vim keybindings”. That’s not because those aren’t important – they might even be more important than these quick hacks. But they don’t fit well into this format because I would need to give a lot more context: who they are useful for, why it’s worth investing time in them etc.\nI’ve grouped these tips and tried to sort them by descending usefulness inside each category, but that’s of course quite subjective. As a final caveat, I don’t explain in detail how to set all of this up. If you’ve played around with configuration files before, the pointers I give are hopefully enough. If you haven’t, this might not be the easiest place to start.\nGeneral Make use of your Caps Lock key The Caps Lock key takes up extremely valuable keyboard real estate, even though most people never use it. I suggest mapping it to Control for non-vim users. If you use vim keybindings, you’ve probably already remapped it to Escape; in that case I would suggest using it as an Escape key when pressed and as Control when held down while pressing another key. How that works depends on your OS (I’m using caps2esc on Archlinux). Fuzzy finder for opening files Opening files using a file browser or by first opening an application and then using the “Open file” dialog is really slow. Instead, you can use a launcher that you can invoke with a keyboard shortcut. You then type in part of the path or filename and once you confirm your selection, the file is opened. On Linux, you can install for example Alfred or ULauncher (which also have additional functionality rather than just opening files). Or you can use rofi, which is extremely flexible but will require a bit more setup. Adjust your typematic delay and rate If you hold a key down, this simulates pressing that key a bunch of times at a high frequency. The typematic rate is this frequency and the typematic delay is the time delay before this effect kicks in. You can adjust these values to your liking, how that works depends on your OS/Desktop environment. On Linux with X, you can use xset r rate \u0026lt;delay in ms\u0026gt; \u0026lt;rate in Hz\u0026gt; (this is temporary, so put this in a script that will be executed on startup). Redshift Use redshift or f.lux to automatically adjust the color temperature of your screen according to the time of day. This will gradually make your screen look warmer during the evening. Dotfiles Put your dotfiles in a git repository. This page contains a few ideas as inspiration on how to best set this up. Personally, I use Dotbot, which means I can put all my configuration files into one directory and they will be symlinked to the right places. You can also do something similar for your /etc files using etckeeper. sshfs This lets you mount a remote directory inside your local file system, after which you can edit, create, move and delete files there using whichever tools you like to use for that on your local machine. It’s in the package repositories of most Linux distributions and using it is as simple as $ sshfs [user@]hostname:[directory] mountpoint Shell Use vim keybindings everywhere There are extensions for most browsers that let you browse web pages with vim keybindings. You can also use them in zsh (add bindkey -v to you ~/.zshrc or use this extension for some improved features). In fish, you can use fish_vi_key_bindings inside your config, and for bash it’s set -o vi. You can even enable them for all readline programs, such as the Python REPL, by adding set editing-mode vi set keymap vi to your ~/.inputrc.\nCtrl-R history search Pressing Ctrl-R inside your terminal will let you search through your history of commands and paste the one you select to your prompt, after which you can edit it. Aliases Pay attention to commands you’re using frequently and create aliases for them. Using this command, you can also show your most common commands, maybe that gives some ideas (though what we really care about is closer to “most common long prefixes of commands”; might be worth it to write a script for that instead). fzf fzf is a general purpose fuzzy finder and you can use it for all kinds of things. But more importantly, for this list, it comes with three pre-defined shell-keybindings: Ctrl-R is replaced with an improved history search, and Alt-C lets you fuzzy search the directories on your machine and will cd to the chosen one. Finally, Ctrl-T lets you search files inside the current directory (recursively) and pastes the selected path into the prompt, which is much faster for typing long paths than the usual TAB completion. Autosuggestions Autosuggestions in your shell mean …","date":1617194940,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617194940,"objectID":"65d07f9dd600b716c9117954ee15c8e9","permalink":"https://ejenner.com/post/computer-tips/","publishdate":"2021-03-31T14:49:00+02:00","relpermalink":"/post/computer-tips/","section":"post","summary":"  Many of us spend a lot of time working with our computer, so it's worth\n  spending some time to make that experience as pleasent and productive\n  as possible. This is a collection of tips that are relatively quick\n  to implement and still very valuable in the long run in my opinion.\n  Mainly geared towards developers and others who work with the shell\n  a lot.\n  ","tags":["Productivity"],"title":"Collection of quick computer tips","type":"post"},{"authors":null,"categories":null,"content":"Evan Chen’s Infinitely Large Napkin is my go-to resource when I want to learn about some new area of mathematics (or at least it used to be; I’m increasingly often running into the issue that it doesn’t have a chapter about what I want to learn – at barely over 900 pages it’s just way too short). I recently asked myself what it is about the Napkin that I like so much, and this post is part of my answer.\nNote that I’m describing the way I like to learn math, not the objectively best way. If you happen to like learning that way too, maybe this post can give you a more explicit idea of what “that way” is. Otherwise, it will at least give you some insight into the brain of someone who learns differently than you do, and maybe that will help if you want to teach math to other people.\nOne way of teaching mathematical concepts is the approach typically employed by textbooks: give some definitions, probably a few examples, state a few theorems, prove them. In some cases, you might also get an intuitive explanation of what these definitions are all about or why a theorem is interesting or how the proof roughly works. But often, these explanations are sparse and confined to the beginning of a section, so as not to dilute the mathematical purity of the remaining text.\nAnother style of explanation is the one that consists almost entirely of hand-waving. Evan Chen describes it as follows:\nSomeone tells you about the hairy ball theorem in the form “you can’t comb the hair on a spherical cat”, then doesn’t tell you anything about why it should be true, what it means to actually “comb the hair”, or any of the underlying theory, leaving you with just some vague notion in your head.\nWe could think of these two approaches as opposite ends of a “rigor”-spectrum. Somewhere in the middle, you might have the way that theoretical physics is often taught: few things are formally defined, but concepts are at least made explicit enough that you are able to use them in calculations, and equations are usually derived (though less rigorously than in math).\nWhere does the Napkin fit in on this axis? Is it more or less rigorous than theoretical physics courses? I think that’s the wrong question to ask because the one-dimensional model of approaches to teaching math is too simplistic. The space of all the ways you could explain mathematical concepts is very large and treating it as one-dimensional isn’t such a great approximation (though I think that the “rigor”-axis might be the best one can do with only one dimension).\nSo what’s a better model? I want to suggest thinking about mathematical explanations using a two-dimensional approximation. On one axis, we have the rigor used when stating definitions or results, while the other axis is the rigor of the derivations of these results. Textbooks have both formal statements and formal derivations, casual hand-wavy explanations have neither. Physics is somewhere in between, though to me the derivations often seem more rigorous than the statements1. Finally, the Napkin takes the opposite approach: it states things with essentially the same rigor as textbooks but then places emphasis on very informal derivations or explanations of why these statements are true.\nFigure 1: Various styles for teaching mathematics. Yes, it would look better as a TikZ figure, but it was either this or nothing. I find that this approach – stating things formally but reasoning about them informally – works extremely well for me when I first learn about a subject. I’m happy to take someone’s word that a statement is true, at least initially and if the statement seems like it should be true. But if I don’t know precisely what the statement is, I find it much harder to get to grips with the subject.\nAs a caveat, note that I said “when I first learn about a subject”. This approach doesn’t teach how to write formal proofs, and it’s by design not as comprehensive as a textbook. But sometimes an intuitive understanding is enough for my purposes, and even if I know I’ll later want to learn a subject in more depth, I find it helpful to build this intuitive understanding before going through all the details.\nThis sounded a bit absurd to me when writing it down – how can you rigorously derive something you haven’t even really stated? But I think there’s some truth to it, and what I mean is roughly this: the calculations done in physics courses are often essentially the same that you’d do for a formal proof. But the objects used in those calculations aren’t formally defined, it’s just taken for granted that everyone has a sense of what they are and how they behave (or maybe it’s explicitly stated how they behave, but not whether they are uniquely characterized by that behavior). ↩︎\n","date":1616580000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616580000,"objectID":"e5c187e8ace2517605939e939b0d8a70","permalink":"https://ejenner.com/post/state-formally-reason-informally/","publishdate":"2021-03-24T11:00:00+01:00","relpermalink":"/post/state-formally-reason-informally/","section":"post","summary":"  There's a style of teaching mathematics that I really like: stating definitions\n  and theorems as formally as in any textbook, but focusing on informal arguments\n  for why they should be true.\n  ","tags":["Math"],"title":"State formally, reason informally","type":"post"},{"authors":null,"categories":null,"content":"The purpose of this post is to point you towards some great features and packages if you’re already using Emacs to edit LaTeX, and to make you jealous if you’re using some other editor1.\nThis isn’t a tutorial for Emacs or even a tutorial on how to write LaTeX inside Emacs. Rather, it’s supposed to give an idea of what’s possible, either as inspiration or to convince you to give Emacs a try. One problem is that setting all of this up can be a huge time sink, so you might want to use a framework such as Doom, where you just need to enable the LaTeX module and get almost everything I describe here.\nThe basics Of course you get all the basics you would expect from a LaTeX editor. Synctex is supported (meaning you can jump from a certain line in the LaTeX code to the corresponding place inside your PDF viewer and the other way around), you can compile files from inside emacs, you can jump to compilation errors if there are any, there is auto-completion and so on.\nVisuals LaTeX can produce beautiful documents, but the source code isn’t very readable when writing mathematical expressions:\n\\alpha \\mapsto \\int_{\\R}e^{-\\alpha x^2}\\,dx Emacs and AUCTeX (which is the de-facto standard package for using LaTeX inside Emacs) have several features that improve this situation:\npreview-latex replaces equations (and other parts of the LaTeX document) with images by compiling them. This means they look exactly the same inside the editor as they will in the compiled document. When the cursor is on an equation, this image preview is automatically replaced by the underlying text so you can still easily edit equations. However, this method of course has a noticeable delay because it requires a call to the LaTeX compiler. LaTeX superscripts and subscripts are displayed as super-/subscripts inside the editor. This is a purely visual feature, editing them doesn’t require “entering” or “exiting” the subscript or anything like that. prettify-symbols-mode allows you to replace any string with any unicode symbol. AUCTeX comes with a fairly comprehensive predefined list, which replaces LaTeX commands such as greek letters, arrows and others with symbolic representations. But you can also add your own. For example, the example above uses \\R, which my custom style file defines as \\mathbb{R}, and it’s possible to add replacement rules for such custom commands (as long as there is a fitting Unicode symbol). This makes the line above look like this in my editor: When the cursor moves over one of those Unicode symbols, it is expanded to the underlying text. And the nice thing about this is that it’s essentially instantaneous because nothing needs to be compiled. Folding is something similar but more general (though unlike prettify-symbols-mode it’s specific to LaTeX). It doesn’t just allow replacing fixed strings but also more complicated expressions. By default, this is used for example to display \\label{some_label} as [l] (which as always expands when the cursor moves over it). The reasoning here is that some elements such as labels are just distractions when reading LaTeX source code. But you can also use this to further improve how math is displayed, see this config for some ideas (and in general for more ideas on how to beautify LaTeX inside Emacs). Editing AUCTeX has a couple of nice features that make typing LaTeX a bit easier. For example, you can let it automatically insert braces {} when typing _ or ^ inside a math environment, you can let it insert \\(\\) when typing a dollar sign, and even \\enquote{} when typing \u0026#34; 2.\nBut things get even better with the evil-tex package. As the name suggests, this is only relevant if you’re using evil-mode (vim keybindings inside emacs), but if so, it’s definitely worth trying. Just a few examples of what this allows you to do:\nSay you’ve typed\n\\(ax^{2} + b\\) and suddenly realize that this is supposed to go into an exponent. With your cursor anywhere on this math environment, type ysim^ (“surround everything inside the math environment as an exponent”) and you’ll get\n\\(^{ax^{2} + b}\\) with the cursor at the ^. Now you just need to enter the base.\nYour equations is now\n\\(e^{ax^{2} + b}\\) and you decide that this merits its own displayed rather than inline equation. So you type csmee (“change the surrounding math environment to equation”) and get\n\\begin{equation} e^{ax^{2} + b} \\end{equation} After a bit more editing, you have (for some reason)\n\\begin{equation} \\beta(e^{ax^{2} + b} + \\frac{1}{x}) \\end{equation} Of course this looks ugly in the compiled document, you need to use \\left( and \\right). With evil-tex, you can just type mtd (“toggle delimiter”) with the cursor anywhere inside the parantheses, and it will add \\left and \\right for you. Type mtd again to go back to just the parantheses.\nEmacs calc’s embedded mode calc is the built-in calculator for Emacs; though saying “calculator” is a bit misleading because it can do symbolic differentiation, unit conversion, linear algebra and more. If your …","date":1615986840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615986840,"objectID":"f3eb33525f848a2903798bff9fa23b61","permalink":"https://ejenner.com/post/latex-emacs/","publishdate":"2021-03-17T14:14:00+01:00","relpermalink":"/post/latex-emacs/","section":"post","summary":"  Emacs has some really amazing features for writing LaTeX; this post gives\n  an overview of some of them, either to convince you to give Emacs a try,\n  or to make you aware that these features exist if you're already using\n  Emacs but didn't know about them.\n  ","tags":["Productivity"],"title":"Emacs as an amazing LaTeX editor","type":"post"},{"authors":null,"categories":null,"content":"Spherical harmonics appear in lots of different places and have different interpretations that at first sight don’t seem to have anything to do with one another. In this post, I’ll try to connect three very common ones (namely as harmonic polynomials, as eigenfunctions of the Laplacian and as irreps of \\(\\operatorname{SO}(3)\\)).\nWe’re going to define spherical harmonics as homogeneous harmonic polynomials \\(\\mathbb{R}^3 \\to \\mathbb{C}\\). Let’s break this down:\nA polynomial of three variables is a finite sum of the form \\[\\sum_{\\alpha} a_\\alpha x^{\\alpha_x}y^{\\alpha_y}z^{\\alpha_z}\\] over multi-indices \\(\\alpha \\in \\mathbb{N}_0^3\\). Some examples are \\(x^2y + 2z\\) or \\(xyz + y^2\\). The coefficients \\(a_\\alpha\\) can be complex numbers, but we will only plug in real numbers for \\(x\\), \\(y\\) and \\(z\\). That’s why we interpret polynomials as functions \\(\\mathbb{R}^3 \\to \\mathbb{C}\\). Homogeneous mean that \\(\\alpha_x + \\alpha_y + \\alpha_z\\) is the same for all the \\(\\alpha\\) we sum over, so all the terms in the sum have the same degree. For example, \\(x^2 + 2yz + xz\\) is homogeneous, while \\(xy + z\\) is not. Harmonic means that the Laplacian of the polynomial vanishes: \\(p\\) is harmonic if \\(\\Delta p = 0\\). Here, \\(\\Delta = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} + \\frac{\\partial^2}{\\partial z^2}\\). We will write \\(\\mathcal{H}_l\\) for the space of all homogeneous harmonic polymonials of degree \\(l\\) (meaning \\(\\alpha_x + \\alpha_y + \\alpha_z = l\\) for all summands).\nIf you’ve seen spherical harmonics before (and you presumably have, if you’re reading this post), it’s probably been in the form of functions \\(Y_l^m(\\theta, \\varphi)\\) defined on the sphere. So why are we talking about these polynomials on \\(\\mathbb{R}^3\\) instead?\nThe answer is that every polynomial \\(p \\in \\mathcal{H}_l\\) can be written in spherical coordinates as \\[p(x, y, z) = r^l Y(\\theta, \\varphi)\\] for some function \\(Y: S^2 \\to \\mathbb{C}\\). To see why, write \\(x\\), \\(y\\) and \\(z\\) in spherical coordinates and plug them into the polynomial. They each have a factor of \\(r\\) and then some factors depending on \\(\\theta\\) and \\(\\varphi\\). So because \\(p\\) is homogeneous, each summand consists of a factor \\(r^l\\) times something that depends only on \\(\\theta\\) and \\(\\varphi\\). So we can think of homogeneous polynomials as polynomials defined on the sphere – their continuation to \\(\\mathbb{R}^3\\) is automatically determined by their degree \\(l\\). Therefore, we won’t really distinguish between homogeneous harmonic polynomials defined on \\(\\mathbb{R}^3\\) and their restrictions to \\(S^2\\), we will refer to both as spherical harmonics.\nThis should also explain the name: spherical harmonics are harmonic polynomials living on the sphere.\nThe functions \\(Y_l^m\\) that you may have seen are just a particular choice of basis for the vector space of spherical harmonics. If you multiply them by \\(r^l\\), you get polynomials in \\(\\mathcal{H}_l\\), and \\[\\{r^l Y_l^m| -l \\leq m \\leq l\\}\\] is a basis for \\(\\mathcal{H}_l\\).\nEigenfunctions of the Laplacian One of the reasons that spherical harmonics are so ubiquitous is that they are the eigenfunctions of the spherical Laplacian \\(\\Delta_{S^2}\\). They key to that is the following fact (which is just a brief calculation): for a function \\(Y: S^2 \\to \\mathbb{C}\\), \\[\\Delta (r^l Y) = r^{l - 2}\\left(l(l + 1)Y + \\Delta_{S^2}Y\\right)\\thinspace.\\] So \\(r^l Y(\\theta, \\varphi)\\) is harmonic if and only if \\[\\Delta_{S^2}Y = -l(l + 1)Y\\thinspace.\\] This already proves that spherical harmonics are eigenfunctions of the spherical Laplacian.\nBut we can say more than that: if we take any eigenfunction \\(f: S^2 \\to \\mathbb{C}\\) of the spherical Laplacian and multiply by \\(r^l\\) (with \\(l\\) such that \\(-l(l + 1)\\) gives the eigenvalue1), then \\(r^l f(\\theta, \\varphi)\\) must be harmonic. So the eigenfunctions of the spherical Laplacian are in fact in 1-to-1 correspondence with harmonic homogeneous functions on \\(\\mathbb{R}^3\\). It then turns out – and this part is far from obvious – that all such functions are polynomials2! So the spherical harmonics aren’t just eigenfunctions of the spherical Laplacian, they make up all of its eigenfunctions.\nIrreducible representations of \\(\\operatorname{SO}(3)\\) Another famous role that spherical harmonics play is as the irreducible representations of \\(\\operatorname{SO}(3)\\) (more precisely: the (complex) irreducible representations of \\(\\operatorname{SO}(3)\\) are exactly the spaces \\(\\mathcal{H}_l\\)). This is connected to the fact that they are the eigenfunctions of the spherical Laplacian.\nThat the eigenspaces of the spherical Laplacian are representations of \\(\\operatorname{SO}(3)\\) follows directly from the fact that the Laplacian commutes with rotations: we have a representation \\(G \\curvearrowright L^2(S^2, \\mathbb{C})\\) via \\[(r \\cdot f)(x) := f(r^{-1}x)\\] for any rotation \\(r\\) and \\(f \\in L^2(S^2, \\mathbb{C})\\). For an eigenfunction of the Laplacian, we get …","date":1615392420,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615392420,"objectID":"78933a14cf291c6fdecf2edf63a600cc","permalink":"https://ejenner.com/post/spherical-harmonics/","publishdate":"2021-03-10T17:07:00+01:00","relpermalink":"/post/spherical-harmonics/","section":"post","summary":"  Spherical harmonics are ubiquitous in math and physics, in part because\n  they naturally appear as solutions to several problems; in particular they\n  are the eigenfunctions of the spherical Laplacian and the irreducible\n  representations of SO(3). But why should the solutions to these problems\n  be the same? And why are they called spherical harmonics?\n  ","tags":["Math"],"title":"Perspectives on spherical harmonics","type":"post"},{"authors":null,"categories":null,"content":"Implicit Layers Layers in neural networks are almost exclusively explicitly specified. That just means that the output \\(y\\) is described as a (usually rather simple) function of the input \\(x\\) and some parameters \\(\\theta\\), i.e. \\[y = f(x; \\theta)\\thinspace.\\]\nThe idea behind implicit layers is the following:\nInstead of specifying how to compute the layer’s output from the input, we specify the conditions that we want the layer’s output to satisfy.\n(that quote is from the Implicit layers tutorial given by Zico Kolter, David Duvenaud and Matt Johnson at NeurIPS 2020, on which this post is based. I definitely recommend you check it out if you’re interested in all the details that I’ll skip over)\n“Conditions that we want the output to satisfy” is a bit vague, what does this look like concretely? Well, it’s vague on purpose because implicit layers are a very general framework. But one simple way to specify a condition is with \\[g(x, y; \\theta) = 0\\thinspace.\\] Whereas a forward pass in a neural network classically means applying the function \\(f\\) at each layer, we now need to solve this equation for \\(y\\) – that solution will be the output of our layer.\nThis may sound a bit insane. Isn’t solving an equation like that much more expensive than just applying an explicit function? Why would you want to do this during each forward pass? There’s some truth to that of course but it’s not as bad as it may sound. First, it turns out that relatively few or even just one implicit layer are often enough, so while each layer is more expensive to compute, you need fewer of them. And secondly, it’s sometimes possible to ensure that the equation describing the layer can be solved reasonably easily. After all, we have control over the class of equations that we need to solve by choosing the network architecture.\nSo it’s not as bad as it could be, but still, what to we gain? One very general answer is that implicit layers can be very expressive. Even using a very simple function \\(g\\), for example, the implicit function \\(x \\mapsto y\\) defined by solving the equation may be quite complex (this ties into the fact that one or a few implicit layers are often enough to do the job). On an even more abstract level, implicit layers decouple what properties we want the output to have from how to compute the output. The learned parameters only need to describe some conditions that the output should satisfy and we can then use any method we want to actually find such an output.\nOne detail I’ve quietly swept under the rug is whether a solution \\(y\\) even exists and whether it is unique. And for the most part, I’m going to continue ignoring this issue because this is meant to be a relatively informal introduction. I’ll just mention that in some cases, you actually get existence and uniqueness guarantees, and in others, you can still hope that it works out empirically.\nBut what about my gradients? So you’ve specified a model architecture (the function \\(g(x, y; \\theta)\\)) and you have some method for doing a forward pass (i.e. finding a \\(y\\) such that \\(g(x, y; \\theta) = 0\\) for a given input \\(x\\)). Now you want to train your model with gradient descent. But you don’t have any explicit function to take the gradient of, so how does that work?\nYou could backpropagate through your solver: after all, you computed the output \\(y\\) somehow, in principle you could backpropagate through that calculation. But that’s inefficient, so let’s try to find a better way.\nTo simplify the notation, we’ll ignore the parameters \\(\\theta\\). You can think of them as being a part of the input \\(x\\) – for our purposes there’s really not much difference between the input to the layer and the parameters, since we need gradients with respect to both.\nSo for a given input \\(x\\), we now want to find the Jacobian \\(\\frac{\\partial y^*}{\\partial x}\\), where \\(y^*\\) is the output such that \\(g(x, y^*) = 0\\). We can think of \\(y^*\\) as a function of \\(x\\): for each input \\(x\\), we have some solution \\(y^*(x)\\). To find the Jacobian of \\(y^*\\), we can use implicit differentiation. We know that \\(g(x, y^*(x)) = 0\\) for all \\(x\\), so if we read the LHS as a function of \\(x\\), it’s just the constant zero function. The derivative of the zero function is of course also 0, so \\(\\frac{d}{dx} g(x, y^*(x)) = 0\\). On the other hand, we can apply the chain rule, \\[\\frac{d}{dx}g(x, y^*(x)) = \\frac{\\partial g}{\\partial x} + \\frac{\\partial g}{\\partial y}\\frac{d y^*}{dx}\\thinspace.\\] Since this expression has to be zero, we can rearrange and get \\[\\frac{\\partial g}{\\partial y}\\frac{d y^*}{dx} = -\\frac{\\partial g}{\\partial x}\\thinspace,\\] which we can further rewrite as1 \\[\\frac{d y^*}{dx} = -\\left(\\frac{\\partial g}{\\partial y}\\right)^{-1}\\frac{\\partial g}{\\partial x}\\thinspace.\\] So now we have the Jacobian of \\(y^*\\) in terms of the Jacobian of \\(g\\), which means we can calculate gradients without backpropagating through the solver.\nMatrix inversion considered harmful There’s one …","date":1614779280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614779280,"objectID":"e1c190c8c312e57a66371180b06dd5d5","permalink":"https://ejenner.com/post/implicit-layers/","publishdate":"2021-03-03T14:48:00+01:00","relpermalink":"/post/implicit-layers/","section":"post","summary":"  Several new architectures for neural networks, such as Neural ODEs and\n  deep equlibirum models can be understood as replacing classical layers\n  that explicitly specify how to compute the output with implicit layers.\n  These layers describe which conditions the output should specify but\n  leave the actual computation up to some solver that can be chosen arbitrarily.\n  This post contains a brief introduction to the main ideas behind implicit layers.\n  ","tags":["Deep learning"],"title":"Deep Implicit layers","type":"post"},{"authors":null,"categories":null,"content":"In Part 1 and Part 2, we’ve seen different methods for learning good policies. One thing that all of them had in common was that they only used trajectories sampled from the environment to do so. This is what’s called model-free RL. In this final post, we will generalize to model-based RL, where we make use of a learned model of the environment to improve the training process.\nThe 10,000-mile satellite’s-eye view on RL From very far away, we can treat all the methods we’ve previously seen as functions that map a trajectory and a current parameter to an update for that parameter. The parameter could describe a value function or a policy. Training an agent means repeatedly sampling a trajectory, calculating that update, and updating the parameter.\nIt will be useful to think about this from the lense of types: an update method is a function that takes in an object of type “trajectory” and one of type “parameters” and returns an update of type \\(\\Delta\\text{parameters}\\)1: \\[\\text{trajectory} \\times \\text{parameters} \\to \\Delta\\text{parameters}\\] Often, we can decompose this function. For example, a 1-step method such as Sarsa calculates update based on individual \\((s, a, r, s, a)\\) tuples, which we’ll call “experience”. So Sarsa really defines a function with signature \\[\\text{experience} \\times \\text{parameters} \\to \\Delta\\text{parameters}\\] We then get the function signature from above by splitting up a trajectory into its underlying experience tuples, applying the Sarsa update to each one, and summing the results. This is a simple example but it illustrates the main idea of this post: to compose small functions in different ways in order to get complete RL algorithms.\nAs a slightly more complex example, consider Actor-Critic methods. We use a policy optimization method (the actor) with a signature such as \\[A: \\text{trajectory} \\times \\text{actor-param} \\times \\text{V-function} \\to \\Delta\\text{actor-param}\\] At the same time, we use some method for learning value functions, which has the signature \\[C: \\text{trajectory} \\times \\text{critic-param} \\to \\Delta\\text{critic-param}\\] And finally the model for the critic, which can be written as \\[V: \\text{critic-param} \\to \\text{V-function}\\] We can combine these functions using some pretty simple boilerplate code, to get a function with the \\(\\text{trajectory} \\times \\text{params} \\to \\Delta\\text{params}\\) signature that we want, where \\(\\text{params} := \\text{actor-param} \\times \\text{critic-param}\\) is the type of the complete collection of parameters. In Python, this might look as follows:\ndef train(trajectory, params): actor_param, critic_param = params v_function = V(critic_param) actor_update = A(trajectory, actor_param, v_function) critic_update = C(trajectory, critic_param) return (actor_update, critic_update) You should read this more as pseudo-code: the point is not that we would actually implement an agent exactly like this, but just to show how these individual functions come together to define an update for the entire agent.\nThe code above is completely agnostic to the choice of \\(A, C\\) and \\(V\\), which is an important point throughout this post: we only care about the function signatures of the methods we use as building blocks, not about how they work internally.\nA complete training loop Our ultimate goal is not to compute updates but to find a good policy. For that we need two more components. First, a function \\[\\text{parameters} \\to \\text{policy}\\] In the case of policy optimization methods, this is simply the parameterization of the policy, i.e. the model of the actor. If we use value-based methods, this can instead be decomposed into the value model \\[\\text{parameters} \\to \\text{Q-function}\\] and a function that determines the policy based on the Q-estimate, e.g. an \\(\\varepsilon\\)-greedy policy, \\[\\text{Q-function} \\to \\text{policy}\\]\nThe second thing we need is a function that samples trajectories – this is the role of the environment: \\[\\text{policy} \\to \\text{trajectory}\\] Then we can combine all of these into one function with the signature2 \\[\\text{parameters} \\to \\text{policy}\\] which takes initial parameter values and then trains a policy (until convergence or some stopping criterion).\nModeling the environment The only role played by the environment in our training algorithm is to provide a function \\[\\text{policy} \\to \\text{trajectory}\\] for sampling trajectories. But remember our motto: we only care about function signatures, not the internals of the functions themselves. So if we can define a function like that in some other way, we can plug it into our training algorithm without changing anything else.\nBefore we discuss how to define such a function, let’s first take a step back and consider how this signature is actually implemented by the environment. An environment is defined by its transition probabilities \\[p(s’, r|s, a)\\] i.e. the probability that the next state will be \\(s’\\) and the reward \\(r\\) if action \\(a\\) is taken …","date":1614159660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614159660,"objectID":"8ec00efeadffcd03eee4a0ad29cabc92","permalink":"https://ejenner.com/post/rl-building-blocks-3/","publishdate":"2021-02-24T10:41:00+01:00","relpermalink":"/post/rl-building-blocks-3/","section":"post","summary":"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is\n  the conclusion of a series with such an overview, covering model-based RL.\n  ","tags":["Reinforcement learning"],"title":"Building Blocks of RL Part III: Model-based RL","type":"post"},{"authors":null,"categories":null,"content":"Additive \\(L_1\\) or \\(L_2\\) penalties are two common regularization methods and their most famous difference is probably that \\(L_1\\) regularization leads to sparse weights (i.e. some weights being exactly 0) whereas \\(L_2\\) regularization doesn’t. There are many pictures and intuitive explanations for this out there but while those are great to build some understanding, I think they conceal the arguably deeper reason why \\(L_1\\) regularization leads to sparse weights. But before we discuss that, we need to understand why \\(L_2\\) regularization does not help to get sparse weights.\n\\(L_2\\) regularization doesn’t lead to any sparsity Let \\(w\\) be a vector of parameters and \\(\\mathcal{L}(w)\\) be any continuously differentiable loss function1. For \\(L_2\\) regularization, we want to find \\[\\operatorname*{argmax}_w \\mathcal{L}(w) + \\beta\\Vert w\\Vert_2^2\\] This means that the gradient has to be zero: \\[\\nabla \\mathcal{L}(w) + 2\\beta w = 0\\] or in components: \\[\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_i}\\right\\rvert_{w_i = 0} + 2\\beta w_i = 0\\]\nSo we can get \\(w_i = 0\\) as the optimal solution only if \\(\\frac{\\partial \\mathcal{L}}{\\partial w_i}\\big\\rvert_{w_i = 0} = 0\\), i.e. if \\(w_i = 0\\) is already optimal without regularization! So \\(L_2\\) regularization doesn’t help to get sparsity at all. The same is true for \\(L_p\\) regularization for any \\(p \u0026gt; 1\\), because \\[\\left.\\frac{\\partial}{\\partial w_i} \\Vert w\\Vert_p^p \\right\\rvert_{w_i = 0} = 0\\]\n\\(L_1\\) regularization: non-differentiability to the rescue \\(L_1\\) regularization just uses the 1-norm instead of the Euclidean norm: \\[\\operatorname*{argmax}_w \\mathcal{L}(w) + \\beta\\Vert w\\Vert_1\\] How does that change things? Well, the 1-norm of a vector is not differentiable at 0. More precisely: \\[\\frac{\\partial}{\\partial w_i} \\Vert w\\Vert_1 = \\begin{cases} +1, \\quad w_i \u0026gt; 0\\\\\\ -1,\\quad w_i \u0026lt; 0\\\\\\ \\text{undefined for } w_i = 0 \\end{cases} \\] So when can \\(w_i = 0\\) be a local minimum of the regularized loss? We can’t just set the derivative to zero as before, because the derivative doesn’t exist.\nTo understand what we can do instead, let’s first recall why setting the derivative to zero works for differentiable functions. If \\(f(x)\\) has a local minimum at 0, then this means that \\(f(x) \\geq f(0)\\) for all sufficiently small \\(x\\). Since we assumed \\(f\\) to be differentiable at \\(0\\), \\(f(x)\\) is well approximated2 by \\[f(x) \\approx f(0) + f’(0)x\\] for small \\(x\\). So if \\(f’(0) \u0026gt; 0\\), then \\(f(x) \u0026lt; f(0)\\) for small negative \\(x\\), and if \\(f’(0) \u0026lt; 0\\) we get the same for positive \\(x\\). So the derivative at the minimum has to be zero, because otherwise taking a small step in the right direction would decrease the value.\nWe can apply the same idea to the loss with \\(L_1\\) regularization: what happens if we take a small step \\(h\\) away from \\(w_i = 0\\)? The loss is differentiable and thus changes approximately linearly: \\[\\mathcal{L}\\Big\\rvert_{w_i = h} \\approx \\mathcal{L}\\Big\\rvert_{w_i = 0} + h\\frac{\\partial\\mathcal{L}}{\\partial w_i}\\] But for the regularization term, the change is always just \\(|h|\\), instead of a linear term: \\[\\Vert w\\Vert_1 \\bigg\\rvert_{w_i = h} = \\Vert w \\Vert_1\\bigg\\rvert_{w_i = 0} + |h|\\]\nSo if we write \\(\\tilde{\\mathcal{L}}\\) for the regularized loss, then we get \\[\\tilde{\\mathcal{L}}\\Big\\rvert_{w_i = h} \\approx \\tilde{\\mathcal{L}}\\Big\\rvert_{w_i = 0} + h\\frac{\\partial\\mathcal{L}}{\\partial w_i} + \\beta |h|\\] As long as \\(\\left|\\frac{\\partial\\mathcal{L}}{\\partial w_i}\\right| \u0026lt; \\beta\\), this is larger than the regularized loss at \\(w_i = 0\\), because the \\(+ \\beta |h|\\) term will dominate. That is why \\(L_1\\) regularization leads to sparser weights: it pulls all those weights to zero whose partial derivative at 0 has absolute value less than \\(\\beta\\)3.\nInterlude: Priors If the loss function \\(\\mathcal{L}\\) models some log-likelihood, then regularization can be interpreted as performing maximum a posteriori (MAP) estimation rather than maximum likelihood estimation (MLE). This means we start with some prior over possible values of the parameter \\(w\\), update this distribution using the evidence from the loss function, and then pick the parameters which are the most probable according to the posterior distribution.\n\\(L_2\\) regularization corresponds to a Gaussian prior and \\(L_1\\) regularization to a Laplace prior (in both cases centered around 0). So it’s natural to try to explain the sparsity behavior of these regularization methods in terms of the underlying priors.\nHere’s what a Gaussian (red) and Laplace (blue) distribution look like, both with unit variance and properly normalized:\nFigure 1: Gaussian and Laplace distribution with unit variance (created using https://www.desmos.com/) One difference is that the Laplace distribution has a higher density at (and around) 0. I’ve seen this used as an explanation for sparsity several times: the Laplace distribution seems more “concentrated” around 0, i.e. assigns a higher prior to 0, which is …","date":1613550780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613550780,"objectID":"ec394e2df73aae24ab882f806438adb0","permalink":"https://ejenner.com/post/sparsity-singularities/","publishdate":"2021-02-17T09:33:00+01:00","relpermalink":"/post/sparsity-singularities/","section":"post","summary":"  L1 regularization is famous for leading to sparse optima, in contrast to\n  L2 regularization. There are several ways of understanding this but I'll\n  argue that it's really all about one fact: the L1 norm has a singularity\n  at the origin, while the L2 norm does not. And this is not just true\n  for L1 and L2 regularization: singularities are always necessary to get sparse weights.\n  ","tags":["Machine learning"],"title":"L1 regularization: sparsity through singularities","type":"post"},{"authors":null,"categories":null,"content":"Informally, Chaitin’s incompleteness theorem states that there is a constant \\(L\\), such that we can’t prove the Kolmogorov complexity of any specific bit string to be larger than \\(L\\). We can of course prove that there are infinitely many bit strings with higher complexity than \\(L\\) – but we can’t name a single one!\nJohn Baez calls this constant \\(L\\) the complexity barrier. And surprisingly, he argues that it is probably very low (on the order of a few thousand bits for reasonable encoding schemes)!\nAt least to me, this is a pretty amazing fact. Consider all the material available on the internet for instance: everything ever written online, all the videos and images, and binaries for every computer program out there. “Obviously” we can’t just write a Python program of a few kilobytes that outputs all of this, … right? Well, I’m pretty sure we can’t, but somewhat incredibly, there’s no proof of that!\nTake a moment to be properly astonished by this result because the aim of this post is to make it as obvious as possible. We’ll get there soon enough, but first let’s look at some fun paradoxa.\nThere are no boring numbers … The follwing “paradox” is quite famous:\nAssume there was an uninteresting natural number. Then the smallest such number would be interesting – because it’s the smallest uninteresting number, that’s quite an interesting property! This is a contradiction, so there can be no uninteresting natural numbers.\nWe can formalize it as follows: we have some boolean “boringness” property, call it \\(P\\), defined over natural numbers. So \\(P(n)\\) just means “\\(n\\) is boring”. Being the lowest boring number is itself interesting: \\[P\\left(\\min_{P(n)} n\\right)\\quad \\text{is false}\\] This is self-contradictory if there are any \\(n\\) such that \\(P(n)\\), so no natural numbers can have property \\(P\\).\n… and every number can be described in 13 words or less There’s a arguably more interesting variation of this paradox: let \\(P_k(n)\\) mean “\\(n\\) cannot be described in fewer than \\(k\\) words”. Consider then the description “the smallest natural number which cannot be described in fewer than 14 words”. In our notation, this would be \\[\\min_{P_{14}(n)} n\\] However, this description is only 13 words long, so \\[P_{14}\\left(\\min_{P_{14}(n)} n\\right)\\quad \\text{is false}\\] which is a contradiction if such a number exists. Therefore, every number can be described in at most 13 words.\nThis seems suspicious: while there are many 13-word phrases, there’s still only a finite number of them1. So there aren’t enough short descriptions to go around for each natural number to get one.\nThere are arbitrarily complex strings … Of course the problem is that “cannot be described in fewer than \\(k\\) words” is not a well-defined property because there is no unambiguous mapping from English descriptions to numbers.\nBut what if we replace English by a formal language to circumvent this issue? For example, a Turing machine without any input either halts and outputs a number (in the form of a bit string), or it runs forever. If we fix an encoding for Turing machines, any Turing machine has a length, so we can define \\(P_k(n)\\) as “\\(n\\) is not the output of any halting Turing machine with length less than \\(k\\)”. Or more briefly: “the Kolmogorov complexity of \\(n\\) is at least \\(k\\)”.\nSo let’s repeat our argument with Turing machines instead of natural language. We need to define a program that outputs \\(\\min_{P_k(n)} n\\) while being itself shorter than \\(k\\). Given a subroutine that checks whether \\(P_k(n)\\) for arbitrary \\(n\\), we could simply iterate over \\(n = 1, 2, \\ldots\\) until we find one such that \\(P_k(n)\\) and then output that. If such an \\(n\\) existed, this program would output \\(\\min_{P_k(n)} n\\), so by the same argument as before, there can’t be any \\(n\\) with complexity higher than \\(k\\) …\n… which can’t be right, because just as with natural language descriptions, there are only finitely many programs of length \\(\\leq k\\), but infinitely many bit strings.\nThis time, the issue is the phrase “Given a subroutine …”: \\(P_k\\) is undecidable, so this subroutine unfortunately doesn’t exist2. Or fortunately, if you value the consistency of mathematics.\nIn essense, the problem is this: in English, the smallest number with a simple property can be described in few words because you only need to describe the property and a few additional words. But the same is not true for Kolmogorov complexity if the property isn’t decidable.\nFor any decidable property, the argument works: the smallest number with that property will have low Kolmogorov complexity (where “low” means “not much larger than the complexity of the property”). Let’s see what we can get by applying that insight.\n… but not provably complex ones! This post is supposed to be about Chaitin’s incompleteness theorem, so we’d better connect all of this talk about paradoxa and complexity to that. The only missing ingredient is to consider provably large complexities, rather than …","date":1612970820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612970820,"objectID":"52ebefb10109c5c62868524ac984663d","permalink":"https://ejenner.com/post/boring-numbers/","publishdate":"2021-02-10T16:27:00+01:00","relpermalink":"/post/boring-numbers/","section":"post","summary":"  There is a \"complexity barrier\": a number such that we can't prove\n  the Kolmogorov complexity of any specific string to be larger than\n  that. The proof of this astonishing fact is closely related to some\n  famous paradoxa and we'll use this connection to get a better intuition\n  for why the complexity barrier exists.\n  ","tags":null,"title":"Boring numbers, complexity and Chaitin's incompleteness theorem","type":"post"},{"authors":null,"categories":null,"content":"This is part 2 of a three-part series. Part 1 covered value-based methods and also gave some introduction and defined some notation. Part 3 will cover model-based RL.\nSo far, we have looked at the building blocks necessary to learn value functions for a given policy, called policy evaluation. We have also seen that with GPI, we can use policy evaluation for control, i.e. to find optimal value functions. The policy was always derived from the value function, by picking actions (\\(\\varepsilon\\)-)greedily.\nIn this post, we take a more direct approach to control: what we really want to learn is a good policy, so why not optimize the policy directly, without the detour of learning a value function?\nPolicy optimization and policy gradient methods Policy optimization in general means that we have a parameterized family of policies \\(\\pi_\\theta(a|s)\\) and want to maximize the expected return with respect to the parameters \\(\\theta\\): \\[\\operatorname*{argmax}_{\\theta} J(\\theta)\\] where \\(J\\) is the expected return: \\[J(\\theta) := \\mathbb{E}_{\\tau \\sim \\pi_\\theta, \\mu} R(\\tau)\\] Here \\(\\tau\\) is a trajectory which is sampled using the initial state distribution \\(\\mu\\) and policy \\(\\pi_\\theta\\). \\(R(\\tau)\\) is the return of that trajectory.\nIn principle, there are many ways we could solve this optimization problem. For example, we could perform a grid search over parameters \\(\\theta\\) and evaluate the expected return for each parameter by sampling lots of episodes. But this wouldn’t scale well (\\(\\theta\\) might very well be a vector with millions of dimensions if we use Deep RL). In practice, most methods instead use stochastic gradient ascent or variations thereof and that is all we will cover in this post.\nOne sidenote before we dive in: why do we use a parameterized policy at all? For value-based methods, we started in a tabular setting, where we could directly assign values to each state. The difference is that even in a tabular setting, the policy is not an arbitrary function – it has to be normalized over actions. So we can’t just update a single probability \\(\\pi(a|s)\\) without also adjusting others.\nSome theory: the policy gradient theorem If you’re only interested in a description of some policy optimization methods, you can skip this and the next section. But it sheds some light onto why these methods are designed the way they are and why they work.\nWe want to optimize the expected return \\(J(\\theta)\\). To see what that entails, we can write it out explicitly as \\[J(\\theta) := \\sum_{a \\in \\mathcal{A}} q^{\\pi_\\theta}(s_0, a) \\pi_\\theta(a|s_0)\\] where \\(s_0\\) is the initial state of the MDP. To optimize this function using gradient ascent, we need to find \\(\\nabla_\\theta J(\\theta)\\). But this seems very difficult at first because while the influence of \\(\\theta\\) on \\(\\pi_\\theta\\) is easy to find, it also affects the state distribution and thereby \\(q^{\\pi_\\theta}\\).\nFortunately, the policy gradient theorem comes to the rescue. It states that \\[\\nabla_\\theta J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} q^{\\pi_\\theta}(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\] where \\(\\mu^\\pi\\) is the on-policy state distribution of \\(\\pi\\). Essentially, we can just apply the gradient to the policy itself and don’t need to know how the state distribution depends on the policy. Section 13.2 of Sutton and Barto’s textbook contains more details and a proof.\nAnother very useful fact is that we can subtract a baseline from the state value: \\[\\nabla_\\theta J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} \\left(q^{\\pi_\\theta}(s, a) - b(s)\\right) \\nabla_\\theta \\pi_\\theta(a|s) \\] (the only difference to the previous equation is the \\(- b(s)\\) term). This fact is also sometimes called the policy gradient theorem. \\(b(s)\\) may be any function or random variable, as long as it doesn’t depend on \\(a\\).\nMore theory: score function estimators Typically, we will still be unable to evaluate the gradient \\[\\nabla_\\theta J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} \\mu^{\\pi_\\theta}(s) \\sum_{a \\in \\mathcal{A}} q^{\\pi_\\theta}(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\] analytically. Some types of gradients of an expected value can be estimated by sampling: \\[\\nabla_x \\mathbb{E}_{x \\sim p} f(x) = \\mathbb{E}_{x \\sim p} \\nabla_x f(x)\\] so if we can sample from \\(p\\) and can calculate \\(\\nabla f(x)\\), we can estimate this gradient. But our case is different: ignoring the expectation over states (which doesn’t pose a problem), we want to evaluate a gradient of the form \\[\\nabla_\\theta \\mathbb{E}_{a \\sim \\pi_\\theta(a)} f(a)\\] The variable \\(\\theta\\), with respect to which we differentiate, appears in the distribution, so we can’t just approximate this gradient by sampling as we did in the other case.\nGradients of this form (called stochastic gradients) appear often in machine learning, not just in RL. One method to calculate them is the reparameterization trick, which you might know …","date":1612334340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612334340,"objectID":"92f37957db5955fa584de51889e7df51","permalink":"https://ejenner.com/post/rl-building-blocks-2/","publishdate":"2021-02-03T07:39:00+01:00","relpermalink":"/post/rl-building-blocks-2/","section":"post","summary":"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 2\n  of a series with such an overview, covering some policy optimization methods.\n  ","tags":["Reinforcement learning"],"title":"Building Blocks of RL Part II: Policy Optimization","type":"post"},{"authors":null,"categories":null,"content":"When proving simple statements in point set topology, there is often only one obvious next step that can be done given the objects and statements you already have1. You don’t need to think about what you eventually want to prove because there is only one step that will lead to a proof of anything.\nAn example: continuous images of compact spaces are compact As an example, let’s go through the proof that the image of a compact space under a continuous map is again compact: we start with a compact space \\(X\\) and a map \\(f: X \\to Y\\). To make the notation less unwieldy, we’ll assume that \\(f\\) is surjective, so we’ll show that \\(Y\\) is compact, but the proof works exactly the same without this assumption (just replace every occurence of \\(Y\\) with \\(f(X)\\)). Because we want to show that \\(Y\\) is compact, i.e. that every open cover of \\(Y\\) has a finite subcover, we also start with a given open cover \\(Y = \\bigcup_i U_i\\).\nWith only these objects available, there isn’t a lot we can do. For example, if \\(X\\) were a normed vector space, we would have access to its zero vector and then could construct \\(f(0)\\) from that. That wouldn’t lead anywhere but it’s a branch in the tree of possible proofs that might distract us. Because \\(X\\) and \\(Y\\) have so little structure, these kinds of options simply don’t exist.\nThe only thing I can come up with is that we can look at the preimage of each of the \\(U_i\\) under \\(f\\). This gives us a collection \\(f^{-1}(U_i), i \\in I\\) of subsets of \\(X\\). Such a collection in itself still doesn’t allow us to do anything interesting, but because preimages preserve unions, we have \\[\\bigcup_i f^{-1}(U_i) = f^{-1}\\left(\\bigcup_i U_i\\right) = f^{-1}(Y) = X\\] so this collection is in fact a cover of \\(X\\). Because \\(f\\) is continuous, it is also an open cover.\nAgain, there isn’t much we can do with this newly constructed open cover. We could map it back to \\(Y\\) with \\(f\\) immediately but that just gives us back the open cover of \\(Y\\) we started with. The other thing we can do with an open cover of \\(X\\) is pick a finite subcover: \\(X\\) is compact, and we can think of that procedurally as a way of turning any open cover into a finite subcover.\nSo now we have a new object: a finite open subcover \\(X = U_{i_1} \\cup \\ldots \\cup U_{i_k}\\). Inside \\(X\\), there isn’t anything else we can do with a (finite) cover, so the only option is to now apply \\(f\\) again, which gives us sets \\(f(f^{-1}(U_{i_1})), \\ldots, f(f^{-1}(U_{i_k})) \\subset Y\\). Because \\(f\\left(f^{-1}(U_i)\\right) = U_i\\), this is a finite subset of the open cover we started with.\nAn then we’re done because it is also a cover: \\[\\bigcup_{j = 1}^k f\\left(U_{i_j}\\right) = f\\left(\\bigcup_{j = 1}^k U_{i_j}\\right) = f(X) = Y\\]\nThe thing that I hope you took away from this walkthrough is how few choices there were at each step. Apart from some steps that obviously didn’t add anything new, there was always only one thing to do next.\nWe didn’t even specifically aim to construct a finite subcover of \\(\\bigcup_i U_i\\) for most of the proof, we just “went with the flow”.\nThis is a feeling that is much more rare in e.g. real analysis, even for proofs that are similarly easy as the one above. With some experience, you might get enough intuition to discard all the wrong options immediately but they’ll still be there. You typically have to keep in mind what you want to prove and deliberately steer your proof in that direction, otherwise the number of possible paths you could take just explodes and you never get anywhere.\nThe importance of (lack of) structure The decisive difference between the point set topology example and real analysis is, I think, how much structure the spaces and objects we are working with have. By “structure”, I mean the same somewhat elusive concept I’ve previously talked about here. In short, a group is a set with some additional structure and a field adds even more structure. The way I use the word, a manifold also has more structure than a topological space (even though it doesn’t require any new choices).\nOne of the aspects of structure I talked about in the post I just linked is that objects with less structure admit fewer definitions and theorems. Applying theorems to the objects we’ve already constructed is how we make progress in our proofs. So having fewer theorems to work with leads to a proof tree with a lower branching factor: at each step of the proof, there are only a few things we can do. In an extreme case, we have a branching factor of one and can do the proof on autopilot, as in the topology example above.\nIf you are working on \\(\\mathbb{R}^n\\) on the other hand, you can use all the topological properties you had before, but you can also view \\(\\mathbb{R}^n\\) as a vector space, you can talk about lengths and angles and even about the Lebesgue measure of sets. This is possible because \\(\\mathbb{R}^n\\) has a lot of canonical structure, so you suddenly have many more tools at your disposal.\nThis explains …","date":1611733980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611733980,"objectID":"bcab6c1869f8f7c4c60505914764f58f","permalink":"https://ejenner.com/post/too-much-structure/","publishdate":"2021-01-27T08:53:00+01:00","relpermalink":"/post/too-much-structure/","section":"post","summary":"  Proving things for object that have a lot of structure can be harder\n  than for object with less structure, simply because the tree of possible\n  proofs is much wider. This is probably why trying to prove a more general\n  case is sometimes a helpful strategy.\n  ","tags":["Structure","Math"],"title":"Too much structure","type":"post"},{"authors":null,"categories":null,"content":"Epistemic status: thinking out loud, not an expert on physics\nIn physics, there appears to be a deep duality between position and momentum, in the sense that they are largely equivalent perspectives on viewing the same system. In classical mechanics, \\(x \\mapsto p\\) and \\(p \\mapsto -x\\) is a canonical transformation, which means that treating momentum as position and position as negative momentum results in unchanged dynamics. In quantum mechanics, the roles of position and momentum can be similarly switched with a change of basis.\nSo mathematically speaking, it would appear that there is nothing special about either position or momentum, both yield similar and equally good descriptions. And yet, human cognition treats position and momentum very differently, they don’t feel like dual descriptions of reality. To us, there is a big difference between a car that is close to us and moving with a high relative velocity (distant in momentum space) and one that is far away and more or less stationary with respect to us.\nBut human cognition runs on brains, which run on physics, which seems to treat position and momentum equivalently. So how can this be? How does the cognitive asymmetry arise from what seems to be symmetry on the fundamental physical level?\nThe motivation for this post is mostly to point out the question. I’m not sure myself what the answer is but I’ll at least give my guesses below.\nFalse assumptions I’ve said that human cognition “runs on brains”, which “run on physics” and the argument loses a lot of its punch if this assumption is false. Cognition not running on physics could mean something like a fundamental Cartesian distinction between body and mind. That doesn’t answer why humans perceive two things differently that appear to be equivalent in physics but at least that fact doesn’t seem as paradoxical anymore.\nThere’s also the possibility that cognition does run on physics but uses physics we don’t know of yet, and in which there is a fundamental difference between position and momentum that our cognition exploits.\nI think that neither of these cases is very likely. If we didn’t find any other explanations for the cognitive difference between position and momentum, then this difference might be strong evidence for a Cartesian view or for new physics playing a role in our cognition. But I think there are other, more promising explanations, based on the observation that while the fundamental physical laws treat position and momentum the same, the Hamiltonian that happens to govern our universe does not. That’s what I’ll get to next.\nHamiltonian part I: Locality This explanation is specific for quantum mechanics. So if it turns out to be the reason for the asymmetry between position and momentum, this would mean that this feature of our cognition is inherently quantum mechanical and would not appear in a classical universe.\nThe Schrödinger equation, which determines the time evolution of a system, can be written in terms of position as follows: \\[i\\hbar \\frac{\\partial}{\\partial t}\\psi(x, t) = \\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x)\\right)\\psi(x, t)\\] This time evolution is local in the following sense: to calculate \\(\\frac{\\partial}{\\partial t} \\psi(x, t)\\), we only need to know the wave function \\(\\psi\\) in an arbitrarily small neighborhood of \\(x\\) (so that we can calculate its second spatial derivative).\nWe can also write the Schrödinger equation in terms of momentum: \\[i\\hbar \\frac{\\partial}{\\partial t}\\psi(p, t) = \\left(\\frac{p^2}{2m} + V\\left(i\\hbar\\frac{\\partial}{\\partial p}\\right)\\right)\\psi(p, t)\\] What does it mean to plug a derivative into the potential \\(V\\)? We’ll assume that \\(V\\) is analytic, which means that it can locally be written as a power series. Then \\(V\\left(i\\hbar \\frac{\\partial}{\\partial p}\\right)\\) is defined by plugging in \\(i \\hbar \\frac{\\partial}{\\partial p}\\) into that power series.\nIf \\(V\\) happens to be a polynomial, this is just a sum of normal differential operators and the time evolution is local in exactly the same sense as for position. But in general, \\(V\\) can be an infinite power series, and we will take arbitrarily high derivatives of \\(\\psi\\). This means that locality can be violated – this power series of derivatives may depend on points that are far away in momentum space1. The most famous example for a power series of differential operators being non-local is probably the fact that \\(\\exp\\left(a \\frac{\\partial}{\\partial x}\\right) f(x) = f(x - a)\\) (see e.g. this stackexchange post). \\(f(x - a)\\) depends on the value of \\(f\\) outside a small enough neighborhood (if \\(a \\neq 0\\)), so in such cases, the time evolution in terms of position is not local in the sense described above.\nThis raises the question: where does the asymmetry between these two formulations of the Schrödinger equation come from? The answer is that it’s all the Hamiltonian’s fault. The Schrödinger equation can be written in basis independent form as \\[i \\hbar …","date":1611049920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611049920,"objectID":"e2137f36d62bfec06b6574e7f4bcb7a4","permalink":"https://ejenner.com/post/position-momentum-asymmetry/","publishdate":"2021-01-19T10:52:00+01:00","relpermalink":"/post/position-momentum-asymmetry/","section":"post","summary":"  In both classical mechanics and QM, there are transformations between position-based\n  and momentum-based representations that preserve the dynamical laws. So from\n  a mathematical perspective, position and momentum seem to play equivalent roles\n  in physics. But they don't play equivalent roles in our cognition, which is part of\n  the physical universe -- seemingly a paradox.\n  ","tags":["Physics"],"title":"Asymmetry between position and momentum in physics","type":"post"},{"authors":null,"categories":null,"content":"Reinforcement Learning consists of atomic building blocks that can be combined to create many of the well-known algorithms. This is not a secret but it can sometimes be obscured when learning about different methods one after another, never getting the big picture view. So this is my attempt at the kind of overview I would have like when I first got into RL. How helpful it is to you probably depends a lot on how similar your learning style is to mine.\nThis is part 1 of of a planned three-part series. Part 2 will be about policy optimization and part 3 about model-based RL.\nMotivation Why consider the building blocks of RL individually at all? There are at least two good reasons:\nIt makes RL methods easier to memorize. This is for two reasons: first, memorization becomes easier when the material is split into small chunks. Second, many of the building blocks are shared by several methods, so we can avoid duplicate effort more effectively by explicitly considering these building blocks. More importantly, it gives a better understanding of the landscape of RL methods. A very naive view of RL methods would just consider them as a very long list of possibilities. But in reality, they are more of a very high-dimensional table, with different options to choose from for different aspects of the algorithm. Target audience and what this is not On its own, this is not an introduction to Reinforcement Learning; I assume that you already know most of the definitions and algorithms and mainly describe how they fit into one common framework. That said, it might be helpful to read this series in parallel to learning about the algorithms it covers. Or you can use it as a review, or to deepen your big picture of RL. If you’re already very familiar with RL theory, you probably won’t find anything new.\nThis is also not a guide on which method to choose for which problem. It might help with that but I don’t focus on the various advantages and disadvantages.\nFinally, this overview is far from exhaustive. My main goal is to present the framework and give enough examples to provide intuition for how concrete algorithms fit in. In particular, I focus on a tabular setting (for Part 1) and cover Deep RL only briefly towards the end. All of the things I discuss for a tabular setting are still relevant for Deep RL, so it should still be useful even if you’re not interested in tabular RL for its own sake. But if you’re looking for an overview of the parts that are specific to Deep RL, this is not it.\nNotation \\(A_t\\) is the action taken at time \\(t\\) while the agent is in state \\(S_t\\). Afterwards, the environment returns a reward \\(R_{t + 1}\\) and a new state \\(S_{t + 1}\\) The return \\(G_t\\) is the discounted sum of rewards from time \\(t\\) onwards \\[G_t = \\sum_{k = 1}^\\infty \\gamma^{k - 1} R_{t + k}\\] Value functions This post is about value-based methods, which means the model explicitly learns and represents a value function and uses that value function to compute the policy (I will cover actor-critic methods when we talk about policy optimization in part 2).\nThere are two types of value functions: state-value functions or V-functions assign a value to every state \\(s\\). We write \\(v_\\pi(s)\\) for such a value function. Q-functions assign a value to every state-action pair \\((s, a)\\), i.e. to taking action \\(a\\) in state \\(s\\), and we write them as \\(q_\\pi(s, a)\\). Many algorithms work essentially the same for both kinds of functions but there will be a few cases where we need to make a distinction.\nFinding these value functions \\(v_\\pi\\) or \\(q_\\pi\\) for a given policy \\(\\pi\\) is called policy evaluation. Of course just evaluating a policy is not that useful by itself. After all, the goal of reinforcement learning is to find a good policy. We do this using generalized policy iteration (GPI), which we will talk about more later. For now you only need to know that GPI is a method (or rather collection of methods) for finding an optimal policy, which needs to evaluate a policy as one of its substeps. So we will start by only discussing policy evaluation, keeping in mind that this will later help us with finding good policies as well.\nGeneral shape of the update We will start in a tabular setting meaning there are only finitely many states and the value function is a simple lookup table. All the value-based methods in this setting have the same general shape: we have some observations \\(S_t, A_t, R_{t + 1}, S_{t + 1}, A_{t + 1}, \\ldots\\), which we got from running policy \\(\\pi\\) on the environment (or on an environment model, more on that in part 3). We keep an estimate \\(V\\) of the true value function \\(v_\\pi\\), which is updated for each observed state \\(S_t\\) as follows: \\[V(S_t) \\gets V(S_t) + \\alpha(\\text{target} - V(S_t))\\] or analogously \\[Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha(\\text{target} - Q(S_t, A_t))\\] for an estimate \\(Q\\) of \\(q_\\pi\\). \\(\\alpha\\) is a learning rate which may or may not be constant. \\(\\text{target}\\) is the …","date":1610553480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610553480,"objectID":"92594d17778954ada1ca960096b63503","permalink":"https://ejenner.com/post/rl-building-blocks-1/","publishdate":"2021-01-13T16:58:00+01:00","relpermalink":"/post/rl-building-blocks-1/","section":"post","summary":"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 1\n  of a series with such an overview, covering value-based methods (mainly in a tabular\n  setting).\n  ","tags":["Reinforcement learning"],"title":"Building Blocks of RL Part I: Value-based methods","type":"post"},{"authors":null,"categories":null,"content":"\\( \\DeclareMathOperator*{\\argmax}{argmax} \\DeclareMathOperator*{\\argmin}{argmin} \\DeclareMathOperator*{\\E}{\\mathbb{E}} % expected value \\newcommand{\\R}{\\mathbb{R}} \\) This is my attempt to tell a story1 about how you might invent variational autoencoders (VAEs). There are already great introductory posts doing this and if you haven’t seen VAEs before, I would strongly recommend you start with one of those. These introductions often start with autoencoders and then extend them to VAEs. In contrast, we will start by asking ourselves how to generate new data that matches a training distribution and then motivate VAEs from there. We won’t assume an autoencoder-like architecture a priori, instead it will arise naturally from this motivation.\nOf course just this motivation of generating new samples given a training distribution won’t uniquely lead to VAEs – after all, there are other good options for generative models. So at some points we will need to make design decisions but hopefully they won’t come out of the blue.\nOne pedagogical note before we start: if this derivation of VAEs seems unnecessarily long and convoluted, that’s because it is. The goal is not to arrive at the VAE framework as quickly as possible, but rather to make each step seem natural and to avoid any unmotivated “magical” jumps. It’s probably best if you forget for a moment what you know about VAEs, in particular that they consist of an encoder and a decoder. We will get there at the very end but initially this preconception might just be confusing.\nGenerative models The goal in generative modeling is the following: we have some family of probability distributions \\(\\mathcal{P}\\). Given a set of training examples \\(\\mathcal{D}\\) (assumed to be i.i.d.), we now want to pick the distribution \\(p \\in \\mathcal{P}\\) from our family that maximizes the likelihood \\(\\prod_{x \\in \\mathcal{D}} p(x)\\). Equivalently, we can maximize the log-likelihood: \\[\\argmax_{p \\in \\mathcal{P}} \\sum_{x \\in \\mathcal{D}} \\log p(x)\\] For now, we will consider the simper special case where we only have a single datapoint \\(x\\) and want to maximize \\(\\log p(x)\\) (we will get back to the general case at the end).\nOptimizing over a family of probability distributions is very abstract. To turn this into a problem we can actually solve numerically, we will use a parameterized family \\(p_\\theta(x)\\) and optimize over the parameter \\(\\theta \\in \\R^p\\). \\(p_\\theta(x)\\) should be differentiable with respect to \\(\\theta\\), then we can at least find a local optimum for our problem using gradient ascent.\nThis still leaves the question which parameterized family we should use. This is the largest crossroads we’ll face in this post: there are many good options to choose from. The challenge we face is to find a good trade-off between having a flexible family of distributions and keeping the number of parameters manageable. For example, if \\(x\\) takes on discrete values, we could in principle use the full categorical distribution over all possible values of \\(x\\). This would be as flexible as possible but the number of parameters might be huge. If \\(x\\) describes a \\(28\\times 28\\) binary image, there are already \\(2^{28 \\cdot 28} = 2^{784} \\approx 10^{236}\\) possible values that \\(x\\) can take, meaning we’d need about that many parameters.\nThe way we will deal with this problem is to use a continuous mixture of simple distributions. We will introduce a new latent variable \\(z \\in \\mathbb{R}^k\\) on which we define a very simple distribution \\(p(z)\\), for example a unit normal, \\(z \\sim \\mathcal{N}(0; I)\\). Then we parameterize a distribution \\(p_\\theta(x|z)\\), which gives us \\[p_\\theta(x) = \\int p(z) p_\\theta(x|z) dz\\] The important point is that for a fixed \\(z\\), \\(p_\\theta(\\cdot|z)\\) may be an extremely simple distribution. In the example above, we could use an independent Bernoulli for each of the 784 pixels, which requires only 784 parameters. But because we additionally have a dependency on \\(z\\), the marginal distribution \\(p_\\theta\\) can be much more complex (in particular, the pixels are typically not independent). Of course the dependency on \\(z\\) will require some additional parameters but this could just be a reasonably sized neural network, which gives us far fewer parameters than the \\(2^{784}\\) that a full categorical distribution would require.\nThis already describes our model. Sampling from this model is easy: we sample \\(z \\sim p(z)\\) and then for this \\(z\\) sample \\(x \\sim p_\\theta(x|z)\\). By assumption, both of those distributions are very simple (and we can also choose them to be easy to sample from).\nBut evaluating the likelihood \\(p_\\theta(x)\\) of a datapoint is intractable for most models \\(p_\\theta(x|z)\\) and \\(p(z)\\) because it requires calculating a complicated integral. Even if we only care about generating samples, this is a problem: to train the model, we want to maximize \\(\\log p_\\theta(x)\\), but we can’t even evaluate it (nor its gradient, for the same …","date":1609940700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609940700,"objectID":"26703bdafec1ddf8497c02555dfdf1ba","permalink":"https://ejenner.com/post/vae-generative/","publishdate":"2021-01-06T14:45:00+01:00","relpermalink":"/post/vae-generative/","section":"post","summary":"  Variational autoencoders are usually introduced as a probabilistic extension of autoencoders\n  with regularization. An alternative view is that the encoder arises naturally as a tool\n  for efficiently training the decoder. This is the perspective I take in this post, deriving\n  VAEs without assuming an autoencoder architecture a priori.\n  ","tags":["Deep learning"],"title":"VAEs from a generative perspective","type":"post"},{"authors":null,"categories":null,"content":"Most of the objects that appear in mathematics can be thought of as sets with additional “structure”. For example, a group is a set \\(G\\) with an operation \\(G \\times G \\to G\\) fulfilling certain axioms. This operation is what makes a group feel more “structured” than a simple set of elements. A topological space is a set equipped with a topology and there is a myriad of other examples (graphs, ordered sets, vector spaces, metric spaces and measure spaces to name a few).\nBut “structure” in this sense is a somewhat elusive concept. We know it when we see it but it’s hard to describe explicitly – which is why I just gave some examples and hoped you knew what I meant. (Sidenote: there is also a more formal notion of structure in mathematical logic but that’s not the topic of this post)\nThe goal of this post is not to give a formal definition of structure – I’m not sure how helpful that would even be – but rather to describe different perspectives that may be useful when thinking about it. To guide us, we will consider one particular question: what does it mean to say that object A has “more structure” than object B? For example, a vector space has more structure than a group, which has more structure than a simple set. We will start with more formal (but also more boring) perspectives and then work our way towards more speculative and fuzzy ones.\nNotation We’ll fix a set \\(X\\) and consider the different possible structures that can be imposed on \\(X\\). Calligraphy letters like \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) refer to the set of all possible structures of some type, for example the set of all groups on \\(X\\). Particular instances are written as \\(A \\in \\mathcal{A}\\) (e.g. a particular group on \\(X\\)).\nWe will write \\(\\mathcal{A} \\prec \\mathcal{B}\\) for the informal notion that \\(\\mathcal{A}\\) has more structure than \\(\\mathcal{B}\\), for example \\(\\text{fields} \\prec \\text{groups}\\). An alternative way to think about this (which hopefully explains why the \\(\\prec\\) sign points the way it does) is that fields are a special case of groups (each field is also a group), which means that the set of fields is in some sense a subset of the set of groups. This leads us right into the first perspective on structure.\nStructure can be canonically removed If \\(\\mathcal{A} \\prec \\mathcal{B}\\) (\\(\\mathcal{A}\\) has more structure than \\(\\mathcal{B}\\)), then there is a canonical way to turn any instance \\(A \\in \\mathcal{A}\\) into an instance \\(B \\in \\mathcal{B}\\). As an example, a vector space can be canonically turned into a group by just using vector addition as the group operation and ignoring scalar multiplication. Or any metric space can be treated as just a topological space by using the topology induced by the metric and ignoring the metric itself (category theory footnote1).\nStructure leads to smaller symmetry groups If \\(\\mathcal{A} \\prec \\mathcal{B}\\), then the automorphism group2 of \\(A \\in \\mathcal{A}\\) is smaller than the group of the corresponding \\(B \\in \\mathcal{B}\\) (where “corresponding” means that \\(B\\) is just \\(A\\) with parts of the structure removed as described in the previous section).\nFor example, we can treat the real numbers as a metric space or as a topological space. For a metric space, the automorphism group consists of only isometries (i.e. maps that don’t change distances between points), which for the real numbers are only translations. If we treat them as a topological space though (which has a lot less structure), then the automorphisms are all the homeomorphisms of the real number line, which form a much larger group.\nMore structure leads to fewer structure-preserving maps If we consider two sets \\(X\\) and \\(Y\\), there are \\(|Y|^{|X|}\\) maps from \\(X\\) to \\(Y\\). If we now introduce a group structure, most of those maps are typically not homomorphisms, i.e. not structure-preserving. If we then turn the groups into rings, even fewer maps will additionally be compatible with the ring multiplication. So adding structure reduces the number of maps which preserve all of that structure (which is pretty obvious when put like that).\nThe previous perspective is a special case of this, where \\(Y = X\\) and we only consider automorphisms rather than any structure-preserving maps, so it shouldn’t be surprising that we also got fewer automorphisms if we had more structure. But I think it’s a very important special case that deserves to be treated seperately because the interpretation via symmetries makes it much more intuitive than this general version.\nStructure allows more definitions and theorems Now we start getting into slightly more hand-wavy territory. If \\(\\mathcal{A} \\prec \\mathcal{B}\\), then there are more concepts we can define for objects with structure \\(\\mathcal{A}\\) than for objects with structure \\(\\mathcal{B}\\). We can also prove more and stronger theorems about objects with structure \\(\\mathcal{A}\\) then about objects with structure \\(\\mathcal{B}\\). This is related to the previous …","date":1609246980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609246980,"objectID":"69ba7c2506c90c124be77a33189948aa","permalink":"https://ejenner.com/post/perspectives-on-structure/","publishdate":"2020-12-29T14:03:00+01:00","relpermalink":"/post/perspectives-on-structure/","section":"post","summary":"  \"Structure\" is a concept that keeps popping up when thinking about mathematics\n  but it's hard to pin down what it is exactly. I discuss several different perspectives\n  for thinking about it.\n  ","tags":["Structure","Math"],"title":"Ways to think about structure in mathematics","type":"post"},{"authors":null,"categories":null,"content":"(Note: this is an analysis of one aspect of the Karger-Stein algorithm, it’s not meant to be a beginner-friendly introduction)\nKarger’s algorithm randomly contracts a graph and surprisingly, this can be used to find a minimum cut with probability \\(\\mathcal{O}(n^{-2})\\), where \\(n\\) is the number of vertices of the graph. This is a much, much higher probability than sampling a graph cut uniformly at random would give! But it still means we need to run the algorithm \\(\\mathcal{O}(n^2\\log n)\\) times to get a high success probability. Karger’s algorithm can be implemented in \\(\\mathcal{O}(n^2)\\) time1, which gives an overall runtime of \\(\\mathcal{O}(n^4 \\log n)\\) – not great, there are much faster deterministic algorithms.\nTo understand how the Karger-Stein algorithm improves upon that, we need the following key result that forms the foundation for Karger’s algorithm:\nTheorem2: When Karger’s algorithm contracts a graph from \\(n\\) to \\(r\\) vertices, any given mincut survives with probability \\(\\geq {r \\choose 2}/{n \\choose 2} = \\frac{r (r - 1)}{n (n - 1)}\\).\nIn particular, for \\(r = 2\\), we get the \\(\\mathcal{O}(n^{-2})\\) probability mentioned above.\nBut note the following: if we make only a few contractions, \\(r \\lesssim n\\), then mincuts are almost guaranteed to survive! This is the key insight that allows us to improve the runtime of Karger’s algorithm, leading to the improved Karger-Stein algorithm.\nThe idea is the following: first we contract the graph down to roughly \\(\\frac{n}{b}\\) vertices, where \\(b\\) is small enough that mincuts are very likely to survive. Then we branch: we again contract the graph down by a factor of \\(b\\), but we do so \\(a\\) times independently from one another. \\(a\\) needs to be chosen high enough that mincuts are very likely to survive in at least one of the branches. We repeat this process until we have contracted the graph down to just 2 vertices3. If we chose \\(a\\) and \\(b\\) right, at least one of the final leaves of our computational tree will likely contain a mincut. So we return the best cut we’ve found among all the leaves.\nThe Karger-Stein algorithm as it was originally described and as it is usually presented uses \\(a = 2\\) and \\(b = \\sqrt{2}\\). So we always split the computation into two branches and reduce the number of vertices by a factor of \\(\\sqrt{2}\\) before branching again. But in this post, I would like to motivate where these numbers come from, as well as show that they’re not the only ones that work. So in the following, we’re going to analyze the “generalized Karger-Stein algorithm” with arbitrary \\(a\\) and \\(b\\).\nSuccess probability As mentioned above, any minimum cut survives a contraction from \\(n\\) to \\(r\\) vertices with probability \\(\\geq {r \\choose 2}/{n \\choose 2} = \\frac{r (r - 1)}{n (n - 1)}\\). I said we first contract to “roughly” \\(\\frac{n}{b}\\) vertices – to be precise we contract until \\(\\lceil \\frac{n}{b} + 1\\rceil\\) vertices are left, this will give us a nice bound. The probability that a mincut survives this contraction is4:\n\\begin{equation} \\begin{split} p \u0026amp;\\geq \\frac{\\lceil \\frac{n}{b} + 1 \\rceil \\lceil \\frac{n}{b} \\rceil}{n (n - 1)}\\\\\\ \u0026amp;\\geq \\frac{(\\frac{n}{b} + 1) \\cdot \\frac{n}{b}}{n (n - 1)}\\\\\\ \u0026amp;\\geq \\frac{1}{b^2} \\end{split} \\end{equation}\nWe can now apply this bound recursively: After we have contracted to \\(\\lceil \\frac{n}{b} + 1 \\rceil\\) vertices, we can forget that this is a partially contracted graph, and just treat this number as the “new \\(n\\)”.\nWe will write \\(p_k\\) for the survival probability if there are \\(k\\) levels of recursion left before we reach the leaves of the tree. So \\(p_0\\) will be the probability in the leaves of the recursion tree. Depending on when precisely we stop the recursion and what method we use to finish the contraction, \\(p_0\\) might take different values, but all that matters for us is that it is some constant.\nUsing the bound we found above, we get the following recurrence: \\[p_{k + 1} \\geq 1 - \\left( 1 - \\frac{p_k}{b^2} \\right)^a\\] What’s going on here? \\(\\frac{p_k}{b^2}\\) is a lower bound on the probability that any given mincut survives in one particular branch. So \\(\\left(1 - \\frac{p_k}{b^2}\\right)^a\\) is an upper bound on the probability that the mincut survives in none of the \\(a\\) branches, and consequently \\(1 - \\left( 1 - \\frac{p_k}{b^2} \\right)^a\\) is a lower bound on the probability that it survives in at least one.\nThis recurrence doesn’t have an obious solution we can just read off but with some rewriting, we can get something that’s good enough for our purposes. Substituting \\(z_k := \\frac{b^2}{p_k} - 1\\), we get\n\\begin{equation} \\begin{split} z_{k + 1} \u0026amp;= \\frac{b^2}{p_{k + 1}} - 1 \\\\\\ \u0026amp;\\leq \\frac{b^2}{1 - \\left(1 - \\frac{1}{z_k + 1}\\right)^a} - 1\\\\\\ \u0026amp;= \\frac{b^2 \\left(z_k + 1\\right)^a}{\\left(z_k + 1\\right)^a - z_k^a} - 1\\\\\\ \u0026amp;\\leq \\frac{b^2 \\left(z_k + 1 \\right)^a}{a z_k^{a - 1}} - 1\\\\\\ \u0026amp;\\leq \\frac{b^2}{a} z_k + \\text{const} \\end{split} \\end{equation}\nwhere we used \\(z_k \\geq 1\\) in the …","date":1607274000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607274000,"objectID":"7c59ca9ec8932905795ed1e2dcf7f1c8","permalink":"https://ejenner.com/post/karger-stein/","publishdate":"2020-12-06T18:00:00+01:00","relpermalink":"/post/karger-stein/","section":"post","summary":"  The Karger-Stein algorithm is an improvement over Karger's beautiful contraction\n  algorithm for minimum graph cuts. In this post, I show how it finds the perfect\n  tradeoff between finding a mincut with high probability and finding it quickly.\n  In the course of doing so, we will also understand where the somewhat opaque\n  factor of sqrt(2) comes from.\n  ","tags":["Graphs"],"title":"Trading off speed against the probability of success in the Karger-Stein Algorithm","type":"post"},{"authors":null,"categories":null,"content":"I’ve heard that when starting a new sketchbook, you should begin by drawing some silly doodles on the first page to break the paralysis that a fresh book full of beautiful blank pages can induce. So for my first blog post I chose the silliest topic that came to mind, namely the intersection of ethics, economics and special relativity.\nDiscounting is the idea that obtaining value \\(V\\) some time \\(\\Delta t\\) into the future is worth only \\(f(\\Delta t)V\\) now where \\(f(\\Delta t) \u0026lt; 1\\) is the discount factor. What exactly “value” means depends on the context. For now, we will talk about money as an example, but we will get back to this point later.\nThis definition of discounting raises an obvious problem: the distance in time to future events is not invariant under Lorentz boosts, so by discounting like this, your value assignments become dependent on your frame of reference. Now, as long as you never accelerate, your frame of reference will stay the same and this isn’t a practical problem (though you may still have objections on aesthetic grounds). But as soon as you change your state of motion, you’ll run into problems.\nImagine that you’re about to leave for your vacation in the Alpha Centauri system, taking the new Starline 90C moving at 90% the speed of light. Suddenly, Omega comes along and offers you a deal: it will pay you $\\$90$ right now but in return you will have to pay $\\$100$ once you arrive at Alpha Centauri in $4.34 / 0.9 = 4.82$ years (as seen from your current frame of reference on earth). This sounds like a great deal to you: you discount at 3% per year, so the $\\$100$ you’ll have to pay are only worth $\\$100 \\cdot 0.97^{4.34} = \\$86.35$ to you now.\nSo you accept the deal, board your spaceship and begin accelerating towards Alpha Centauri. But as you do, you feel your value assignments shifting – or rather you realize that you will be on Alpha Centauri in only $4.82 \\cdot \\sqrt{1 - (0.9)^2} = 2.10$ years in your new reference frame because of time dilation. This means that you suddenly value the $\\$100$ you will have to pay on arrival at $\\$100 \\cdot 0.97^{2.1} = \\$93.80$, just because you stepped into a spaceship and took off.\nSo clearly, improper discounting is an important financial hazard for space tourists. But what should you do instead, if you want to keep your normal discounting procedures while on earth?\nNow we need to get back to what we meant by “value”. If value refers to money, then discounting is closely related to the fact that you can invest money you already have now, so getting money at a later point in time is less valuable. The amount of time used for discounting calculations should then be the proper time of the money, so it depends on whether you were going to leave most of your money invested on earth (in which case you should discount with the 4.82 years in earth’s frame of reference) or whether you were going to invest it aboard the Starline 90C (in which case you should discount with the travel time of 2.10 years).\nBut what if you want to discount pure utilities? In that case the question is no longer one of economics but of ethics. We are looking for a discount function \\(f\\) that satisfies the following criteria:\n\\(f\\) assigns some discount factor to each point in your future light cone – all of those are events that you might be able to influence and therefore need to take into account for utility calculations. On the world line of your current frame of reference, these factors coincide with the old discounting factor \\(f(t)\\) – “It all adds up to normality”. \\(f\\) is invariant under Lorentz boosts in the sense that if your velocity suddenly changes, and you recalculate all discount factors, they would remain the same. Essentially, your ethical judgements don’t change just because you take a flight to Alpha Centauri at relativistic speeds. I think there is only one way of discounting that satisfies all of these desiderata1: use the spacetime interval instead of the time as measured in your current reference frame.\nThe spacetime interval between two points is\n\\begin{equation} \\Delta s = \\sqrt{\\left(c\\Delta t\\right)^2 - \\left(\\Delta x\\right)^2 - \\left(\\Delta y\\right)^2 - \\left(\\Delta z\\right)^2}, \\end{equation}\nwhere \\(\\Delta t\\) is their time difference (what we used for discounting before) and \\(\\Delta x, \\Delta y, \\Delta z\\) are the spatial distances. The nice thing about \\(\\Delta s\\) is that it is invariant under Lorentz transformations, so if instead of discounting with \\(f(\\Delta t)\\), you discount with \\(f(\\Delta s)\\), then your value assignments won’t change when you change frames of reference.\nWhat consequences does this have? For small spatial distances, not much changes. The \\(c\\) in the equation above means that as long as you could reach an event while travelling much slower than the speed of light, \\(\\Delta s \\approx \\Delta t\\). On the other hand, events that are close to the edges of your future light cone have \\(\\Delta s\\) close to 0, meaning they …","date":1592648700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592648700,"objectID":"b79ae52cf351c219e428d1da408c1bea","permalink":"https://ejenner.com/post/discounting-relativistic-universe/","publishdate":"2020-06-20T12:25:00+02:00","relpermalink":"/post/discounting-relativistic-universe/","section":"post","summary":"  For people who want to discount the future, special relativity creates\n  some challenges. There are different ways to handle those but none\n  seem completely satisfactory which may be yet another argument against\n  discounting pure utilities.\n  ","tags":["Physics"],"title":"Discounting in a relativistic universe","type":"post"},{"authors":null,"categories":null,"content":"This website uses GoatCounter to count page views, which doesn’t collect any personal information. More precisely, only the following information is collected:\nThe Referer header The name of the country and region (based on the IP address) A hash of IP address, User-Agent, and a random salt (to identify continuous sessions, deleted after 8 hours) None of this information is shared with any third parties.\nBecause this page is hosted with Github Pages, Github may log visits, including IP adresses as per their privacy policy.\nContact me at erik@ejenner.com for any questions.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://ejenner.com/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"This website uses GoatCounter to count page views, which doesn’t collect any personal information. More precisely, only the following information is collected:\nThe Referer header The name of the country and region (based on the IP address) A hash of IP address, User-Agent, and a random salt (to identify continuous sessions, deleted after 8 hours) None of this information is shared with any third parties.","tags":null,"title":"Privacy","type":"page"}]