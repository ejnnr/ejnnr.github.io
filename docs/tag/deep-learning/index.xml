<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep learning | Erik Jenner</title>
    <link>https://ejenner.com/tag/deep-learning/</link>
      <atom:link href="https://ejenner.com/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 18 Jun 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ejenner.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Deep learning</title>
      <link>https://ejenner.com/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Steerable Partial Differential Operators for Equivariant Neural Networks</title>
      <link>https://ejenner.com/publication/steerable-pdos/</link>
      <pubDate>Fri, 18 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://ejenner.com/publication/steerable-pdos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Implicit layers</title>
      <link>https://ejenner.com/post/implicit-layers/</link>
      <pubDate>Wed, 03 Mar 2021 14:48:00 +0100</pubDate>
      <guid>https://ejenner.com/post/implicit-layers/</guid>
      <description>&lt;h2 id=&#34;implicit-layers&#34;&gt;Implicit Layers&lt;/h2&gt;
&lt;p&gt;Layers in neural networks are almost exclusively explicitly specified. That
just means that the output \(y\) is described as a (usually rather simple)
function of the input \(x\) and some parameters \(\theta\), i.e.
\[y = f(x; \theta)\thinspace.\]&lt;/p&gt;
&lt;p&gt;The idea behind &lt;em&gt;implicit&lt;/em&gt; layers is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Instead of specifying how to compute the layer’s output from the input,
we specify the conditions that we want the layer’s output to satisfy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(that quote is from the &lt;a href=&#34;http://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Implicit layers tutorial&lt;/a&gt; given by Zico Kolter, David Duvenaud
and Matt Johnson at NeurIPS 2020, on which this post is based.
I definitely recommend you check it out if you&amp;rsquo;re interested in all the details that I&amp;rsquo;ll skip over)&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Conditions that we want the output to satisfy&amp;rdquo; is a bit vague, what does
this look like concretely? Well, it&amp;rsquo;s vague on purpose because implicit
layers are a very general framework. But one simple way to
specify a condition is with
\[g(x, y; \theta) = 0\thinspace.\]
Whereas a forward pass in a neural network classically means applying
the function \(f\) at each layer, we now need to solve this equation for \(y\)
&amp;ndash; that solution will be the output of our layer.&lt;/p&gt;
&lt;p&gt;This may sound a bit insane. Isn&amp;rsquo;t solving an equation like that much
more expensive than just applying an explicit function? Why would you
want to do this during each forward pass? There&amp;rsquo;s some truth to that
of course but it&amp;rsquo;s not as bad as it may sound. First, it turns out
that relatively few or even just one implicit layer are often enough, so
while each layer is more expensive to compute, you need fewer of them.
And secondly, it&amp;rsquo;s sometimes possible to ensure that the equation describing
the layer can be solved reasonably easily. After all, we have control over the
class of equations that we need to solve by choosing the network architecture.&lt;/p&gt;
&lt;p&gt;So it&amp;rsquo;s not as bad as it could be, but still, what to we gain? One very general
answer is that implicit layers can be very expressive. Even using a very simple
function \(g\), for example, the implicit function \(x \mapsto y\) defined by
solving the equation may be quite complex (this ties into the
fact that one or a few implicit layers are often enough to do the job).
On an even more abstract level, implicit layers decouple &lt;em&gt;what&lt;/em&gt; properties
we want the output to have from &lt;em&gt;how&lt;/em&gt; to compute the output. The learned parameters
only need to describe some conditions that the output should satisfy and we can then
use any method we want to actually find such an output.&lt;/p&gt;
&lt;p&gt;One detail I&amp;rsquo;ve quietly swept under the rug is whether a solution \(y\) even
exists and whether it is unique. And for the most part, I&amp;rsquo;m going
to continue ignoring this issue because this is meant to be a relatively
informal introduction. I&amp;rsquo;ll just mention that in some cases, you actually
get existence and uniqueness guarantees, and in others, you can still hope
that it works out empirically.&lt;/p&gt;
&lt;h2 id=&#34;but-what-about-my-gradients&#34;&gt;But what about my gradients?&lt;/h2&gt;
&lt;p&gt;So you&amp;rsquo;ve specified a model architecture (the function \(g(x, y; \theta)\))
and you have some method for doing a forward pass (i.e. finding a \(y\) such
that \(g(x, y; \theta) = 0\) for a given input \(x\)). Now you want to train
your model with gradient descent. But you don&amp;rsquo;t have any explicit function
to take the gradient of, so how does that work?&lt;/p&gt;
&lt;p&gt;You could backpropagate through your solver: after all, you computed the output
\(y\) &lt;em&gt;somehow&lt;/em&gt;, in principle you could backpropagate through that calculation.
But that&amp;rsquo;s inefficient, so let&amp;rsquo;s try to find a better way.&lt;/p&gt;
&lt;p&gt;To simplify the notation, we&amp;rsquo;ll ignore the parameters \(\theta\). You can think
of them as being a part of the input \(x\) &amp;ndash; for our purposes there&amp;rsquo;s really not
much difference between the input to the layer and the parameters, since we
need gradients with respect to both.&lt;/p&gt;
&lt;p&gt;So for a given input \(x\), we now want to find the Jacobian \(\frac{\partial y^*}{\partial x}\),
where \(y^*\) is the output such that \(g(x, y^*) = 0\). We can think of \(y^*\)
as a function of \(x\): for each input \(x\), we have some solution \(y^*(x)\).
To find the Jacobian of \(y^*\), we can use &lt;em&gt;implicit differentiation&lt;/em&gt;.
We know that \(g(x, y^*(x)) = 0\) for all \(x\), so if we read the LHS as a function
of \(x\), it&amp;rsquo;s just the constant zero function. The derivative of the zero function
is of course also 0, so \(\frac{d}{dx} g(x, y^*(x)) = 0\). On the other hand, we
can apply the chain rule,
\[\frac{d}{dx}g(x, y^*(x)) = \frac{\partial g}{\partial x} + \frac{\partial g}{\partial y}\frac{d y^*}{dx}\thinspace.\]
Since this expression has to be zero, we can rearrange and get
\[\frac{\partial g}{\partial y}\frac{d y^*}{dx} = -\frac{\partial g}{\partial x}\thinspace,\]
which we can further rewrite as&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
\[\frac{d y^*}{dx} = -\left(\frac{\partial g}{\partial y}\right)^{-1}\frac{\partial g}{\partial x}\thinspace.\]
So now we have the Jacobian of \(y^*\) in terms of the Jacobian of \(g\), which
means we can calculate gradients without backpropagating through the solver.&lt;/p&gt;
&lt;h2 id=&#34;matrix-inversion-considered-harmful&#34;&gt;&lt;a href=&#34;https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix inversion considered harmful&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s one remaining issue: while we now know how to calculate the Jacobian \(\frac{d y^*}{dx}\),
doing so requires us to invert a matrix, which is expensive. Luckily, we don&amp;rsquo;t actually
need to explicitly compute the Jacobian for gradient descent. What we ultimately care about
is the gradient of the loss, which is a scalar function. For example, if the implicit layer
described by \(g\) is the last one before the loss function \(L\), we want
\[\frac{dL}{dx} = \frac{dL}{dy}\frac{dy^*}{dx}\thinspace,\]
where \(\frac{dL}{dy}\) is the gradient of \(L\) as a row vector. More generally, we only
need to be able to compute products of the form
\[w^T \frac{dy^*}{dx} = -w^T\left(\frac{\partial g}{\partial y}\right)^{-1}\frac{\partial g}{\partial x}\thinspace,\]
not the Jacobian itself. We can do this by first solving
\[u^T\frac{\partial g}{\partial y} = -w^T\]
for \(u\), the so called &lt;em&gt;adjoint variable&lt;/em&gt;. Crucially, this is possible without
explicitly computing and storing the inverse (for example using &lt;a href=&#34;https://en.wikipedia.org/wiki/Iterative%5Fmethod#Linear%5Fsystems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iterative methods&lt;/a&gt;). Then we can calculate
\[w^T \frac{dy^*}{dx} = u^T\frac{\partial g}{\partial x}\thinspace.\]&lt;/p&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ve seen some of the basic ideas and themes surrounding implicit layers.
Instead of explicitly describing how to compute the output, they specify
a condition that the output should satisfy. Using implicit differentiation,
we can still effectively backpropagate through these layers, independent
of the solver we use in the forward pass.&lt;/p&gt;
&lt;p&gt;But we haven&amp;rsquo;t talked at all about concrete instantiations of implicit layers.
What would a network using these actually look like? And can we use contraints
other than those of the form \(g(x, y; \theta) = 0\)? All of that and more
is discussed in the &lt;a href=&#34;http://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implicit layers tutorial&lt;/a&gt; that I already mentioned above.
It starts with Deep equilbrium models (which essentially use layers defined
by \(g(x, y) = 0\) as in this post, just framed differently) but then applies
the same ideas to constraints described by ODEs, leading to Neural ODEs.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;This is assuming that the Jacobian \(\frac{\partial g}{\partial y}\) is invertible. This is exactly the condition under which the &lt;a href=&#34;https://en.wikipedia.org/wiki/Implicit%5Ffunction%5Ftheorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implicit function theorem&lt;/a&gt; holds. In that case, \(y^*\) is indeed differentiable (so we can apply the chain rule as we did). And if we drop the assumption that \(g(x, y) = 0\) is uniquely solvable, then the implicit function theorem at least guarantees that a unique solution \(y*(x)\) exists locally around a point \((x_0, y_0)\). So this theorem is sort of a theoretical backbone, which guarantees that what we do actually works. But if we assume that a solution function \(y^*\) exists and is differentiable, then computing it&amp;rsquo;s Jacobian doesn&amp;rsquo;t require any heavy machinery: as you can see, we just apply the chain rule once and then rearrange a bit.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>VAEs from a generative perspective</title>
      <link>https://ejenner.com/post/vae-generative/</link>
      <pubDate>Wed, 06 Jan 2021 14:45:00 +0100</pubDate>
      <guid>https://ejenner.com/post/vae-generative/</guid>
      <description>&lt;p&gt;\(
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\E}{\mathbb{E}} % expected value
\newcommand{\R}{\mathbb{R}}
\)
This is my attempt to tell a story&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
about how you might invent variational autoencoders (VAEs).
There are already &lt;a href=&#34;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great introductory posts&lt;/a&gt; doing this and if you haven&amp;rsquo;t seen VAEs before, I would
strongly recommend you start with one of those. These introductions often start with
autoencoders and then extend them to VAEs. In contrast, we will start by asking ourselves
how to generate new data that matches a training distribution and then motivate VAEs
from there. We won&amp;rsquo;t assume an autoencoder-like architecture a priori, instead it will arise naturally
from this motivation.&lt;/p&gt;
&lt;p&gt;Of course just this motivation of generating new samples given a training distribution won&amp;rsquo;t uniquely lead to VAEs
&amp;ndash; after all, there are other good options for generative models. So at some points we will
need to make design decisions but hopefully they won&amp;rsquo;t come out of the blue.&lt;/p&gt;
&lt;p&gt;One pedagogical note before we start: if this derivation of VAEs seems unnecessarily
long and convoluted, that&amp;rsquo;s because it is. The goal is not to arrive at the VAE framework
as quickly as possible, but rather to make each step seem natural and to avoid any unmotivated
&amp;ldquo;magical&amp;rdquo; jumps. It&amp;rsquo;s probably best if you forget for a moment what you know about VAEs,
in particular that they consist of an encoder and a decoder. We will get there at the very end
but initially this preconception might just be confusing.&lt;/p&gt;
&lt;h2 id=&#34;generative-models&#34;&gt;Generative models&lt;/h2&gt;
&lt;p&gt;The goal in generative modeling is the following: we have some family of probability
distributions \(\mathcal{P}\). Given a set of training examples \(\mathcal{D}\) (assumed to be i.i.d.), we now want
to pick the distribution \(p \in \mathcal{P}\) from our family that maximizes the likelihood \(\prod_{x \in \mathcal{D}} p(x)\).
Equivalently, we can maximize the log-likelihood:
\[\argmax_{p \in \mathcal{P}} \sum_{x \in \mathcal{D}} \log p(x)\]
For now, we will consider the simper special case where we only have a single datapoint \(x\) and want
to maximize \(\log p(x)\) (we will get back to the general case at the end).&lt;/p&gt;
&lt;p&gt;Optimizing over a family of probability distributions is very abstract. To turn this into a problem
we can actually solve numerically, we will use a parameterized family \(p_\theta(x)\) and optimize over the
parameter \(\theta \in \R^p\). \(p_\theta(x)\) should
be differentiable with respect to \(\theta\), then we can at least find a local optimum for our
problem using gradient ascent.&lt;/p&gt;
&lt;p&gt;This still leaves the question which parameterized family we should use. This is the largest
crossroads we&amp;rsquo;ll face in this post: there are many good options to choose from. The challenge we
face is to find a good trade-off between having a flexible family of distributions and
keeping the number of parameters manageable. For example, if \(x\) takes on discrete values,
we could in principle use the full categorical distribution over all possible values of \(x\).
This would be as flexible as possible but the number of parameters might be huge. If \(x\)
describes a \(28\times 28\) binary image, there are already \(2^{28 \cdot 28} = 2^{784} \approx 10^{236}\)
possible values that \(x\) can take, meaning we&amp;rsquo;d need about that many parameters.&lt;/p&gt;
&lt;p&gt;The way we will deal with this problem is to use a continuous mixture of simple distributions.
We will introduce a new latent variable \(z \in \mathbb{R}^k\) on which we &lt;em&gt;define&lt;/em&gt; a very simple distribution \(p(z)\), for
example a unit normal, \(z \sim \mathcal{N}(0; I)\). Then we parameterize a distribution
\(p_\theta(x|z)\), which gives us
\[p_\theta(x) = \int p(z) p_\theta(x|z) dz\]
The important point is that for a fixed \(z\), \(p_\theta(\cdot|z)\) may be an extremely simple distribution.
In the example above, we could use an independent Bernoulli for each of the 784 pixels, which
requires only 784 parameters. But because we additionally have a dependency on \(z\), the marginal
distribution \(p_\theta\) can be much more complex (in particular, the pixels are typically not independent).
Of course the dependency on \(z\) will require some additional parameters but this could just be a reasonably
sized neural network, which gives us far fewer parameters than the \(2^{784}\) that a full categorical
distribution would require.&lt;/p&gt;
&lt;p&gt;This already describes our model. Sampling from this model is easy: we sample \(z \sim p(z)\) and
then for this \(z\) sample \(x \sim p_\theta(x|z)\). By assumption, both of those distributions are
very simple (and we can also choose them to be easy to sample from).&lt;/p&gt;
&lt;p&gt;But evaluating the likelihood \(p_\theta(x)\) of a datapoint is intractable for most models \(p_\theta(x|z)\)
and \(p(z)\) because it requires calculating a complicated integral. Even if we only care about generating
samples, this is a problem: to train the model, we want to maximize \(\log p_\theta(x)\), but we can&amp;rsquo;t even
evaluate it (nor its gradient, for the same reason).&lt;/p&gt;
&lt;p&gt;The cleverness of VAEs lies in using the right approximations to make this optimization problem
tractable, and that is what the remainder of this post is about.&lt;/p&gt;
&lt;h2 id=&#34;variational-inference&#34;&gt;Variational inference&lt;/h2&gt;
&lt;p&gt;First, we expand the log-likelihood a bit. For any value of \(z\), we have
\[\log p(x) = \log p(z) p(x|z) - \log p(z|x)\]
(not writing our the dependency on \(\theta\) for now).
The first term is easy to evaluate. So if we could evaluate the second term, our problem
would be solved (sidenote on motivation&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;p&gt;This is where variational inference comes into play (&lt;a href=&#34;https://ermongroup.github.io/cs228-notes/inference/variational/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; is a tutorial if you want to dive a bit deeper
but that&amp;rsquo;s not necessary for this post). The idea of variational inference is that
you have some distribution \(p\) that you care about, but which is intractable to work with.
So you define a family \(\mathcal{Q}\) of simpler distributions and then find
\[q^* := \argmin_{q \in \mathcal{Q}} D(q\Vert p)\]
where \(D(\cdot \Vert \cdot)\) is the Kullback-Leibler divergence (which measures &amp;ldquo;distances&amp;rdquo;
between probability distributions, though it is not a metric in the mathematical sense&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;). We
can then use \(q^*\) in place of \(p\) whenever we need to evaluate it.&lt;/p&gt;
&lt;p&gt;This may sound like an enourmous amount of computational overhead: to just evaluate
our objective, we will have to solve an entire optimization problem each time! We will later
find a way to alleviate this issue but for now, let&amp;rsquo;s just ignore it and understand
how we would solve the problem naively.&lt;/p&gt;
&lt;p&gt;To apply this to our problem, we will approximate \(p(z|x)\) with a simpler distribution \(q_\lambda(z)\),
parameterized by a new parameter \(\lambda\). For example, \(q_\lambda\) could be a Gaussian
and \(\lambda\) would be its mean and covariance matrix. Note that while \(q_\lambda(z)\) does not
explicitly depend on \(x\), the optimal parameter
\(\lambda^*\) does depend on \(x\) because we minimize the Kullback-Leibler divergence between
\(q_\lambda\) and \(p(\cdot | x)\).&lt;/p&gt;
&lt;p&gt;The variational inference problem is now minimizing
\[D(q_\lambda\Vert p(\cdot | x)) = \mathbb{E}_{z \sim q_\lambda}\left[\log q_\lambda(z) - \log p(z|x)\right]\]
This still contains the \(p(z|x)\) term that we can&amp;rsquo;t evaluate. But we can get rid of that by writing
\[\mathbb{E}_{z \sim q_\lambda}\left[\log q_\lambda(z) - \log p(z|x)\right]
= \mathbb{E}_{z \sim q_\lambda}\left[\log q_\lambda(z) - \log p(x, z)\right] + \log p(x)\]
We have reintroduced \(\log p(x)\), which is intractable, but crucially,
it doesn&amp;rsquo;t depend on \(\lambda\). So to solve the minimization problem above, we can
also minimize the expected value on the right. Usually, we instead maximize the
negative of that:
\[\argmax_\lambda \E_{z \sim q_\lambda}\left[\log p(x, z) - \log q_\lambda(z)\right]\]
The objective
\[\E_{z \sim q_\lambda}\left[\log p(x, z) - \log q_\lambda(z)\right]\]
is called the &lt;em&gt;evidence lower bound&lt;/em&gt; (ELBO) because it is a lower bound on the log evidence \(\log p(x)\):
\[\E_{z \sim q_\lambda}\left[\log p(x, z) - \log q_\lambda(z)\right] = \log p(x) - D(q_\lambda\Vert p(\cdot|x)) \leq \log p(x)\]
For now this fact isn&amp;rsquo;t really interesting, but it will become relevant later.&lt;/p&gt;
&lt;p&gt;Maximizing the ELBO is finally a tractable problem: we can write
\[\log p(x, z) = \log p(z) + \log p_\theta(x|z)\]
which is something we can easily evaluate. The expectation is over \(q_\lambda\) which
also doesn&amp;rsquo;t pose a problem&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;combining-the-optimization-problems&#34;&gt;Combining the optimization problems&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s briefly recap our progress so far. We originally wanted to find
\[\argmax_\theta \log p_\theta(x)\]
which we rewrote as
\[\argmax_\theta \log p_\theta(x, z) - \log p_\theta(z|x)\]
for an arbitrarily chosen \(z\).
We then used variational inference to approximate the intractable term as
\[\log p_\theta(z|x) \approx \log q_{\lambda^*}(z)\]
where \(\lambda^*\) is the solution to the variational problem:
\[\lambda^*(\theta) = \argmax_\lambda \E_{z \sim q_\lambda}\left[\log p_\theta(x, z) - \log q_\lambda(z)\right]\]&lt;/p&gt;
&lt;p&gt;So we could now in principle plug in this approximation and solve
\[\argmax_\theta \log p_\theta(x) \approx \log p_\theta(x, z) - \log q_{\lambda^*}(z)\]
but there are problems with this.
First, note that \(\lambda^*\) depends on \(\theta\). If we for example use gradient
ascent to optimize over \(\theta\), we would need to find the new
optimal \(\lambda\) after each gradient step.
Second, using an arbitrarily chosen \(z\) is kind of silly: we optimized
\(q_\lambda\) such that the entire distribution approximates \(p(z|x)\) well,
we should make use of this entire distribution.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s go back. We know that the solution to
\[\argmax_\theta \log p_\theta(x) = \argmax_\theta \log p_\theta(x, z) - \log p_\theta(z|x)\]
is the same for any \(z\). So we can also maximize
\[\E_{z \sim q}\left[\log p_\theta(x, z) - \log p_\theta(z|x)\right]\]
instead, for an arbitrary distribution \(q\).
Plugging in our approximation, we get
\[\E_{z \sim q}\left[\log p_\theta(x, z) - \log q_{\lambda^*}(z)\right]\]&lt;/p&gt;
&lt;p&gt;The question now is which distribution \(q\) to use. But note that by using \(q = q_{\lambda^*}\),
we again get the ELBO, this time as the objective for our original optimization problem.
This is a good choice for two reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ELBO is a lower bound on the evidence, \(\text{ELBO} \leq \log p(x)\). If we used another
distribution \(q\), we wouldn&amp;rsquo;t have any guarantee that we&amp;rsquo;re optimizing for the right thing
if the approximation \(p(z|x) \approx q_{\lambda^*}(z)\) became bad enough. This way, we&amp;rsquo;re
at least optimizing a lower bound on what we really care about.&lt;/li&gt;
&lt;li&gt;We saw above that we need to find the new \(\lambda^*(\theta)\) after each update to \(\theta\),
which is very inefficient. But the ELBO is already our objective for \(\lambda\), so now we have
the same optimization objective for both parameters and can optimize them jointly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this choice of \(q = q_{\lambda^*}\), the joint optimization problem becomes
\[\argmax_{\theta, \lambda} \E_{z \sim q_\lambda}\left[\log p_\theta(x, z) - \log q_\lambda(z)\right]\]
We can use the reparameterization trick / pathwise gradients to optimize this
efficiently with gradient ascent.&lt;/p&gt;
&lt;h2 id=&#34;using-an-encoder&#34;&gt;Using an encoder&lt;/h2&gt;
&lt;p&gt;For a single datapoint \(x\), we now have an efficiently solvable problem. But now we get
back to the more interesting setting of an entire dataset \(\mathcal{D}\). We then want to
optimize the likelihood of the entire dataset:
\[\sum_{x \in \mathcal{D}} \log p_\theta(x)\]
The problem is that \(q_\lambda(z)\) is supposed to
approximate \(p(z|x)\), so the optimal \(\lambda\) is different for each datapoint \(x\).
Full variational inference would mean using a separate parameter
\(\lambda\) for each datapoint. The optimization would then be over \(\theta, \lambda_1, \ldots, \lambda_n\),
where \(\lambda_i\) is the parameter for the \(i\)-th datapoint. This again gets us into the realm
of a huge number of parameters and computational infeasibility.&lt;/p&gt;
&lt;p&gt;So instead, we use &lt;em&gt;&lt;a href=&#34;https://gordonjo.github.io/post/amortized%5Fvi/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amortized variational inference&lt;/a&gt;&lt;/em&gt;. This means that instead of optimizing parameters for
\(q_\lambda\) for each \(x\), we learn a function \(x \mapsto \lambda\). This function is trained to approximate the optimal solution
\(\lambda^*(x)\). The downside is that we&amp;rsquo;re introducing yet another approximation, which can only
worsen how well we maximize the likelihood \(p(x)\). But the big advantage is that evaluating it is
much cheaper than solving an entire optimization problem.&lt;/p&gt;
&lt;p&gt;In practice, this means we train a neural
network \(f_\varphi(x)\) to find the best \(\lambda\) (in terms of the objective above) for a given \(x\). We then
use \(q_{f_\varphi(x)}(z)\) in place of \(q_{\lambda^*}(z)\). To make the notation a bit nicer,
we write this as
\[q_\varphi(z|x) := q_{f_\varphi(x)}(z)\]&lt;/p&gt;
&lt;p&gt;Then we finally get the VAE objective:
\[\argmax_{\theta, \varphi} \E_{z \sim q_\varphi}\left[\log p(z)p_\theta(x|z) - \log q_\varphi(z|x)\right]\]&lt;/p&gt;
&lt;h2 id=&#34;connection-to-vaes-in-practice&#34;&gt;Connection to VAEs in practice&lt;/h2&gt;
&lt;p&gt;As you&amp;rsquo;ve probably guessed by now, \(p_\theta(x|z)\) is the decoder of a VAE
and \(q_\varphi(z|x)\) is the encoder. The ELBO can be rewritten as
\[\begin{aligned}\E_{z \sim q(\cdot | x)} \left[\log p(z)p(x|z) - \log q(z|x)\right]
&amp;amp;= \E_{z \sim q(\cdot | x)} \log p(x|z) - \E_{z \sim q(\cdot | x)} \log \frac{q(z|x)}{p(z)}\\\
&amp;amp;= \E_{z \sim q(\cdot | x)}\log p(x | z) - D(q(\cdot|x)\Vert p)\end{aligned}\]
which gives us the interpretation as &amp;ldquo;reconstruction + regularization loss&amp;rdquo;
that you may have encountered elsewhere (to treat this as a loss that is minimized,
you would multiply everything by \(-1\)).&lt;/p&gt;
&lt;p&gt;\(q_\varphi\) is typically chosen as a normal distribution, because that makes the KL divergence
in the ELBO easy to calculate if \(p(z)\) is chosen as a unit normal. The choice of \(p_\theta\)
depends on the type of data. As mentioned, for binary images we might use independent
Bernoulli distributions for each pixel. For continuous output, a normal distribution is a common
choice.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We saw how to arrive at VAEs starting from a purely generative motivation, without
assuming an autoencoder architecture a priori. Interestingly, this gives a very different
impression than the &amp;ldquo;autoencoder perspective&amp;rdquo;: what we really care about is the
decoder, whereas the encoder is just a useful trick to be able to train the decoder efficiently.&lt;/p&gt;
&lt;p&gt;This doesn&amp;rsquo;t mean that the autoencoder perspective is wrong of course. Having an encoder
can be intrinsically useful for some applications, and this is something which is missing
in this post. But I think the perspective we took here demonstrates that the VAE architecture
is far less arbitrary than it may seem when starting from autoencoders.&lt;/p&gt;
&lt;h2 id=&#34;further-reading&#34;&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://ermongroup.github.io/cs228-notes/extras/vae/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 228 lecture notes on VAEs&lt;/a&gt; take a somewhat similar approach to this post in terms of emphasizing
the variational inference perspective. They Also contain details on some points that I basically ignored, for example on the reparameterization
trick&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.05908&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carl Doersch&amp;rsquo;s tutorial on VAEs&lt;/a&gt; contains much more detail and also has a different motivation for why
we want to approximate \(p(z|x)\) (namely to use that to estimate the integral by sampling values of \(z\)
that contribute the most)&lt;/li&gt;
&lt;li&gt;There is also a &lt;a href=&#34;https://arxiv.org/abs/1906.02691&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial by Kingma and Welling&lt;/a&gt;, the authors who introduced VAEs. You could also look
at their &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original paper&lt;/a&gt; but that&amp;rsquo;s a lot terser&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Michael Nielsen calls this &amp;ldquo;discovery fiction&amp;rdquo;, mentioned for example &lt;a href=&#34;http://cognitivemedium.com/srs-mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;If you intrinsically care about \(p(z|x)\), for example because you hope the latent variables will have an interesting meaning, this and parts of the remaining post are unnecessary, you&amp;rsquo;ll get to VAEs more directly. But my point is that you don&amp;rsquo;t &lt;em&gt;need&lt;/em&gt; that motivation &amp;ndash; VAEs arise pretty naturally even if you only care about finding a good model \(p_\theta(x)\) of the training data.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;In particular, the Kullback-Leibler divergence is not symmetric, which raises the question why we use \(D(q\Vert p)\) and not \(D(p\Vert q)\). The reason is that the latter would itself lead to an intractable optimization problem and so we wouldn&amp;rsquo;t have made any progress.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;If you&amp;rsquo;ve read the previous footnote: this is the point where using the other KL divergence would mean we&amp;rsquo;re stuck because we have an expectation with respect to \(p(z|x)\)&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
