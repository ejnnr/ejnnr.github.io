<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Structure | Erik Jenner</title>
    <link>https://ejenner.com/tag/structure/</link>
      <atom:link href="https://ejenner.com/tag/structure/index.xml" rel="self" type="application/rss+xml" />
    <description>Structure</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 27 Jan 2021 08:53:00 +0100</lastBuildDate>
    <image>
      <url>https://ejenner.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Structure</title>
      <link>https://ejenner.com/tag/structure/</link>
    </image>
    
    <item>
      <title>Too much structure</title>
      <link>https://ejenner.com/post/too-much-structure/</link>
      <pubDate>Wed, 27 Jan 2021 08:53:00 +0100</pubDate>
      <guid>https://ejenner.com/post/too-much-structure/</guid>
      <description>&lt;p&gt;When proving simple statements in point set topology, there is often only
one obvious next step that can be done given the objects and statements you
already have&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. You don&amp;rsquo;t need to think about what you eventually want to prove
because there is only one step that will lead to a proof of &lt;em&gt;anything&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;an-example-continuous-images-of-compact-spaces-are-compact&#34;&gt;An example: continuous images of compact spaces are compact&lt;/h2&gt;
&lt;p&gt;As an example, let&amp;rsquo;s go through the proof that the image of a compact space
under a continuous map is again compact: we start with a compact space \(X\)
and a map \(f: X \to Y\). To make the notation less unwieldy, we&amp;rsquo;ll assume that \(f\) is surjective,
so we&amp;rsquo;ll show that \(Y\) is compact, but the proof works exactly the same without this assumption
(just replace every occurence of \(Y\) with \(f(X)\)).
Because we want to show that \(Y\) is compact,
i.e. that every open cover of \(Y\) has a finite subcover, we also start with
a given open cover \(Y = \bigcup_i U_i\).&lt;/p&gt;
&lt;p&gt;With only these objects available, there isn&amp;rsquo;t a lot we can do. For example,
if \(X\) were a normed vector space, we would have access to its zero vector
and then could construct \(f(0)\) from that. That wouldn&amp;rsquo;t lead anywhere but it&amp;rsquo;s a branch in the
tree of possible proofs that might distract us. Because \(X\) and \(Y\) have
so little structure, these kinds of options simply don&amp;rsquo;t exist.&lt;/p&gt;
&lt;p&gt;The only thing I can come up with is that we can look at the preimage of
each of the \(U_i\) under \(f\). This gives us a collection \(f^{-1}(U_i), i \in I\)
of subsets of \(X\). Such a collection in itself still doesn&amp;rsquo;t allow us to do anything
interesting, but because preimages preserve unions, we have
\[\bigcup_i f^{-1}(U_i) = f^{-1}\left(\bigcup_i U_i\right) = f^{-1}(Y) = X\]
so this collection is in fact a cover of \(X\). Because \(f\) is continuous, it is also
an &lt;em&gt;open&lt;/em&gt; cover.&lt;/p&gt;
&lt;p&gt;Again, there isn&amp;rsquo;t much we can do with this newly constructed open cover. We could
map it back to \(Y\) with \(f\) immediately but that just gives us back the open cover
of \(Y\) we started with.
The other thing we can do with an open cover of \(X\) is pick a finite subcover:
\(X\) is compact, and we can think of that procedurally as a way of turning any
open cover into a finite subcover.&lt;/p&gt;
&lt;p&gt;So now we have a new object: a finite open subcover \(X = U_{i_1} \cup \ldots \cup U_{i_k}\).
Inside \(X\), there isn&amp;rsquo;t anything else we can do with a (finite) cover, so the only
option is to now apply \(f\) again, which gives us sets \(f(f^{-1}(U_{i_1})), \ldots, f(f^{-1}(U_{i_k})) \subset Y\).
Because \(f\left(f^{-1}(U_i)\right) = U_i\), this is a finite subset of the open cover we
started with.&lt;/p&gt;
&lt;p&gt;An then we&amp;rsquo;re done because it is also a cover:
\[\bigcup_{j = 1}^k f\left(U_{i_j}\right) = f\left(\bigcup_{j = 1}^k U_{i_j}\right) = f(X) = Y\]&lt;/p&gt;
&lt;p&gt;The thing that I hope you took away from this walkthrough is how few choices
there were at each step. Apart from some steps that obviously didn&amp;rsquo;t add anything new,
there was always only one thing to do next.&lt;/p&gt;
&lt;p&gt;We didn&amp;rsquo;t even specifically aim to construct a finite subcover of \(\bigcup_i U_i\)
for most of the proof, we just &amp;ldquo;went with the flow&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This is a feeling that is much more rare in e.g. real analysis, even for proofs that
are similarly easy as the one above. With some experience, you might get enough
intuition to discard all the wrong options immediately but they&amp;rsquo;ll still be there.
You typically have to keep in mind what you want to prove and deliberately
steer your proof in that direction, otherwise the number of possible paths you
could take just explodes and you never get anywhere.&lt;/p&gt;
&lt;h2 id=&#34;the-importance-of--lack-of--structure&#34;&gt;The importance of (lack of) structure&lt;/h2&gt;
&lt;p&gt;The decisive difference between the point set topology example and real analysis is,
I think, how much structure the spaces and objects
we are working with have. By &amp;ldquo;structure&amp;rdquo;, I mean the same somewhat
elusive concept I&amp;rsquo;ve previously talked about &lt;a href=&#34;https://ejenner.com/post/perspectives-on-structure&#34;&gt;here&lt;/a&gt;. In short, a group is
a set with some additional structure and a field adds even more structure.
The way I use the word, a manifold also has more structure than a topological
space (even though it doesn&amp;rsquo;t require any new choices).&lt;/p&gt;
&lt;p&gt;One of the aspects of structure I talked about in the post I just linked is
that objects with less structure admit fewer definitions and theorems.
Applying theorems to the objects we&amp;rsquo;ve already constructed is how we
make progress in our proofs. So having fewer theorems to work with
leads to a proof tree with a lower branching factor: at each step of
the proof, there are only a few things we can do. In an extreme case,
we have a branching factor of one and can do the proof on autopilot,
as in the topology example above.&lt;/p&gt;
&lt;p&gt;If you are working on \(\mathbb{R}^n\) on the other hand, you can use all the topological
properties you had before, but you can also view \(\mathbb{R}^n\) as a vector space, you
can talk about lengths and angles and even about the Lebesgue measure of sets. This is possible
because \(\mathbb{R}^n\) has a lot of canonical structure,
so you suddenly have many more tools at your disposal.&lt;/p&gt;
&lt;p&gt;This explains why it can help to generalize a statement you are trying to prove:
afterwards, you have less structure to work with. Assuming the statement is still
true in its more general form, the tree of possible proofs has a much smaller
branching factor and becomes easier to explore.&lt;/p&gt;
&lt;h2 id=&#34;propositions-as-types&#34;&gt;Propositions as types&lt;/h2&gt;
&lt;p&gt;One last thing to mention is that all of this is closely connected to the &amp;ldquo;propositions as types&amp;rdquo;
interpretation: mathematical propositions can be interpreted as types, with proving a proposition
corresponding to constructing a term of that type. I already talked about constructing new objects
using the available objects and theorems and this is exactly the same idea but the language of
type theory formalizes this. If you want to see an example like the topology proof I gave but explicitly
using the propositions as types view, check out section II. in &lt;a href=&#34;https://www.lesswrong.com/posts/Xfw2d5horPunP2MSK/dependent-type-theory-and-zero-shot-reasoning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;. If you haven&amp;rsquo;t seen
the correspondence between propositions and types before and want to learn more,
&lt;a href=&#34;https://www.youtube.com/watch?v=IOiZatlZtGU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this talk&lt;/a&gt; is very fun to watch.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Note the &amp;ldquo;simple&amp;rdquo; &amp;ndash; there are obviously really hard to prove statements in point set topology, as in any discipline&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ways to think about structure in mathematics</title>
      <link>https://ejenner.com/post/perspectives-on-structure/</link>
      <pubDate>Tue, 29 Dec 2020 14:03:00 +0100</pubDate>
      <guid>https://ejenner.com/post/perspectives-on-structure/</guid>
      <description>&lt;p&gt;Most of the objects that appear in mathematics can be thought of as
sets with additional &amp;ldquo;structure&amp;rdquo;. For example, a group is a set \(G\) with
an operation \(G \times G \to G\) fulfilling certain axioms. This operation
is what makes a group feel more &amp;ldquo;structured&amp;rdquo; than a simple set of elements.
A topological space is a set equipped with a topology
and there is a myriad of other examples (graphs, ordered sets, vector spaces,
metric spaces and measure spaces to name a few).&lt;/p&gt;
&lt;p&gt;But &amp;ldquo;structure&amp;rdquo; in this sense is a somewhat elusive concept. We know it when we see it
but it&amp;rsquo;s hard to describe explicitly &amp;ndash; which is why I just gave some examples and hoped
you knew what I meant.
&lt;em&gt;(Sidenote: there is also a more formal notion of structure in mathematical logic but that&amp;rsquo;s not the topic of this post)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The goal of this post is not to give a formal definition of structure &amp;ndash; I&amp;rsquo;m not sure
how helpful that would even be &amp;ndash; but rather to describe different perspectives
that may be useful when thinking about it.
To guide us, we will consider one particular question: what does it mean
to say that object A has &amp;ldquo;more structure&amp;rdquo; than object B? For example, a vector space
has more structure than a group, which has more structure than a simple set.
We will start with more formal (but also more boring) perspectives and then work
our way towards more speculative and fuzzy ones.&lt;/p&gt;
&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll fix a set \(X\) and consider the different possible structures that can be imposed
on \(X\). Calligraphy letters like \(\mathcal{A}\) and \(\mathcal{B}\) refer to the set of all possible structures
of some type, for example the set of all groups on \(X\). Particular instances
are written as \(A \in \mathcal{A}\) (e.g. a particular group on \(X\)).&lt;/p&gt;
&lt;p&gt;We will write \(\mathcal{A} \prec \mathcal{B}\) for the informal notion that \(\mathcal{A}\)
has &lt;em&gt;more&lt;/em&gt; structure than \(\mathcal{B}\), for example \(\text{fields} \prec \text{groups}\).
An alternative way to think about this (which hopefully explains why the \(\prec\) sign points the way
it does) is that fields are a special case of groups (each field is also a group),
which means that the set of fields is in some sense a subset of the set of groups.
This leads us right into the first perspective on structure.&lt;/p&gt;
&lt;h2 id=&#34;structure-can-be-canonically-removed&#34;&gt;Structure can be canonically removed&lt;/h2&gt;
&lt;p&gt;If \(\mathcal{A} \prec \mathcal{B}\) (\(\mathcal{A}\) has more structure than \(\mathcal{B}\)),
then there is a canonical way to turn any instance
\(A \in \mathcal{A}\) into an instance \(B \in \mathcal{B}\).
As an example, a vector space can be canonically turned into a group by just using vector
addition as the group operation and ignoring scalar multiplication. Or any metric space
can be treated as just a topological space by using the topology induced by the metric
and ignoring the metric itself (category theory footnote&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;h2 id=&#34;structure-leads-to-smaller-symmetry-groups&#34;&gt;Structure leads to smaller symmetry groups&lt;/h2&gt;
&lt;p&gt;If \(\mathcal{A} \prec \mathcal{B}\), then the automorphism group&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; of \(A \in \mathcal{A}\)
is smaller than the group of the corresponding \(B \in \mathcal{B}\) (where &amp;ldquo;corresponding&amp;rdquo;
means that \(B\) is just \(A\) with parts of the structure removed as described in the previous
section).&lt;/p&gt;
&lt;p&gt;For example, we can treat the real numbers as a metric space or as a topological space.
For a metric space, the automorphism group consists of only isometries (i.e. maps that
don&amp;rsquo;t change distances between points), which for the real numbers are only translations.
If we treat them as a topological space though (which has a lot less structure), then the
automorphisms are all the homeomorphisms of the real number line, which form a much larger
group.&lt;/p&gt;
&lt;h2 id=&#34;more-structure-leads-to-fewer-structure-preserving-maps&#34;&gt;More structure leads to fewer structure-preserving maps&lt;/h2&gt;
&lt;p&gt;If we consider two sets \(X\) and \(Y\), there are \(|Y|^{|X|}\) maps from \(X\) to \(Y\). If we now introduce
a group structure, most of those maps are typically not homomorphisms, i.e. not structure-preserving.
If we then turn the groups into rings, even fewer maps will additionally be compatible with
the ring multiplication. So adding structure reduces the number of maps which preserve
all of that structure (which is pretty obvious when put like that).&lt;/p&gt;
&lt;p&gt;The previous perspective is a special case of this, where \(Y = X\) and we only consider automorphisms
rather than any structure-preserving maps, so it shouldn&amp;rsquo;t be surprising that we also got fewer automorphisms
if we had more structure. But I think it&amp;rsquo;s a very important special case that deserves to be treated seperately
because the interpretation via symmetries makes it much more intuitive than this general
version.&lt;/p&gt;
&lt;h2 id=&#34;structure-allows-more-definitions-and-theorems&#34;&gt;Structure allows more definitions and theorems&lt;/h2&gt;
&lt;p&gt;Now we start getting into slightly more hand-wavy territory. If \(\mathcal{A} \prec \mathcal{B}\), then
there are more concepts we can define for objects with structure \(\mathcal{A}\) than for
objects with structure \(\mathcal{B}\). We can also prove more and stronger theorems
about objects with structure \(\mathcal{A}\) then about objects with structure \(\mathcal{B}\).
This is related to the previous observation that any object with structure \(\mathcal{A}\)
can be canonically turned into one with structure \(\mathcal{B}\). The important observation
here is that this process &amp;ldquo;is compatible with definitions and theorems&amp;rdquo; (I told you it was getting hand-wavy).
What I mean by that is that if some property holds for \(B\), then it also holds for any object
\(A\) which can be turned into \(B\) by forgetting parts of its structure.&lt;/p&gt;
&lt;p&gt;Some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On a Riemannian manifold, we can do things like measure the angle at which two curves intersect,
which is simply not a concept that makes sense for a manifold without a metric&lt;/li&gt;
&lt;li&gt;Rings allow us to talk about divisibility, which does not have an analogon if only a group structure
is avaliable&lt;/li&gt;
&lt;li&gt;All vector spaces have a basis but the same is not true for all modules (which have less structure
than vector spaces)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While adding structure &amp;ldquo;preserves definitions and theorems&amp;rdquo;, it can sometimes make definitions
trivial or collapse certain distinct concepts into one. For example, divisibility becomes very boring
on fields because every element is divisible by every other element (except 0).&lt;/p&gt;
&lt;h2 id=&#34;algorithmic-complexity&#34;&gt;Algorithmic complexity&lt;/h2&gt;
&lt;p&gt;Now it&amp;rsquo;s getting really hand-wavy, so activate your lack-of-rigor-deflectors.&lt;/p&gt;
&lt;p&gt;Loosely speaking, algorithmic complexity (or Kolmogorov complexity) measures how long the shortest
possible description of some object is. This can be formally defined for bit sequences
but I will appeal to your intuition to also apply it to other things like mathematical
structures, without explicitly specifying how to encode those as bit sequences.&lt;/p&gt;
&lt;p&gt;One connection between structure and complexity is quite obvious: if \(\mathcal{A} \prec \mathcal{B}\),
then describing \(\mathcal{A}\) is more complex. For example, describing what a field is
takes slightly longer than just describing what a group is because there are more axioms
that need to be specified. Similarly, defining a Riemannian manifold is more complex
than just defining what a topological space is. Note that I switched from talking about
a description of \(\mathcal{A}\), e.g. the set of all groups, to talking about a definition of what a group is.
But in terms of descriptive complexity those are essentially the same since the shortest
description of the set of all groups on \(X\) consists of a definition of what a group
is and then saying &amp;ldquo;all groups on \(X\)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;There are some cases where this complexity perspective becomes a bit of a stretch. For example,
it&amp;rsquo;s not obvious that defining a metric space is more complex than defining a topological space
(unlike in the case of fields and groups, where the hierarchy is clear). I&amp;rsquo;d argue that it is in fact
more complex because you need concepts like the real numbers which are pretty complicated
compared to topological spaces. But there might be other examples where there really is a very
short description of something which nevertheless has a lot of structure in terms of the other
perspectives above. This is fine: our goal here is not to give a formal definition of structure
but rather to list some of the properties that are typically associated with it.&lt;/p&gt;
&lt;p&gt;There is another, more interesting way in which complexity comes into play when talking about
structure: how long is an average description of a particular element \(A \in \mathcal{A}\) (given
a description of \(\mathcal{A}\))? Some examples to build intuition about this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specifying a topological space can be extremely complex. Because there is such a large number of
possible topologies on a fixed set, most of them need to have very long desriptions.
Also note that those topological spaces with very simple descriptions are often those that have
a natural additional structure. For example, to define the Euclidean topology on \(\mathbb{R}^n\),
we usually first define its vector space structure, use that to define a metric and then use that
to define a topology&lt;/li&gt;
&lt;li&gt;Specifying a field on a finite set is very easy: there is at most one anyways (up to isomorphism)&lt;/li&gt;
&lt;li&gt;If the cardinality of \(X\) is prime, there is also only one group on \(X\). Otherwise, there might be more
but still far fewer than there are topologies&lt;sup&gt;[I think, citation needed]&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This seems to point towards more structure making it easier to specify a particular instance. But this is
not always the case. For example, a Riemannian smooth manifold has more structure than just a smooth
manifold (according to all the previous perspectives). But since every smooth manifold can be equipped
with a Riemannian metric but that metric is not uniquely determined by the manifold, describing
a Riemannian manifold usually takes longer than just describing a smooth manifold without a metric,
because the choice of metric needs to be specified.&lt;/p&gt;
&lt;p&gt;In general, adding structure means that there might be additional choices that need to be specified
(such as a Riemannian metric) but it can also impose restrictions (for example, many topological spaces
can&amp;rsquo;t be turned into metric spaces). These two factors pull the descriptive complexity of individual
instances in opposite directions.&lt;/p&gt;
&lt;h2 id=&#34;inherent-structure-of-objects&#34;&gt;Inherent structure of objects&lt;/h2&gt;
&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; a new perspective for thinking about structure. Instead, I will give an example for a possible &amp;ldquo;application&amp;rdquo; of the
complexity-based perspective. Hopefully that will illustrate how these perspectives can be useful
to have in your mental toolkit.&lt;/p&gt;
&lt;p&gt;There are interesting connections between what we discussed in the previous section and &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%5Fstructure%5Ffunction#The%5Falgorithmic%5Fsufficient%5Fstatistic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov sufficient statistics&lt;/a&gt;. Intuitively speaking,
the Kolmogorov sufficient statistic of a bit string is the part of that string that has &amp;ldquo;structure&amp;rdquo; in the
sense of not being algorithmically random. Any bit string can be efficiently described by first describing
its Kolmogorov sufficient statistic (which is a list of bit strings with the same &amp;ldquo;structure&amp;rdquo;) and then
specifying its algorithmically random component (by giving its index in that list).&lt;/p&gt;
&lt;p&gt;This is exactly analogous to describing e.g. a group on \(X\) by first defining what a group is (or rather
defining an enumeration of all groups on \(X\)) and then saying &amp;ldquo;the 14th object in that enumeration&amp;rdquo;.
The important property of the Kolmogorov sufficient statistic is that this description in two parts is
efficient (there is no shorter description using some other scheme, up to an additive constant).
As an example where this is not the case, we could also specify a group by first defining an enumeration of monoids and then
saying &amp;ldquo;the 247th object in that enumeration&amp;rdquo;. But because there are many more monoids than group,
this description would probably be inefficient in most cases: we save ourselves the specification
of a single axiom but we pay by needing to specify a much higher index.&lt;/p&gt;
&lt;p&gt;Perhaps this idea can be used to define the &amp;ldquo;true inherent structure&amp;rdquo; on an object as its Kolmogorov
sufficient statistic. But fleshing that out is a topic for another post.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In summary, here are all the perspectives we talked about:
If an object has more structure, &amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this structure can always be canonically removed&lt;/li&gt;
&lt;li&gt;it has fewer symmetries&lt;/li&gt;
&lt;li&gt;there are fewer maps between it and other objects that preserve all the structure&lt;/li&gt;
&lt;li&gt;more concepts can be defined and more theorems proven&lt;/li&gt;
&lt;li&gt;specifying the class of objects with that structure tends to be more complex&lt;/li&gt;
&lt;li&gt;specifying that particular object is often easier because the structure
restricts the space of options, but there are exceptions&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;In category theory these are forgetful functors but I&amp;rsquo;m just interested in intuition here, not formalism. In this example, there is also a natural way to turn any abelian group into a \(k\)-vector space, for a given field \(k\), by tensoring with \(k\). But that vector space won&amp;rsquo;t be over \(X\) anymore and in many other cases there is no canonical way to add structure at all.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;The automorphism group is the set of all isomorphisms from an object to itself (which becomes a group via composition as the group operation)&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
