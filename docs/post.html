<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="icon" href="./favicon.png" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	
		<link href="./_app/immutable/assets/0.BV6eIN1u.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.D3urAubC.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/BZIfHQ7j.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/CXEaPN9q.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/BH9Iu36z.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/IITJ3E17.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.Dlb8Ldyb.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/DgNpRiRI.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/ZND8Q6sY.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/lNFSlT5-.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/CIAIJGM7.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.DUYzCeOk.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/BpsNOneS.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/3.BIwGx_Sr.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/D1wvszJw.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/dy7G579c.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/CkMDHBy_.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/Dw7dpdZy.js"><!--[--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="" crossorigin="anonymous"><!--]--><!--[--><meta name="description" content="All blog posts"><!--]--><title>Blog Posts</title>
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents"><!--[--><!--[--><!----><header class="sticky top-0 z-10 w-full bg-white/90 shadow-sm backdrop-blur-sm transition-transform duration-300 ease-in-out relative // Apply fixed/translate classes based on state"><nav class="mx-auto flex max-w-3xl items-center p-4"><div class="space-x-4"><a href="/" class="text-gray-600 hover:text-blue-600">Home</a> <a href="/post" class="text-gray-600 hover:text-blue-600">Posts</a></div></nav></header> <main class="mx-auto max-w-3xl p-4"><!----><div class="mx-auto max-w-3xl px-4 py-8"><!--[--><div class="grid gap-8"><!--[--><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/swoosh" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Swoosh: Rethinking Activation Functions</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2023-04-01T00:00:00.000Z">April 1, 2023</time><!--]--></div> <!--[!--><!--]--> <!--[--><div class="text-gray-700"><!----><p>Introducing the new Swoosh activation function. Perfect test set generalization
guaranteed.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/einsum" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Einsum is easy and useful</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2022-11-05T00:00:00.000Z">November 5, 2022</time><!--]--></div> <!--[!--><!--]--> <!--[--><div class="text-gray-700"><!----><p><code>einsum</code> is one of the most useful functions in Numpy/Pytorch/Tensorflow and yet many people don&#39;t use it. It seems to have a reputation as being difficult to understand and use, which is completely backwards in my view: the reason <code>einsum</code> is great is precisely because it is <em>easier</em> to use and reason about than the alternatives. So this post tries to set the record straight and show how simple <code>einsum</code> really is.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/distributions-operations" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Distributions Part II: What can we do with distributions?</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2022-03-05T23:00:00.000Z">March 5, 2022</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Math</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  As promised in part I, we can do a lot of the same things with Schwartz
  distributions as with classical functions. To see how, we&#39;ll cover
  derivatives, convolutions, and Fourier transforms of distributions.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/random-vs-grid-search" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Visualizing random vs grid search</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-12-22T00:00:00.000Z">December 22, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Machine learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>Random search usually works better than grid search
  for hyperparameter optimization. This brief post suggests a way
  to visualize the reason for this geometrically.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/karger-extensions" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Extensions of Karger's algorithm</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-09-10T09:41:00.000Z">September 10, 2021</time><!--]--></div> <!--[!--><!--]--> <!--[--><div class="text-gray-700"><!----><script>
  import Spoiler from '$lib/components/Spoiler.svelte';
  import Figure from '$lib/components/Figure.svelte';
</script>


<p><em>If you prefer videos, check out our <a href="https://www.youtube.com/watch?v=1sURbgamyvU">ICCV presentation</a>, which covers similar content as this blog post.
For more details, see <a href="http://arxiv.org/abs/2110.02750">our paper</a>.</em>...</p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/distributions-intro" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Distributions Part I: the Delta distribution</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-07-06T12:15:00.000Z">July 6, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Math</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Did you always want to know kind of object this weird Dirac delta &quot;function&quot;
  actually is? Well, it&#39;s a Schwartz distribution. If that doesn&#39;t help much,
  then keep reading.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/automation-productivity" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Scripting for personal productivity</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-04-14T17:42:00.000Z">April 14, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Productivity</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  If you can program, you can use that to support your habits and automate
  some routines. This post gives a few examples.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/bias-variance-tradeoff" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Troubles with the Bias-Variance tradeoff</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-04-07T16:19:00.000Z">April 7, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Machine learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  The bias-variance tradeoff is a key idea in machine learning. But I&#39;ll
  argue that we know surprisingly little about it: when does it hold?
  How does it relate to the Double Descent phenomenon? And what do we
  even formally mean when we talk about it?
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/computer-tips" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Collection of quick computer tips</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-03-31T12:49:00.000Z">March 31, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Productivity</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Many of us spend a lot of time working with our computer, so it&#39;s worth
  spending some time to make that experience as pleasent and productive
  as possible. This is a collection of tips that are relatively quick
  to implement and still very valuable in the long run in my opinion.
  Mainly geared towards developers and others who work with the shell
  a lot.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/state-formally-reason-informally" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">State formally, reason informally</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-03-24T10:00:00.000Z">March 24, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Math</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  There&#39;s a style of teaching mathematics that I really like: stating definitions
  and theorems as formally as in any textbook, but focusing on informal arguments
  for why they should be true.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/latex-emacs" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Emacs as an amazing LaTeX editor</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-03-17T13:14:00.000Z">March 17, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Productivity</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Emacs has some really amazing features for writing LaTeX; this post gives
  an overview of some of them, either to convince you to give Emacs a try,
  or to make you aware that these features exist if you&#39;re already using
  Emacs but didn&#39;t know about them.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/spherical-harmonics" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Perspectives on spherical harmonics</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-03-10T16:07:00.000Z">March 10, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Math</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Spherical harmonics are ubiquitous in math and physics, in part because
  they naturally appear as solutions to several problems; in particular they
  are the eigenfunctions of the spherical Laplacian and the irreducible
  representations of SO(3). But why should the solutions to these problems
  be the same? And why are they called spherical harmonics?
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/implicit-layers" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Deep Implicit layers</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-03-03T13:48:00.000Z">March 3, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Deep learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Several new architectures for neural networks, such as Neural ODEs and
  deep equlibirum models can be understood as replacing classical layers
  that explicitly specify how to compute the output with implicit layers.
  These layers describe which conditions the output should specify but
  leave the actual computation up to some solver that can be chosen arbitrarily.
  This post contains a brief introduction to the main ideas behind implicit layers.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/rl-building-blocks-3" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Building Blocks of RL Part III: Model-based RL</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-02-24T09:41:00.000Z">February 24, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Reinforcement learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is
  the conclusion of a series with such an overview, covering model-based RL.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/sparsity-singularities" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">L1 regularization: sparsity through singularities</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-02-17T08:33:00.000Z">February 17, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Machine learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  L1 regularization is famous for leading to sparse optima, in contrast to
  L2 regularization. There are several ways of understanding this but I&#39;ll
  argue that it&#39;s really all about one fact: the L1 norm has a singularity
  at the origin, while the L2 norm does not. And this is not just true
  for L1 and L2 regularization: singularities are always necessary to get sparse weights.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/boring-numbers" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Boring numbers, complexity and Chaitin's incompleteness theorem</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-02-10T15:27:00.000Z">February 10, 2021</time><!--]--></div> <!--[!--><!--]--> <!--[--><div class="text-gray-700"><!----><p>  There is a &quot;complexity barrier&quot;: a number such that we can&#39;t prove
  the Kolmogorov complexity of any specific string to be larger than
  that. The proof of this astonishing fact is closely related to some
  famous paradoxa and we&#39;ll use this connection to get a better intuition
  for why the complexity barrier exists.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/rl-building-blocks-2" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Building Blocks of RL Part II: Policy Optimization</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-02-03T06:39:00.000Z">February 3, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Reinforcement learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is part 2
  of a series with such an overview, covering some policy optimization methods.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/too-much-structure" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Too much structure</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-01-27T07:53:00.000Z">January 27, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Structure</span><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Math</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Proving things for object that have a lot of structure can be harder
  than for object with less structure, simply because the tree of possible
  proofs is much wider. This is probably why trying to prove a more general
  case is sometimes a helpful strategy.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/position-momentum-asymmetry" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Asymmetry between position and momentum in physics</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-01-19T09:52:00.000Z">January 19, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Physics</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  In both classical mechanics and QM, there are transformations between position-based
  and momentum-based representations that preserve the dynamical laws. So from
  a mathematical perspective, position and momentum seem to play equivalent roles
  in physics. But they don&#39;t play equivalent roles in our cognition, which is part of
  the physical universe -- seemingly a paradox.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/rl-building-blocks-1" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Building Blocks of RL Part I: Value-based methods</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-01-13T15:58:00.000Z">January 13, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Reinforcement learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Reinforcement Learning consists of a few key building blocks that can be combined to create
  many of the well-known algorithms. Framing RL in terms of these building blocks
  can give a good overview and better understanding of these algorithms. This is part 1
  of a series with such an overview, covering value-based methods (mainly in a tabular
  setting).
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/vae-generative" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">VAEs from a generative perspective</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2021-01-06T13:45:00.000Z">January 6, 2021</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Deep learning</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  Variational autoencoders are usually introduced as a probabilistic extension of autoencoders
  with regularization. An alternative view is that the encoder arises naturally as a tool
  for efficiently training the decoder. This is the perspective I take in this post, deriving
  VAEs without assuming an autoencoder architecture a priori.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/perspectives-on-structure" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Ways to think about structure in mathematics</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2020-12-29T13:03:00.000Z">December 29, 2020</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Structure</span><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Math</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  &quot;Structure&quot; is a concept that keeps popping up when thinking about mathematics
  but it&#39;s hard to pin down what it is exactly. I discuss several different perspectives
  for thinking about it.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/karger-stein" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Trading off speed against the probability of success in the Karger-Stein Algorithm</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2020-12-06T17:00:00.000Z">December 6, 2020</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Graphs</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  The Karger-Stein algorithm is an improvement over Karger&#39;s beautiful contraction
  algorithm for minimum graph cuts. In this post, I show how it finds the perfect
  tradeoff between finding a mincut with high probability and finding it quickly.
  In the course of doing so, we will also understand where the somewhat opaque
  factor of sqrt(2) comes from.
  </p>
<!----></div><!--]--></a></article><article class="overflow-hidden rounded-lg border border-gray-200 shadow-sm transition-shadow hover:shadow-md"><a href="/post/discounting-relativistic-universe" class="block p-6"><h2 class="mb-2 text-2xl font-bold tracking-tight text-gray-900">Discounting in a relativistic universe</h2> <div class="mb-3 flex items-center gap-4 text-sm text-gray-600"><!--[--><time datetime="2020-06-20T10:25:00.000Z">June 20, 2020</time><!--]--></div> <!--[--><div class="mb-3 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Physics</span><!--]--></div><!--]--> <!--[--><div class="text-gray-700"><!----><p>  For people who want to discount the future, special relativity creates
  some challenges. There are different ways to handle those but none
  seem completely satisfactory which may be yet another argument against
  discounting pure utilities.
  </p>
<!----></div><!--]--></a></article><!--]--></div><!--]--></div><!----><!----></main><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_jgw6mo = {
						base: new URL(".", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("./_app/immutable/entry/start.D3urAubC.js"),
						import("./_app/immutable/entry/app.Dlb8Ldyb.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data: [null,{"type":"data","data":{posts:[{title:"Swoosh: Rethinking Activation Functions",date:"2023-04-01T00:00:00.000Z",summary:"Introducing the new Swoosh activation function. Perfect test set generalization\nguaranteed.\n  ",tags:[],path:"/post/swoosh",slug:"swoosh"},{title:"Einsum is easy and useful",date:"2022-11-05T00:00:00.000Z",summary:"`einsum` is one of the most useful functions in Numpy/Pytorch/Tensorflow and yet many people don't use it. It seems to have a reputation as being difficult to understand and use, which is completely backwards in my view: the reason `einsum` is great is precisely because it is *easier* to use and reason about than the alternatives. So this post tries to set the record straight and show how simple `einsum` really is.\n  ",tags:[],path:"/post/einsum",slug:"einsum"},{title:"Distributions Part II: What can we do with distributions?",date:"2022-03-05T23:00:00.000Z",summary:"  As promised in part I, we can do a lot of the same things with Schwartz\n  distributions as with classical functions. To see how, we'll cover\n  derivatives, convolutions, and Fourier transforms of distributions.\n  ",tags:["Math"],path:"/post/distributions-operations",slug:"distributions-operations"},{title:"Visualizing random vs grid search",date:"2021-12-22T00:00:00.000Z",summary:"Random search usually works better than grid search\n  for hyperparameter optimization. This brief post suggests a way\n  to visualize the reason for this geometrically.\n  ",tags:["Machine learning"],path:"/post/random-vs-grid-search",slug:"random-vs-grid-search"},{title:"Extensions of Karger's algorithm",date:"2021-09-10T09:41:00.000Z",summary:"\u003Cscript>\n  import Spoiler from '$lib/components/Spoiler.svelte';\n  import Figure from '$lib/components/Figure.svelte';\n\u003C/script>\n\n\n_If you prefer videos, check out our [ICCV presentation](https://www.youtube.com/watch?v=1sURbgamyvU), which covers similar content as this blog post.\nFor more details, see [our paper](http://arxiv.org/abs/2110.02750)._...",tags:[],path:"/post/karger-extensions",slug:"karger-extensions"},{title:"Distributions Part I: the Delta distribution",date:"2021-07-06T12:15:00.000Z",summary:"  Did you always want to know kind of object this weird Dirac delta \"function\"\n  actually is? Well, it's a Schwartz distribution. If that doesn't help much,\n  then keep reading.\n  ",tags:["Math"],path:"/post/distributions-intro",slug:"distributions-intro"},{title:"Scripting for personal productivity",date:"2021-04-14T17:42:00.000Z",summary:"  If you can program, you can use that to support your habits and automate\n  some routines. This post gives a few examples.\n  ",tags:["Productivity"],path:"/post/automation-productivity",slug:"automation-productivity"},{title:"Troubles with the Bias-Variance tradeoff",date:"2021-04-07T16:19:00.000Z",summary:"  The bias-variance tradeoff is a key idea in machine learning. But I'll\n  argue that we know surprisingly little about it: when does it hold?\n  How does it relate to the Double Descent phenomenon? And what do we\n  even formally mean when we talk about it?\n  ",tags:["Machine learning"],path:"/post/bias-variance-tradeoff",slug:"bias-variance-tradeoff"},{title:"Collection of quick computer tips",date:"2021-03-31T12:49:00.000Z",summary:"  Many of us spend a lot of time working with our computer, so it's worth\n  spending some time to make that experience as pleasent and productive\n  as possible. This is a collection of tips that are relatively quick\n  to implement and still very valuable in the long run in my opinion.\n  Mainly geared towards developers and others who work with the shell\n  a lot.\n  ",tags:["Productivity"],path:"/post/computer-tips",slug:"computer-tips"},{title:"State formally, reason informally",date:"2021-03-24T10:00:00.000Z",summary:"  There's a style of teaching mathematics that I really like: stating definitions\n  and theorems as formally as in any textbook, but focusing on informal arguments\n  for why they should be true.\n  ",tags:["Math"],path:"/post/state-formally-reason-informally",slug:"state-formally-reason-informally"},{title:"Emacs as an amazing LaTeX editor",date:"2021-03-17T13:14:00.000Z",summary:"  Emacs has some really amazing features for writing LaTeX; this post gives\n  an overview of some of them, either to convince you to give Emacs a try,\n  or to make you aware that these features exist if you're already using\n  Emacs but didn't know about them.\n  ",tags:["Productivity"],path:"/post/latex-emacs",slug:"latex-emacs"},{title:"Perspectives on spherical harmonics",date:"2021-03-10T16:07:00.000Z",summary:"  Spherical harmonics are ubiquitous in math and physics, in part because\n  they naturally appear as solutions to several problems; in particular they\n  are the eigenfunctions of the spherical Laplacian and the irreducible\n  representations of SO(3). But why should the solutions to these problems\n  be the same? And why are they called spherical harmonics?\n  ",tags:["Math"],path:"/post/spherical-harmonics",slug:"spherical-harmonics"},{title:"Deep Implicit layers",date:"2021-03-03T13:48:00.000Z",summary:"  Several new architectures for neural networks, such as Neural ODEs and\n  deep equlibirum models can be understood as replacing classical layers\n  that explicitly specify how to compute the output with implicit layers.\n  These layers describe which conditions the output should specify but\n  leave the actual computation up to some solver that can be chosen arbitrarily.\n  This post contains a brief introduction to the main ideas behind implicit layers.\n  ",tags:["Deep learning"],path:"/post/implicit-layers",slug:"implicit-layers"},{title:"Building Blocks of RL Part III: Model-based RL",date:"2021-02-24T09:41:00.000Z",summary:"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is\n  the conclusion of a series with such an overview, covering model-based RL.\n  ",tags:["Reinforcement learning"],path:"/post/rl-building-blocks-3",slug:"rl-building-blocks-3"},{title:"L1 regularization: sparsity through singularities",date:"2021-02-17T08:33:00.000Z",summary:"  L1 regularization is famous for leading to sparse optima, in contrast to\n  L2 regularization. There are several ways of understanding this but I'll\n  argue that it's really all about one fact: the L1 norm has a singularity\n  at the origin, while the L2 norm does not. And this is not just true\n  for L1 and L2 regularization: singularities are always necessary to get sparse weights.\n  ",tags:["Machine learning"],path:"/post/sparsity-singularities",slug:"sparsity-singularities"},{title:"Boring numbers, complexity and Chaitin's incompleteness theorem",date:"2021-02-10T15:27:00.000Z",summary:"  There is a \"complexity barrier\": a number such that we can't prove\n  the Kolmogorov complexity of any specific string to be larger than\n  that. The proof of this astonishing fact is closely related to some\n  famous paradoxa and we'll use this connection to get a better intuition\n  for why the complexity barrier exists.\n  ",tags:[],path:"/post/boring-numbers",slug:"boring-numbers"},{title:"Building Blocks of RL Part II: Policy Optimization",date:"2021-02-03T06:39:00.000Z",summary:"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 2\n  of a series with such an overview, covering some policy optimization methods.\n  ",tags:["Reinforcement learning"],path:"/post/rl-building-blocks-2",slug:"rl-building-blocks-2"},{title:"Too much structure",date:"2021-01-27T07:53:00.000Z",summary:"  Proving things for object that have a lot of structure can be harder\n  than for object with less structure, simply because the tree of possible\n  proofs is much wider. This is probably why trying to prove a more general\n  case is sometimes a helpful strategy.\n  ",tags:["Structure","Math"],path:"/post/too-much-structure",slug:"too-much-structure"},{title:"Asymmetry between position and momentum in physics",date:"2021-01-19T09:52:00.000Z",summary:"  In both classical mechanics and QM, there are transformations between position-based\n  and momentum-based representations that preserve the dynamical laws. So from\n  a mathematical perspective, position and momentum seem to play equivalent roles\n  in physics. But they don't play equivalent roles in our cognition, which is part of\n  the physical universe -- seemingly a paradox.\n  ",tags:["Physics"],path:"/post/position-momentum-asymmetry",slug:"position-momentum-asymmetry"},{title:"Building Blocks of RL Part I: Value-based methods",date:"2021-01-13T15:58:00.000Z",summary:"  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 1\n  of a series with such an overview, covering value-based methods (mainly in a tabular\n  setting).\n  ",tags:["Reinforcement learning"],path:"/post/rl-building-blocks-1",slug:"rl-building-blocks-1"},{title:"VAEs from a generative perspective",date:"2021-01-06T13:45:00.000Z",summary:"  Variational autoencoders are usually introduced as a probabilistic extension of autoencoders\n  with regularization. An alternative view is that the encoder arises naturally as a tool\n  for efficiently training the decoder. This is the perspective I take in this post, deriving\n  VAEs without assuming an autoencoder architecture a priori.\n  ",tags:["Deep learning"],path:"/post/vae-generative",slug:"vae-generative"},{title:"Ways to think about structure in mathematics",date:"2020-12-29T13:03:00.000Z",summary:"  \"Structure\" is a concept that keeps popping up when thinking about mathematics\n  but it's hard to pin down what it is exactly. I discuss several different perspectives\n  for thinking about it.\n  ",tags:["Structure","Math"],path:"/post/perspectives-on-structure",slug:"perspectives-on-structure"},{title:"Trading off speed against the probability of success in the Karger-Stein Algorithm",date:"2020-12-06T17:00:00.000Z",summary:"  The Karger-Stein algorithm is an improvement over Karger's beautiful contraction\n  algorithm for minimum graph cuts. In this post, I show how it finds the perfect\n  tradeoff between finding a mincut with high probability and finding it quickly.\n  In the course of doing so, we will also understand where the somewhat opaque\n  factor of sqrt(2) comes from.\n  ",tags:["Graphs"],path:"/post/karger-stein",slug:"karger-stein"},{title:"Discounting in a relativistic universe",date:"2020-06-20T10:25:00.000Z",summary:"  For people who want to discount the future, special relativity creates\n  some challenges. There are different ways to handle those but none\n  seem completely satisfactory which may be yet another argument against\n  discounting pure utilities.\n  ",tags:["Physics"],path:"/post/discounting-relativistic-universe",slug:"discounting-relativistic-universe"}]},"uses":{}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>