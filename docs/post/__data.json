{"type":"data","nodes":[null,{"type":"data","data":[{"posts":1},[2,9,16,24,32,39,46,54,61,68,75,82,89,97,105,112,119,126,134,142,149,156,163,171],{"title":3,"date":4,"summary":5,"tags":6,"path":7,"slug":8},"Swoosh: Rethinking Activation Functions","2023-04-01T00:00:00.000Z","Introducing the new Swoosh activation function. Perfect test set generalization\nguaranteed.\n  ",[],"/post/swoosh","swoosh",{"title":10,"date":11,"summary":12,"tags":13,"path":14,"slug":15},"Einsum is easy and useful","2022-11-05T00:00:00.000Z","`einsum` is one of the most useful functions in Numpy/Pytorch/Tensorflow and yet many people don't use it. It seems to have a reputation as being difficult to understand and use, which is completely backwards in my view: the reason `einsum` is great is precisely because it is *easier* to use and reason about than the alternatives. So this post tries to set the record straight and show how simple `einsum` really is.\n  ",[],"/post/einsum","einsum",{"title":17,"date":18,"summary":19,"tags":20,"path":22,"slug":23},"Distributions Part II: What can we do with distributions?","2022-03-05T23:00:00.000Z","  As promised in part I, we can do a lot of the same things with Schwartz\n  distributions as with classical functions. To see how, we'll cover\n  derivatives, convolutions, and Fourier transforms of distributions.\n  ",[21],"Math","/post/distributions-operations","distributions-operations",{"title":25,"date":26,"summary":27,"tags":28,"path":30,"slug":31},"Visualizing random vs grid search","2021-12-22T00:00:00.000Z","Random search usually works better than grid search\n  for hyperparameter optimization. This brief post suggests a way\n  to visualize the reason for this geometrically.\n  ",[29],"Machine learning","/post/random-vs-grid-search","random-vs-grid-search",{"title":33,"date":34,"summary":35,"tags":36,"path":37,"slug":38},"Extensions of Karger's algorithm","2021-09-10T09:41:00.000Z","\u003Cscript>\n  import Spoiler from '$lib/components/Spoiler.svelte';\n  import Figure from '$lib/components/Figure.svelte';\n\u003C/script>\n\n\n_If you prefer videos, check out our [ICCV presentation](https://www.youtube.com/watch?v=1sURbgamyvU), which covers similar content as this blog post.\nFor more details, see [our paper](http://arxiv.org/abs/2110.02750)._...",[],"/post/karger-extensions","karger-extensions",{"title":40,"date":41,"summary":42,"tags":43,"path":44,"slug":45},"Distributions Part I: the Delta distribution","2021-07-06T12:15:00.000Z","  Did you always want to know kind of object this weird Dirac delta \"function\"\n  actually is? Well, it's a Schwartz distribution. If that doesn't help much,\n  then keep reading.\n  ",[21],"/post/distributions-intro","distributions-intro",{"title":47,"date":48,"summary":49,"tags":50,"path":52,"slug":53},"Scripting for personal productivity","2021-04-14T17:42:00.000Z","  If you can program, you can use that to support your habits and automate\n  some routines. This post gives a few examples.\n  ",[51],"Productivity","/post/automation-productivity","automation-productivity",{"title":55,"date":56,"summary":57,"tags":58,"path":59,"slug":60},"Troubles with the Bias-Variance tradeoff","2021-04-07T16:19:00.000Z","  The bias-variance tradeoff is a key idea in machine learning. But I'll\n  argue that we know surprisingly little about it: when does it hold?\n  How does it relate to the Double Descent phenomenon? And what do we\n  even formally mean when we talk about it?\n  ",[29],"/post/bias-variance-tradeoff","bias-variance-tradeoff",{"title":62,"date":63,"summary":64,"tags":65,"path":66,"slug":67},"Collection of quick computer tips","2021-03-31T12:49:00.000Z","  Many of us spend a lot of time working with our computer, so it's worth\n  spending some time to make that experience as pleasent and productive\n  as possible. This is a collection of tips that are relatively quick\n  to implement and still very valuable in the long run in my opinion.\n  Mainly geared towards developers and others who work with the shell\n  a lot.\n  ",[51],"/post/computer-tips","computer-tips",{"title":69,"date":70,"summary":71,"tags":72,"path":73,"slug":74},"State formally, reason informally","2021-03-24T10:00:00.000Z","  There's a style of teaching mathematics that I really like: stating definitions\n  and theorems as formally as in any textbook, but focusing on informal arguments\n  for why they should be true.\n  ",[21],"/post/state-formally-reason-informally","state-formally-reason-informally",{"title":76,"date":77,"summary":78,"tags":79,"path":80,"slug":81},"Emacs as an amazing LaTeX editor","2021-03-17T13:14:00.000Z","  Emacs has some really amazing features for writing LaTeX; this post gives\n  an overview of some of them, either to convince you to give Emacs a try,\n  or to make you aware that these features exist if you're already using\n  Emacs but didn't know about them.\n  ",[51],"/post/latex-emacs","latex-emacs",{"title":83,"date":84,"summary":85,"tags":86,"path":87,"slug":88},"Perspectives on spherical harmonics","2021-03-10T16:07:00.000Z","  Spherical harmonics are ubiquitous in math and physics, in part because\n  they naturally appear as solutions to several problems; in particular they\n  are the eigenfunctions of the spherical Laplacian and the irreducible\n  representations of SO(3). But why should the solutions to these problems\n  be the same? And why are they called spherical harmonics?\n  ",[21],"/post/spherical-harmonics","spherical-harmonics",{"title":90,"date":91,"summary":92,"tags":93,"path":95,"slug":96},"Deep Implicit layers","2021-03-03T13:48:00.000Z","  Several new architectures for neural networks, such as Neural ODEs and\n  deep equlibirum models can be understood as replacing classical layers\n  that explicitly specify how to compute the output with implicit layers.\n  These layers describe which conditions the output should specify but\n  leave the actual computation up to some solver that can be chosen arbitrarily.\n  This post contains a brief introduction to the main ideas behind implicit layers.\n  ",[94],"Deep learning","/post/implicit-layers","implicit-layers",{"title":98,"date":99,"summary":100,"tags":101,"path":103,"slug":104},"Building Blocks of RL Part III: Model-based RL","2021-02-24T09:41:00.000Z","  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is\n  the conclusion of a series with such an overview, covering model-based RL.\n  ",[102],"Reinforcement learning","/post/rl-building-blocks-3","rl-building-blocks-3",{"title":106,"date":107,"summary":108,"tags":109,"path":110,"slug":111},"L1 regularization: sparsity through singularities","2021-02-17T08:33:00.000Z","  L1 regularization is famous for leading to sparse optima, in contrast to\n  L2 regularization. There are several ways of understanding this but I'll\n  argue that it's really all about one fact: the L1 norm has a singularity\n  at the origin, while the L2 norm does not. And this is not just true\n  for L1 and L2 regularization: singularities are always necessary to get sparse weights.\n  ",[29],"/post/sparsity-singularities","sparsity-singularities",{"title":113,"date":114,"summary":115,"tags":116,"path":117,"slug":118},"Boring numbers, complexity and Chaitin's incompleteness theorem","2021-02-10T15:27:00.000Z","  There is a \"complexity barrier\": a number such that we can't prove\n  the Kolmogorov complexity of any specific string to be larger than\n  that. The proof of this astonishing fact is closely related to some\n  famous paradoxa and we'll use this connection to get a better intuition\n  for why the complexity barrier exists.\n  ",[],"/post/boring-numbers","boring-numbers",{"title":120,"date":121,"summary":122,"tags":123,"path":124,"slug":125},"Building Blocks of RL Part II: Policy Optimization","2021-02-03T06:39:00.000Z","  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 2\n  of a series with such an overview, covering some policy optimization methods.\n  ",[102],"/post/rl-building-blocks-2","rl-building-blocks-2",{"title":127,"date":128,"summary":129,"tags":130,"path":132,"slug":133},"Too much structure","2021-01-27T07:53:00.000Z","  Proving things for object that have a lot of structure can be harder\n  than for object with less structure, simply because the tree of possible\n  proofs is much wider. This is probably why trying to prove a more general\n  case is sometimes a helpful strategy.\n  ",[131,21],"Structure","/post/too-much-structure","too-much-structure",{"title":135,"date":136,"summary":137,"tags":138,"path":140,"slug":141},"Asymmetry between position and momentum in physics","2021-01-19T09:52:00.000Z","  In both classical mechanics and QM, there are transformations between position-based\n  and momentum-based representations that preserve the dynamical laws. So from\n  a mathematical perspective, position and momentum seem to play equivalent roles\n  in physics. But they don't play equivalent roles in our cognition, which is part of\n  the physical universe -- seemingly a paradox.\n  ",[139],"Physics","/post/position-momentum-asymmetry","position-momentum-asymmetry",{"title":143,"date":144,"summary":145,"tags":146,"path":147,"slug":148},"Building Blocks of RL Part I: Value-based methods","2021-01-13T15:58:00.000Z","  Reinforcement Learning consists of a few key building blocks that can be combined to create\n  many of the well-known algorithms. Framing RL in terms of these building blocks\n  can give a good overview and better understanding of these algorithms. This is part 1\n  of a series with such an overview, covering value-based methods (mainly in a tabular\n  setting).\n  ",[102],"/post/rl-building-blocks-1","rl-building-blocks-1",{"title":150,"date":151,"summary":152,"tags":153,"path":154,"slug":155},"VAEs from a generative perspective","2021-01-06T13:45:00.000Z","  Variational autoencoders are usually introduced as a probabilistic extension of autoencoders\n  with regularization. An alternative view is that the encoder arises naturally as a tool\n  for efficiently training the decoder. This is the perspective I take in this post, deriving\n  VAEs without assuming an autoencoder architecture a priori.\n  ",[94],"/post/vae-generative","vae-generative",{"title":157,"date":158,"summary":159,"tags":160,"path":161,"slug":162},"Ways to think about structure in mathematics","2020-12-29T13:03:00.000Z","  \"Structure\" is a concept that keeps popping up when thinking about mathematics\n  but it's hard to pin down what it is exactly. I discuss several different perspectives\n  for thinking about it.\n  ",[131,21],"/post/perspectives-on-structure","perspectives-on-structure",{"title":164,"date":165,"summary":166,"tags":167,"path":169,"slug":170},"Trading off speed against the probability of success in the Karger-Stein Algorithm","2020-12-06T17:00:00.000Z","  The Karger-Stein algorithm is an improvement over Karger's beautiful contraction\n  algorithm for minimum graph cuts. In this post, I show how it finds the perfect\n  tradeoff between finding a mincut with high probability and finding it quickly.\n  In the course of doing so, we will also understand where the somewhat opaque\n  factor of sqrt(2) comes from.\n  ",[168],"Graphs","/post/karger-stein","karger-stein",{"title":172,"date":173,"summary":174,"tags":175,"path":176,"slug":177},"Discounting in a relativistic universe","2020-06-20T10:25:00.000Z","  For people who want to discount the future, special relativity creates\n  some challenges. There are different ways to handle those but none\n  seem completely satisfactory which may be yet another argument against\n  discounting pure utilities.\n  ",[139],"/post/discounting-relativistic-universe","discounting-relativistic-universe"],"uses":{}}]}
