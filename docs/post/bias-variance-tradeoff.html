<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="icon" href="../favicon.png" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	
		<link href="../_app/immutable/assets/0.BV6eIN1u.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.D3urAubC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BZIfHQ7j.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CXEaPN9q.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BH9Iu36z.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/IITJ3E17.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.Dlb8Ldyb.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DgNpRiRI.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/ZND8Q6sY.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/lNFSlT5-.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CIAIJGM7.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.DUYzCeOk.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BpsNOneS.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.DzdLjZ1b.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D1wvszJw.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CkMDHBy_.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/dy7G579c.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BInqBSHA.js"><!--[--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="" crossorigin="anonymous"><!--]--><!--[--><!--[--><meta name="description" content="  The bias-variance tradeoff is a key idea in machine learning. But I'll
  argue that we know surprisingly little about it: when does it hold?
  How does it relate to the Double Descent phenomenon? And what do we
  even formally mean when we talk about it?
  "><!--]--><!--]--><title>Troubles with the Bias-Variance tradeoff</title>
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents"><!--[--><!--[--><!----><header class="sticky top-0 z-10 w-full bg-white/90 shadow-sm backdrop-blur-sm transition-transform duration-300 ease-in-out relative // Apply fixed/translate classes based on state"><nav class="mx-auto flex max-w-3xl items-center p-4"><div class="space-x-4"><a href="/" class="text-gray-600 hover:text-blue-600">Home</a> <a href="/post" class="text-gray-600 hover:text-blue-600">Posts</a></div></nav></header> <main class="mx-auto max-w-3xl p-4"><!----><article class="mx-auto max-w-3xl px-4 py-8"><header class="mb-8"><h1 class="mb-4 border-b pb-4 text-4xl leading-tight font-bold text-gray-900">Troubles with the Bias-Variance tradeoff</h1> <!--[--><div class="mt-2 flex items-center gap-4 text-sm text-gray-600"><time datetime="2021-04-07T16:19:00.000Z">April 7, 2021</time> <!--[!--><!--]--></div><!--]--> <!--[--><div class="mt-4 flex flex-wrap gap-2"><!--[--><span class="rounded-full bg-blue-100 px-3 py-1 text-xs font-medium text-blue-800">Machine learning</span><!--]--></div><!--]--></header> <!--[!--><!--]--> <!--[!--><!--]--> <div class="prose prose-lg prose-h1:text-4xl prose-h2:text-3xl prose-h3:text-2xl max-w-none"><!----><p><em>(Last updated: 2021-04-22)</em></p> <p>Arguably one of the most important concepts in machine learning, taught in
any introductory course, is under- and overfitting. The story goes like this:
if your model is too simplistic, you won’t be able to fit the data
well and get a large error, you <em>underfit</em>. On the other hand, if your model is too complex,
it will fit any noise that is present in the data, i.e. you <em>overfit</em>.
Such a model won’t generalize to the test set, so you also get a large error.
Somewhere in between those two is a sweet spot with minimal test error.</p> <p>The choice of words, under- and overfitting, already implies that we believe
a tradeoff exists: they are two ends of a scale, and we need to find the point
in the middle where we’re neither under- nor overfitting too much.</p> <p>Under- and overfitting can be formalized using the notions of bias and variance
(there’ll be a short recap in the next section). Underfitting means that we have
a high test error because of high bias, while overfitting means that high variance
causes a high error. Phrased in those terms, the tradeoff between under- and overfitting
becomes the <em>bias-variance tradeoff</em>: methods with low bias tend to have high variance
and vice versa.</p> <p>This idea is ubiquitous in machine learning. So when I recently wanted to look
up some details, I expected to find troves of information. Tons of empirical evidence,
a formal definition of what exactly we mean by “tradeoff”, and hopefully even
theorems showing that this tradeoff exists. As you can tell from the setup,
that’s not what happened. So in this post, I’m going to describe what I found
(and what I did not find). My goal is to clear up some potential misconceptions,
and hopefully convince you that the bias-variance tradeoff is less simple and more
interesting than you thought.</p> <h2>Primer: Bias and variance</h2> <p>Under- and overfitting can be explained in terms of bias and variance.
I’m going to discuss everything in a supervised learning setting. So the setup
is the following:</p> <ul><li>There is a true (unknown) function <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><!----></span>, which we want to approximate</li> <li>We have a dataset <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">D = \{(x_1, y_1), \ldots, (x_n, y_n)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)}</span></span></span></span><!----></span> of datapoints
that we use to learn an approximation, where <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_i = f(x_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><!----></span></li> <li>We have some training process that takes in this dataset <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span><!----></span> and produces
a function <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{f}(x; D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><!----></span> that approximates <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><!----></span></li> <li>Finally, we imagine there is some distribution over datasets <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span><!----></span>. This
distribution is unknown an it’s a somewhat elusive concept, but think of it
like this: we created our dataset with some process, e.g. by taking lots of
photos and then having people label them. The distribution over datasets
describes how likely this process is to produce any particular dataset.</li></ul> <p>Now we can define precisely what we mean by bias and variance:</p> <ul><li>The bias is the difference between the expected value of <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{f}(x; D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><!----></span> and the true value <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><!----></span>, i.e. <div class="math math-display"><!----><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="double-struck">E</mi><mi>D</mi></msub><mo stretchy="false">[</mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_D[\hat{f}(x; D)] - f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span><!----></div></li> <li>By “variance” we mean the variance of <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{f}(x; D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><!----></span> with respect to <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span><!----></span>, i.e. <div class="math math-display"><!----><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="double-struck">E</mi><mi>D</mi></msub><msup><mrow><mo fence="true">(</mo><msub><mi mathvariant="double-struck">E</mi><mi>D</mi></msub><mo stretchy="false">[</mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>−</mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathbb{E}_D \left(\mathbb{E}_D[\hat{f}(x; D)] - \hat{f}(x; D)\right)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.004em;vertical-align:-0.65em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.354em;"><span style="top:-3.6029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><!----></div></li></ul> <p>Ideally, we want both bias and variance to be small. The reason is the <em>bias-variance decomposition</em> of the expected squared error<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>: <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="double-struck">E</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><msup><mtext>bias</mtext><mn>2</mn></msup><mo>+</mo><mtext>variance</mtext></mrow><annotation encoding="application/x-tex">\mathbb{E}_D (\hat{f}(x; D) - f(x))^2 = \text{bias}^2 + \text{variance}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9818em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord text"><span class="mord">bias</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8984em;"><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6679em;"></span><span class="mord text"><span class="mord">variance</span></span></span></span></span><!----></span> Assuming we ultimately want to minimize this expected squared error, it’s clear
that all else being equal, we prefer methods with low bias and variance.</p> <p>Ok, now what about the tradeoff between bias and variance? The idea is that methods
with low bias tend to have high variance, and those with low variance
tend to have high bias. So we can’t get both low bias and low variance, instead we
need to find a tradeoff between the two, such that the expected squared error is minimized.
This is illustrated by the following figure (from <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="nofollow">this essay</a>, which I recommend if you want
to gain more intuition about bias, variance, and their tradeoff): <img src="/_app/immutable/assets/tradeoff.BK1_s3iu.png" alt="Bias-variance tradeoff curve showing U-shaped risk"></p> <p>If we use an overly simplistic method, we have a high bias because
our model just can’t get close to the true function <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><!----></span>, no matter what we feed
in as training data. With a more complex model, we can fit the true function better,
but the trained model <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{f}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1523em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span><!----></span> also depends a lot more on the training data <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span><!----></span>,
so the variance increases.</p> <p>This explanation sounds sort of intuitive, but I find it a bit unsatisfying. Why exactly
do complex models vary more depending on the training data? When does this hold? Can I have
a theorem, please? And what is this “model complexity” anyways?</p> <h2>A non-answer</h2> <p>Unfortunately, the bias-variance <em>decomposition</em> and the bias-variance <em>tradeoff</em> are often conflated somewhat,
so let’s get one thing out of the way: the bias-variance decomposition is not
an explanation for the tradeoff (even though some handy-wavy explanations
suggest this with varying degrees of explicitness). The only way this could
work is if the total error was constant; this would indeed imply a tradeoff between
bias and variance. But it clearly isn’t constant (neither in the figure
above, nor in practice). If the total error <em>was</em> constant,
then we wouldn’t care about the tradeoff between bias and variance: it wouldn’t
matter which model we chose.</p> <p>Instead, the bias-variance decomposition just motivates why we care about bias
and variance at all. So it explains why the bias-variance tradeoff is important, but it can’t
explain why it’s a thing in the first place.</p> <h2>Evidence for a tradeoff</h2> <p>Why do we believe in a tradeoff between bias and variance? Well, I mainly do because
others have told me about it and it seems pretty intuitive. But those aren’t very good
reasons, so what to we have in terms of hard evidence?</p> <p>As an example, let’s look at what <em>Elements of Statistical Learning</em> by Hastie et al.
has to say. It considers two cases in Section 7.3: k-NN regression and linear regression.
For k-NN, it gives a theoretical expression for bias and variance, and at least the variance
does indeed increase with increasing model complexity (i.e. decreasing <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span><!----></span>).
A caveat is that the formula assumes the input points <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><!----></span> are fixed and
only the targets <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><!----></span> vary — a very strong and usually unrealistic assumption.
Similarly, there are theoretical expressions in the case of ridge regression,
and based on those, variance should typically decrease and bias increase with
stronger regularization. In the same section, there are small toy experiments
that demonstrate a tradeoff empirically for k-NN and for linear models.</p> <p>This seems to be fairly representative of the kind of evidence we currently
have in favor of a tradeoff. If you are curious you should look at Section 3.4 in <a href="https://arxiv.org/pdf/1912.08286.pdf" rel="nofollow">this thesis</a>, which is more comprehensive than I’m going to be in a blog post.
But in brief, there is empirical evidence for methods such as decision trees,
fitting polynomials, or kernel regression. Then there are some general theoretical
results that are weak evidence for a tradeoff if you squint a bit. And that’s pretty
much it.</p> <p>If you are one of the two or three people working with neural networks,
this doesn’t really inspire confidence. After all, the fact that something holds for linear
regression and polynomials isn’t very strong evidence that it’s also true
for a 25 million parameter ResNet. Which brings us to the question: <em>does</em> the bias-variance
tradeoff exist outside the simple methods named above?</p> <h2>Double Descent</h2> <p>Over the last few years, the bias-variance tradeoff has ben supplemented by
a more complicated narrative, dubbed “Double descent” by <a href="https://arxiv.org/abs/1812.11118" rel="nofollow">Belkin et al.</a> in 2018.
The following figure illustrates this concept: <img src="/_app/immutable/assets/double_descent.dtUoBZ4j.png" alt="Double descent curve showing second descent in risk after interpolation threshold"></p> <p>On the left, we have the classical U-shaped curve for the squared error already shown above.
But the claim is that if you increase the model complexity past the interpolation
threshold, where the model can perfectly fit the training data, then the error
decreases again.</p> <p>There is quite a bit of empirical evidence for this double descent curve,
especially in the context of <a href="https://arxiv.org/abs/1912.02292" rel="nofollow">neural networks</a><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>. The reason seems to be
that the variance curve <a href="https://arxiv.org/abs/2002.11328" rel="nofollow">is</a> <a href="https://arxiv.org/abs/1810.08591" rel="nofollow">unimodal</a>, i.e. it first increases with model
complexity, as we’d classicaly suspect, but then decreases again. Of course
that just passes the buck: why does the variance decrease again after the
interpolation threshold?</p> <p>I think that this is an extremely interesting question, but in this post
I’m not going to speculate on it. Instead, I’m still interested in the bias-variance
tradeoff in the classical regime: as long as the model complexity is below the interpolation
threshold, does a tradeoff always exist? If so, why? If not, when does it exist?
Understanding this question better should also help understanding the behavior
beyond the interpolation threshold: if we really understood why the tradeoff exists
at all, we would also know how it could potentially be violated.</p> <h2>General theorems are hard</h2> <p>In an ideal world, we could give a formal statement of the bias-variance tradeoff
and then prove that it occurs under some fairly general circumstances.
Maybe that is indeed possible, but it’s easy to rule out the most general kinds
of theorems through a few counterexamples.</p> <p>The strongest form of bias-variance tradeoff would be the claim
“any decrease in variance leads to an increase in bias” (and vice versa).
This is clearly false. For example, let’s say we start with a traing procedure that chooses <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">sha1</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{f}(x; D) = \operatorname{sha1}(D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">sha1</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><!----></span>, i.e. which uses a hash of (some serialization of)
the training data for its prediction (there’s nothing special about this example,
just think “a really ridiculous training procedure”).
This procedure has high variance, but also high bias. Switching
to any reasonable training procedure will decrease both considerably.</p> <p>But let’s consider a more reasonable statement of the bias-variance-tradeoff:
assume we have a class of models, and the training procedure picks the model from that
class that has the lowest training error. We now want to know how the choice of
model class influences bias and variance. Furthermore, we only consider a nested
set of model classes. So we have an ordering of model classes from simple to complex,
where successively more complex classes contain the simpler ones.</p> <p>Note that this is a rather typical example of what we mean when
we talk about “model complexity” and the bias-variance tradeoff in practical
contexts. For example, a wider neural network can always instantiate any function
that a more narrow one can, by having some weights be zero.</p> <p>In this setting, the bias-variance tradeoff can be formulated as two separate claims:
the bias decreases with increasing complexity, while the variance increases.
Unfortunately, neither one is true in general.</p> <p>A case where bias increases is easy to construct: Let’s say the inputs <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span><!----></span> and
the targets <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><!----></span> are both real numbers, and there is some noise, i.e. <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">y = f(x) + \varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ε</span></span></span></span><!----></span>,
with <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">E</mi><mi>ε</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbb{E} \varepsilon = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">E</span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span><!----></span>. One very simple model class is <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>f</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{f\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mclose">}</span></span></span></span><!----></span>, and
this class of course leads to a bias of zero. Then there is a more complex model
class <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>f</mi><mo separator="true">,</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{f, f + 1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span><!----></span>, where we have added a model that always predicts one more
than <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><!----></span>. Because of the noise, we might get unlucky with the training data and
pick this second model. So this more complex model class has non-zero bias.</p> <p>A slight modification leads to an example where variance decreases with model
complexity: we take <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>f</mi><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{f - 1, f + 1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span><!----></span> as the simple model class and <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>f</mi><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{f - 1, f, f + 1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span><!----></span> as a larger class. Assuming that we have a very large amount of training data,
we will almost always pick <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><!----></span> in the second case, so the variance will be very
low. In contrast, in the first case, we pick each of <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">f - 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">f + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span><!----></span> half the
time, so we have a constant variance, no matter how much training data we have.</p> <p>Note that this decrease in variance
happens in the classical regime, in the sense that in either case, there is
at most one model that fits the data perfectly. So we haven’t crossed the
interpolation threshold yet, and still the variance went down when we increased
model complexity.</p> <p>Obviously, these examples are somewhat silly, and certainly not representative
of real-world scenarios. They do not constitute a good argument that there is no
bias-variance tradeoff in practice, but they do put some limits on what kinds of
bias-variance tradeoffs we can <em>prove</em>. So we’ll have to put in a bit of work into
finding the right formalization of what we mean by “bias-variance tradeoff”, and
in particular when this tradeoff is supposed to hold. As far as I’m aware, such
a formalization does not exist yet.</p> <h2>Model complexity</h2> <p>Formalizing the bias-variance tradeoff probably also requires formalizing the
notion of “model complexity” to some extent. This is a somewhat elusive
concept, with many different aspects of the training procedure falling under
its umbrella. The prototypical example of model complexity is the size of the
class of models we use. For example, increasing the width of a neural network,
or using a larger basis of features for linear regression, both increase
the size of the model class under consideration.</p> <p>But the size of the model class is not the entire story. For example,
regularization terms in the loss function don’t change the model class but instead affect which model
is selected from that class. The same is true for the number of trainings steps,
or more generally the optimization procedure.</p> <p><a href="https://arxiv.org/abs/1912.02292" rel="nofollow">Nakkiran et al.</a> define the <em>effective model complexity</em> of a training procedure as the maximum number of training points for which the
expected training error remains below some threshold <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi></mrow><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ε</span></span></span></span><!----></span>.
That definition captures a lot of my intuition about what we mean by “complexity”
in the context of the bias-variance tradeoff. On the other hand, it seems slightly
ad-hoc and I’m still hoping for an even better notion of model complexity.</p> <p>One thing that comes to mind when talking about “complexity” is of course Kolmogorov
complexity. But note that
we don’t want the (expected) Kolmogorov complexity of the learned model.
For example, a randomly initialized neural network has high Kolmogorov complexity,
but its model complexity should be zero (it’s all the way at
“high bias, low variance<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>”). What we might be able
to use instead is the algorithmic mutual information between the learned
model and the dataset. Another approach would be the (information-theoretic)
mutual information of dataset and model with respect to the distribution over datasets.
Both of these would probably be hard to estimate in practice (and I haven’t
thought very long about whether they make sense theoretically and intuitively).
But they illustrate the type of more fundamental definition that I’m hoping exists.</p> <p>Whether a good definition of model complexity would lead to a formalization
of the bias-variance tradeoff is not clear. For example, just
saying “bias/variance decreases/increases with increasing effective model
complexity” doesn’t work (the examples from the previous sections are still
counterexamples). But at least I suspect that having the right notion
of model comlexity is <em>necessary</em> for finding a general formal statement
about the bias-variance tradeoff, even if we then need additional conditions,
under which it holds (for example that it is only true in the classical regime,
i.e. when the model isn’t overparameterized).</p> <h2>Conclusion</h2> <p>After putting the bias-variance tradeoff under a lot of scrutiny, I also
want to emphasize the other side of the coin: it’s clear that the bias-variance tradeoff
is real, that it appears in many different settings, and that it’s extremely important
to keep in mind when doing machine learning.</p> <p>But I do think that we don’t understand it nearly as well as we could, and that
the way the tradeoff is often presented doesn’t do a good job of hightlighting
the parts we don’t understand. I highly recommend <a href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update" rel="nofollow">this essay</a>, which makes a similar
point more explicitly.</p> <div class="footnotes"><hr> <ol><li id="fn-1">I’m ignoring label noise here. More generally, if the labels are sampled from <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">y = f(x) + \varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ε</span></span></span></span><!----></span> with <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">E</mi><mo stretchy="false">[</mo><mi>ε</mi><mo stretchy="false">]</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbb{E}[\varepsilon] = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">E</span><span class="mopen">[</span><span class="mord mathnormal">ε</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span><!----></span>, there would be an additional <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ε</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\varepsilon^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">ε</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><!----></span>-term, the <em>irreducible error</em>.<a href="#fnref-1" class="footnote-backref">↩</a></li> <li id="fn-2">Though just to make things even weirder: this only seems to occur when increasing the <em>width</em> of networks. Increasing the depth often isn’t tested in papers on the topic, and when it is, there’s no double descent curve.<a href="#fnref-2" class="footnote-backref">↩</a></li> <li id="fn-3">To forestall potential confusion: the variance <em>with respect to the training data</em> is low. Of course there is high variance with respect to the random initialization itself, but that’s not what we’re interested in.<a href="#fnref-3" class="footnote-backref">↩</a></li></ol></div><!----></div> <div class="mt-12 border-t border-gray-200 pt-8"><div class="flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between"><a href="/post" class="inline-flex items-center text-blue-600 hover:text-blue-800"><svg xmlns="http://www.w3.org/2000/svg" class="mr-2 h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M12.707 5.293a1 1 0 010 1.414L9.414 10l3.293 3.293a1 1 0 01-1.414 1.414l-4-4a1 1 0 010-1.414l4-4a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> Back to all posts</a></div></div></article><!----><!----></main><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_jgw6mo = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.D3urAubC.js"),
						import("../_app/immutable/entry/app.Dlb8Ldyb.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>