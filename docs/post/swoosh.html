<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="icon" href="../favicon.png" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	
		<link href="../_app/immutable/assets/0.BV6eIN1u.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.BU4YA098.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CiKng2ZP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CXEaPN9q.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BH9Iu36z.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/IITJ3E17.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.B4LPIQdn.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DgNpRiRI.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/ZND8Q6sY.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/lNFSlT5-.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CIAIJGM7.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.DUYzCeOk.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BpsNOneS.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/25.BBtP74Ht.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D1wvszJw.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CkMDHBy_.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/C2OPhFAa.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/dy7G579c.js"><!--[--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="" crossorigin="anonymous"><!--]--><!--[--><!--[--><meta name="description" content="Introducing the new Swoosh activation function. Perfect test set generalization
guaranteed.
  "><!--]--><!--]--><title>Swoosh: Rethinking Activation Functions</title>
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents"><!--[--><!--[--><!----><header class="sticky top-0 z-10 w-full bg-white/90 shadow-sm backdrop-blur-sm transition-transform duration-300 ease-in-out relative // Apply fixed/translate classes based on state"><nav class="mx-auto flex max-w-3xl items-center p-4"><div class="space-x-4"><a href="/" class="text-gray-600 hover:text-blue-600">Home</a> <a href="/post" class="text-gray-600 hover:text-blue-600">Posts</a></div></nav></header> <main class="mx-auto max-w-3xl p-4"><!----><article class="mx-auto max-w-3xl px-4 py-8"><header class="mb-8"><h1 class="mb-4 border-b pb-4 text-4xl leading-tight font-bold text-gray-900">Swoosh: Rethinking Activation Functions</h1> <!--[--><div class="mt-2 flex items-center gap-4 text-sm text-gray-600"><time datetime="2023-04-01T00:00:00.000Z">April 1, 2023</time> <!--[!--><!--]--></div><!--]--> <!--[!--><!--]--></header> <!--[!--><!--]--> <!--[!--><!--]--> <div class="prose prose-lg prose-h1:text-4xl prose-h2:text-3xl prose-h3:text-2xl max-w-none"><!----><p>Improvements to the activation function in neural networks have hit diminishing returns over recent years. Whereas the choice between Sigmoids and ReLUs can make or break your training, the same cannot be said for the difference between Swish, Mish, GLish, and Phish. (I made up one of these, but I bet you don’t know which.)</p> <p>In our recent paper “Swoosh: Rethinking Activation Functions”<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>, we argue that the problem is an overly narrow conception of what an activation function can be. Our key idea is the following observation:</p> <blockquote><p>Different activation functions work best for different problems. A good activation function should thus have <em>many hyperparameters</em>.</p></blockquote> <p>Activation functions with learnable parameters have been proposed before, but they suffer from two flaws:</p> <ol><li>The number of parameters is usually quite small. For example, Swish uses only a single parameter. That’s about 175 billion fewer parameters than even a tiny model like GPT-3! And <a href="https://twitter.com/AndrewSteinwold/status/1594889562526027777" rel="nofollow">it’s well known that more parameters = more powerful</a>.</li> <li>They use <em>parameters</em>. As we will argue, using <em>hyperparameters</em> instead offers countless advantages.</li></ol> <h1>Introducing Swoosh</h1> <p>The simplest version of the Swoosh activation function is <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><mi>f</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>W</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\operatorname{Swoosh}_{f}(x) := f(Wf(x) + b),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span><!----></span> where <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><!----></span> is some simple non-linear function (such as <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x) := \max(x, 0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span><!----></span>), <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span><!----></span> is a matrix and <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span><!----></span> is a vector. The entries of <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span><!----></span> are hyperparameters of the Swoosh activation function.</p> <p>For even better results, we recommend using <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">f = \operatorname{Swoosh}_{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span><!----></span> for a simple non-linear function <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span><!----></span>, or even</p> <p><span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><mi>g</mi></msub></msub></mrow><annotation encoding="application/x-tex">f = \operatorname{Swoosh}_{\operatorname{Swoosh}_g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0418em;vertical-align:-0.3473em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop mtight"><span class="mord mathrm mtight">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3473em;"><span></span></span></span></span></span></span></span></span></span><!----></span></p> <p>and so on.
Note that in this case, the hyperparameters for each instance of Swoosh should be tuned independently.</p> <p>Astonishingly, our experiments show that just a single hidden layer is enough for most tasks when using the Swoosh activation function. In slogan form: 2-layer networks are all you need!<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup> The full network is then simply <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">net</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>U</mi><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>V</mi><mi>x</mi><mo>+</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\operatorname{net}(x) = U \operatorname{Swoosh}(Vx + a) + b.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">net</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mord">.</span></span></span></span><!----></span></p> <h1>Why hyperparameters?</h1> <p>One of our key contributions is that we treat the entries of <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span><!----></span> in Swoosh as <em>hyperparameters</em>. This is a much better choice than using them as learnable parameters, like existing activation functions typically do. We’ll briefly sketch a few of the advantages:</p> <ul><li>“Hyper” is greek for “above”, so the name should already tell us that hyperparameters are above (i.e. better than) parameters.</li> <li>Specifically for Swoosh, the network output is differentiable in these hyperparameters, which means you can still use SGD! All common opitimizers and backpropagation implementations work out of the box for optimizing Swoosh hyperparameters.</li> <li>Neural networks with higher parameter count tend to have higher memory requirements, longer training and inference times, higher compute costs, and a bigger environmental footprint. By using hyperparameters instead of parameters, we can keep the parameter count low.</li> <li>Models sometimes perform worse on the test dataset than on the training data.<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> Using the well-known technique of choosing hyperparameters based on test performance, this is an issue of the past with Swoosh.</li></ul> <h1>Conclusion and future work</h1> <p>We have shown how expanding our conception of what an activation function can be solves several key problems in deep learning, such as test set generalization and high compute requirements. We believe that this insight will not only revitalize the field of activation functions, but that it in fact heralds the end of the AI Winter we currently find ourselves in.</p> <p>To make it as easy as possible for practitioners to adopt Swoosh, we have worked closely with the PyTorch team. We are excited to announce that Swoosh is <a href="https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html" rel="nofollow">available now in the PyTorch library</a>! Tensorflow and JAX users can easily implement it in a few lines of code.</p> <p>In the future, we are planning to improve the design of Swoosh in various ways. For example, we believe it could be promising to replace the matrix multiplication <span class="math math-inline"><!----><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Wx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span></span></span></span><!----></span> with a convolution, especially on inputs with translational symmetry such as images. There is a vast unexplored space of possible Swoosh architectures to discover, and we invite other researchers to join us in this exciting journey!</p> <div class="footnotes"><hr> <ol><li id="fn-1">We will release the full paper upon acceptance of this blog post.<a href="#fnref-1" class="footnote-backref">↩</a></li> <li id="fn-2">In fact, in an upcoming paper we will show that for Swoosh activation functions, there are universal approximation theorems with bounded width and depth 1, improving on a result by <a href="https://www.semanticscholar.org/paper/Lower-bounds-for-approximation-by-MLP-neural-Maiorov-Pinkus/4bdd1f845d26e488d67c0e4549cff17407b980ad" rel="nofollow">Maiorov et al.</a><a href="#fnref-2" class="footnote-backref">↩</a></li> <li id="fn-3">Or so I’ve heard, this has never happened to me personally.<a href="#fnref-3" class="footnote-backref">↩</a></li></ol></div><!----></div> <div class="mt-12 border-t border-gray-200 pt-8"><div class="flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between"><a href="/post" class="inline-flex items-center text-blue-600 hover:text-blue-800"><svg xmlns="http://www.w3.org/2000/svg" class="mr-2 h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M12.707 5.293a1 1 0 010 1.414L9.414 10l3.293 3.293a1 1 0 01-1.414 1.414l-4-4a1 1 0 010-1.414l4-4a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> Back to all posts</a></div></div></article><!----><!----></main><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_6e0cus = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.BU4YA098.js"),
						import("../_app/immutable/entry/app.B4LPIQdn.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 25],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>