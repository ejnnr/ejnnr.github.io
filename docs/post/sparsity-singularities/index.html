<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Erik Jenner" />

  
  
  
    
  
  <meta name="description" content="  L1 regularization is famous for leading to sparse optima, in contrast to
  L2 regularization. There are several ways of understanding this but I&#39;ll
  argue that it&#39;s really all about one fact: the L1 norm has a singularity
  at the origin, while the L2 norm does not. And this is not just true
  for L1 and L2 regularization: singularities are always necessary to get sparse weights.
  " />

  
  <link rel="alternate" hreflang="en-us" href="https://ejenner.com/post/sparsity-singularities/" />

  









  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#707070" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.fd9b978ceb8f3cb70c65357f0dc17c1c.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://ejenner.com/post/sparsity-singularities/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@jenner_erik" />
    <meta property="twitter:creator" content="@jenner_erik" />
  
  <meta property="og:site_name" content="Erik Jenner" />
  <meta property="og:url" content="https://ejenner.com/post/sparsity-singularities/" />
  <meta property="og:title" content="L1 regularization: sparsity through singularities | Erik Jenner" />
  <meta property="og:description" content="  L1 regularization is famous for leading to sparse optima, in contrast to
  L2 regularization. There are several ways of understanding this but I&#39;ll
  argue that it&#39;s really all about one fact: the L1 norm has a singularity
  at the origin, while the L2 norm does not. And this is not just true
  for L1 and L2 regularization: singularities are always necessary to get sparse weights.
  " /><meta property="og:image" content="https://ejenner.com/post/sparsity-singularities/featured.png" />
    <meta property="twitter:image" content="https://ejenner.com/post/sparsity-singularities/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2021-02-17T09:33:00&#43;01:00"
      />
    
    <meta property="article:modified_time" content="2021-02-17T09:33:00&#43;01:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ejenner.com/post/sparsity-singularities/"
  },
  "headline": "L1 regularization: sparsity through singularities",
  
  "image": [
    "https://ejenner.com/post/sparsity-singularities/featured.png"
  ],
  
  "datePublished": "2021-02-17T09:33:00+01:00",
  "dateModified": "2021-02-17T09:33:00+01:00",
  
  "author": {
    "@type": "Person",
    "name": "Erik Jenner"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Erik Jenner",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ejenner.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "  L1 regularization is famous for leading to sparse optima, in contrast to\n  L2 regularization. There are several ways of understanding this but I'll\n  argue that it's really all about one fact: the L1 norm has a singularity\n  at the origin, while the L2 norm does not. And this is not just true\n  for L1 and L2 regularization: singularities are always necessary to get sparse weights.\n  "
}
</script>

  

  

  

  





  <title>L1 regularization: sparsity through singularities | Erik Jenner</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="ec394e2df73aae24ab882f806438adb0" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>L1 regularization: sparsity through singularities</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 17, 2021
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    
      6
    
    min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Additive \(L_1\) or \(L_2\) penalties are two common regularization methods
and their most famous difference is probably that \(L_1\) regularization
leads to sparse weights (i.e. some weights being exactly 0) whereas \(L_2\)
regularization doesn&rsquo;t. There are many pictures and intuitive explanations
for this out there but while those are great to build some understanding,
I think they conceal the arguably deeper reason why \(L_1\) regularization
leads to sparse weights. But before we discuss that, we need to understand
why \(L_2\) regularization does <em>not</em> help to get sparse weights.</p>
<h2 id="l-2--regularization-doesn-t-lead-to-any-sparsity">\(L_2\) regularization doesn&rsquo;t lead to any sparsity</h2>
<p>Let \(w\) be a vector of parameters and \(\mathcal{L}(w)\) be any continuously
differentiable loss function<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
For \(L_2\) regularization, we want to find
\[\operatorname*{argmax}_w \mathcal{L}(w) + \beta\Vert w\Vert_2^2\]
This means that the gradient has to be zero:
\[\nabla \mathcal{L}(w) + 2\beta w = 0\]
or in components:
\[\left.\frac{\partial\mathcal{L}}{\partial w_i}\right\rvert_{w_i = 0} + 2\beta w_i = 0\]</p>
<p>So we can get \(w_i = 0\) as the optimal solution only if \(\frac{\partial \mathcal{L}}{\partial w_i}\big\rvert_{w_i = 0} = 0\),
i.e. if \(w_i = 0\) is already optimal without regularization! So \(L_2\) regularization
doesn&rsquo;t help to get sparsity <em>at all</em>. The same is true for \(L_p\) regularization for
any \(p &gt; 1\), because
\[\left.\frac{\partial}{\partial w_i} \Vert w\Vert_p^p \right\rvert_{w_i = 0} = 0\]</p>
<h2 id="l-1--regularization-non-differentiability-to-the-rescue">\(L_1\) regularization: non-differentiability to the rescue</h2>
<p>\(L_1\) regularization just uses the 1-norm instead of the Euclidean
norm:
\[\operatorname*{argmax}_w \mathcal{L}(w) + \beta\Vert w\Vert_1\]
How does that change things? Well, the 1-norm of a vector is not
differentiable at 0. More precisely:
\[\frac{\partial}{\partial w_i} \Vert w\Vert_1 = \begin{cases}
+1, \quad w_i &gt; 0\\\
-1,\quad w_i &lt; 0\\\
\text{undefined for } w_i = 0
\end{cases}
\]
So when can \(w_i = 0\) be a local minimum of the regularized loss? We can&rsquo;t just
set the derivative to zero as before, because the derivative doesn&rsquo;t exist.</p>
<p>To understand what we can do instead, let&rsquo;s first recall why setting the derivative
to zero works for differentiable functions. If \(f(x)\) has a local minimum
at 0, then this means that \(f(x) \geq f(0)\) for all sufficiently small \(x\).
Since we assumed \(f\) to be differentiable at \(0\), \(f(x)\) is well approximated<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>
by
\[f(x) \approx f(0) + f'(0)x\]
for small \(x\). So if \(f'(0) &gt; 0\), then \(f(x) &lt; f(0)\) for small negative \(x\),
and if \(f'(0) &lt; 0\) we get the same for positive \(x\). So the derivative at
the minimum has to be zero, because otherwise taking a small step in the right
direction would decrease the value.</p>
<p>We can apply the same idea to the loss with \(L_1\) regularization: what happens
if we take a small step \(h\) away from \(w_i = 0\)? The loss is differentiable
and thus changes approximately linearly:
\[\mathcal{L}\Big\rvert_{w_i = h} \approx \mathcal{L}\Big\rvert_{w_i = 0} + h\frac{\partial\mathcal{L}}{\partial w_i}\]
But for the regularization term, the change is always just \(|h|\), instead
of a linear term:
\[\Vert w\Vert_1 \bigg\rvert_{w_i = h} = \Vert w \Vert_1\bigg\rvert_{w_i = 0} + |h|\]</p>
<p>So if we write \(\tilde{\mathcal{L}}\) for the regularized loss, then we get
\[\tilde{\mathcal{L}}\Big\rvert_{w_i = h} \approx \tilde{\mathcal{L}}\Big\rvert_{w_i = 0} + h\frac{\partial\mathcal{L}}{\partial w_i} + \beta |h|\]
As long as \(\left|\frac{\partial\mathcal{L}}{\partial w_i}\right| &lt; \beta\),
this is larger than the regularized loss at \(w_i = 0\), because
the \(+ \beta |h|\) term will dominate. That is why \(L_1\) regularization
leads to sparser weights: it pulls all those weights to zero whose
partial derivative at 0 has absolute value less than \(\beta\)<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<h2 id="interlude-priors">Interlude: Priors</h2>
<p>If the loss function \(\mathcal{L}\) models some log-likelihood, then
regularization can be interpreted as performing maximum a posteriori (MAP) estimation
rather than maximum likelihood estimation (MLE). This means we start with some
prior over possible values of the parameter \(w\), update this distribution
using the evidence from the loss function, and then pick the parameters
which are the most probable according to the posterior distribution.</p>
<p>\(L_2\) regularization corresponds to a Gaussian prior and \(L_1\) regularization
to a <a href="https:en.wikipedia.org/wiki/Laplace%5Fdistribution" target="_blank" rel="noopener">Laplace prior</a> (in both cases centered around 0). So it&rsquo;s natural to
try to explain the sparsity behavior of these regularization methods
in terms of the underlying priors.</p>
<p>Here&rsquo;s what a Gaussian (red) and Laplace (blue) distribution look like, both with
unit variance and properly normalized:</p>














<figure  id="figure-figure-1-gaussian-and-laplace-distribution-with-unit-variance-created-using-">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Figure 1: Gaussian and Laplace distribution with unit variance (created using )" srcset="
               /post/sparsity-singularities/fig/sparsity-regularization/gaussian_laplace_hudf3bcb2df9dfe189816964ee986ea242_74768_726e162160eb52d0319a97b816cca137.png 400w,
               /post/sparsity-singularities/fig/sparsity-regularization/gaussian_laplace_hudf3bcb2df9dfe189816964ee986ea242_74768_948f91e97afa11b33a27693d78698f5a.png 760w,
               /post/sparsity-singularities/fig/sparsity-regularization/gaussian_laplace_hudf3bcb2df9dfe189816964ee986ea242_74768_1200x1200_fit_lanczos_3.png 1200w"
               src="/post/sparsity-singularities/fig/sparsity-regularization/gaussian_laplace_hudf3bcb2df9dfe189816964ee986ea242_74768_726e162160eb52d0319a97b816cca137.png"
               width="760"
               height="760"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Figure 1: Gaussian and Laplace distribution with unit variance (created using <a href="https://www.desmos.com/">https://www.desmos.com/</a>)
    </figcaption></figure>
<p>One difference is that the Laplace distribution has a higher density
at (and around) 0. I&rsquo;ve seen this used as an explanation for sparsity several times:
the Laplace distribution seems more &ldquo;concentrated&rdquo; around 0, i.e. assigns a higher
prior to 0, which is why we get sparse solutions.</p>
<p>But that is very misleading (and depending on what is meant by &ldquo;concentrated&rdquo; just wrong). Consider the following
figure:</p>














<figure  id="figure-figure-2-narrower-gaussian">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Figure 2: Narrower Gaussian" srcset="
               /post/sparsity-singularities/fig/sparsity-regularization/narrow_gaussian_laplace_hu1bcdef45b5d96c976c595bddc380f10d_61833_ad521c5f5f96aebab09db3967c68bff7.png 400w,
               /post/sparsity-singularities/fig/sparsity-regularization/narrow_gaussian_laplace_hu1bcdef45b5d96c976c595bddc380f10d_61833_f55fb9dccb8e2732aa3500260a17b57c.png 760w,
               /post/sparsity-singularities/fig/sparsity-regularization/narrow_gaussian_laplace_hu1bcdef45b5d96c976c595bddc380f10d_61833_1200x1200_fit_lanczos_3.png 1200w"
               src="/post/sparsity-singularities/fig/sparsity-regularization/narrow_gaussian_laplace_hu1bcdef45b5d96c976c595bddc380f10d_61833_ad521c5f5f96aebab09db3967c68bff7.png"
               width="760"
               height="760"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Figure 2: Narrower Gaussian
    </figcaption></figure>
<p>These are still a normalized Gaussian and a Laplace distribution, the only difference is that
I&rsquo;ve chosen a much smaller variance for the Gaussian. This corresponds to
choosing a higher coefficient \(\beta\) for the \(L_2\) penalty. I&rsquo;d argue that in this case the Gaussian is much more
&ldquo;concentrated around 0&rdquo;, at least its density is much higher. But even with arbitrarily
high \(\beta\), \(L_2\) regularization won&rsquo;t lead to sparse solutions: you can make
the prior as narrow as you like, and you&rsquo;ll get weights that are closer and closer to
zero but never precisely.</p>
<p>The real difference is the singularity (i.e. non-differentiability) of the Laplace distribution
at 0. Since the logarithm is a diffeomorphism, singularities of the prior correspond
1-to-1 with singularities of the log prior, i.e. the regularization term.</p>
<h2 id="singularities-are-necessary-for-sparsity">Singularities are necessary for sparsity</h2>
<p>We&rsquo;ve seen that the difference between \(L_1\) and \(L_2\) regularization can be explained
by the fact that the \(L_1\) norm has a singularity while the \(L_2\) norm doesn&rsquo;t, or equivalently
that the Laplace prior has one while the Gaussian prior doesn&rsquo;t.</p>
<p>But we can say more than that: a singularity is in fact <em>unavoidable</em> if we want to make sparse weights likely.
If we choose any continuously differentiable prior \(p\) (or any continuously differentiable
additive regularization term), then the overall objective \(\tilde{\mathcal{L}}\) is
continuously differentiable and therefore, the gradient has to be zero at a local minimum.
So for \(w_i = 0\) to be a local minimum, we&rsquo;d need
\[\frac{\partial \mathcal{L}}{\partial w_i}\bigg\rvert_{w_i = 0} + \frac{\partial \log p}{\partial w_i}\bigg\rvert_{w_i = 0} = 0\]
which puts an <em>equality</em> constrain on \(\frac{\partial \mathcal{L}}{\partial w_i}\): we only
get sparse weights if the gradient has precisely the right value. Typically, this will
almost surely not happen (in the mathematical sense, i.e. with probability 0), so non-singular
regularization won&rsquo;t lead to sparse weights.</p>
<p>In contrast, we saw that
the singularity of the \(L_1\) norm (or the Laplace prior) creates an <em>inequality</em> constraint
for the partial derivative: it leads to \(w_i = 0\) as long as the derivative lies in a certain
range of values. This is what makes sparse weights likely.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>This is a restriction, for example a model containign ReLUs will typically only be differentiable almost everywhere, and as we&rsquo;ll see, individual non-differentiable points will play a big role. It might be possible to argue that the types of non-differentiable points created by ReLUs don&rsquo;t change the conclusions of following discussion but we&rsquo;ll just assume a differentiable loss so we can focus on the conceptual insights.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>meaning that the error \(f(x) - (f(0) + f'(0)x)\) doesn&rsquo;t just approach 0 for \(x \to 0\) (that would be continuity), it approaches 0 fast enough that even \[\frac{f(x) - (f(0) + f'(0)x)}{x} \to 0\] This is enough to make the rest of argument work out.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Of course this is not guaranteed for complex loss functions: there might be another local optimum somewhere else. This is just the condition for 0 to be one of the local optima at which we might end up.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/machine-learning/">Machine learning</a>
  
</div>













  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://ejenner.com/"><img class="avatar mr-3 avatar-circle" src="/authors/erik/avatar_hub109d6f6def8deb616a16db0d971c7d0_285493_270x270_fill_q75_lanczos_center.jpg" alt="Erik Jenner"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://ejenner.com/">Erik Jenner</a></h5>
      <h6 class="card-subtitle">AI Master&rsquo;s Student</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:erik@ejenner.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/jenner_erik" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=8DgF8HcAAAAJ" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/ejnnr" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/erik-jenner" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/bias-variance-tradeoff/">Troubles with the Bias-Variance tradeoff</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy</a>
    
    
  </p>
  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> â€” the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script>

    <script data-goatcounter="https://ejnnr.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>



</body>
</html>
