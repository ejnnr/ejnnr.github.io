import{t as C,a as H}from"../chunks/ZND8Q6sY.js";import"../chunks/D1wvszJw.js";import{s as n,f as J,c as a,b as s,n as e}from"../chunks/CXEaPN9q.js";import{h as t}from"../chunks/CkMDHBy_.js";import{l as X,s as Z}from"../chunks/lNFSlT5-.js";import{_ as E}from"../chunks/C2OPhFAa.js";const W={title:"Swoosh: Rethinking Activation Functions",summary:`Introducing the new Swoosh activation function. Perfect test set generalization
guaranteed.
  `,date:"2023-04-01T00:00:00.000Z",tags:[],draft:!1,image:{preview_only:"true"}},{title:ma,summary:pa,date:ia,tags:oa,draft:la,image:ra}=W;var K=C(`<p>Improvements to the activation function in neural networks have hit diminishing returns over recent years. Whereas the choice between Sigmoids and ReLUs can make or break your training, the same cannot be said for the difference between Swish, Mish, GLish, and Phish. (I made up one of these, but I bet you don’t know which.)</p> <p>In our recent paper “Swoosh: Rethinking Activation Functions”<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>, we argue that the problem is an overly narrow conception of what an activation function can be. Our key idea is the following observation:</p> <blockquote><p>Different activation functions work best for different problems. A good activation function should thus have <em>many hyperparameters</em>.</p></blockquote> <p>Activation functions with learnable parameters have been proposed before, but they suffer from two flaws:</p> <ol><li>The number of parameters is usually quite small. For example, Swish uses only a single parameter. That’s about 175 billion fewer parameters than even a tiny model like GPT-3! And <a href="https://twitter.com/AndrewSteinwold/status/1594889562526027777" rel="nofollow">it’s well known that more parameters = more powerful</a>.</li> <li>They use <em>parameters</em>. As we will argue, using <em>hyperparameters</em> instead offers countless advantages.</li></ol> <h1>Introducing Swoosh</h1> <p>The simplest version of the Swoosh activation function is <span class="math math-inline"><!></span> where <span class="math math-inline"><!></span> is some simple non-linear function (such as <span class="math math-inline"><!></span>), <span class="math math-inline"><!></span> is a matrix and <span class="math math-inline"><!></span> is a vector. The entries of <span class="math math-inline"><!></span> and <span class="math math-inline"><!></span> are hyperparameters of the Swoosh activation function.</p> <p>For even better results, we recommend using <span class="math math-inline"><!></span> for a simple non-linear function <span class="math math-inline"><!></span>, or even</p> <p><span class="math math-inline"><!></span></p> <p>and so on.
Note that in this case, the hyperparameters for each instance of Swoosh should be tuned independently.</p> <p>Astonishingly, our experiments show that just a single hidden layer is enough for most tasks when using the Swoosh activation function. In slogan form: 2-layer networks are all you need!<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup> The full network is then simply <span class="math math-inline"><!></span></p> <h1>Why hyperparameters?</h1> <p>One of our key contributions is that we treat the entries of <span class="math math-inline"><!></span> and <span class="math math-inline"><!></span> in Swoosh as <em>hyperparameters</em>. This is a much better choice than using them as learnable parameters, like existing activation functions typically do. We’ll briefly sketch a few of the advantages:</p> <ul><li>“Hyper” is greek for “above”, so the name should already tell us that hyperparameters are above (i.e. better than) parameters.</li> <li>Specifically for Swoosh, the network output is differentiable in these hyperparameters, which means you can still use SGD! All common opitimizers and backpropagation implementations work out of the box for optimizing Swoosh hyperparameters.</li> <li>Neural networks with higher parameter count tend to have higher memory requirements, longer training and inference times, higher compute costs, and a bigger environmental footprint. By using hyperparameters instead of parameters, we can keep the parameter count low.</li> <li>Models sometimes perform worse on the test dataset than on the training data.<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> Using the well-known technique of choosing hyperparameters based on test performance, this is an issue of the past with Swoosh.</li></ul> <h1>Conclusion and future work</h1> <p>We have shown how expanding our conception of what an activation function can be solves several key problems in deep learning, such as test set generalization and high compute requirements. We believe that this insight will not only revitalize the field of activation functions, but that it in fact heralds the end of the AI Winter we currently find ourselves in.</p> <p>To make it as easy as possible for practitioners to adopt Swoosh, we have worked closely with the PyTorch team. We are excited to announce that Swoosh is <a href="https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html" rel="nofollow">available now in the PyTorch library</a>! Tensorflow and JAX users can easily implement it in a few lines of code.</p> <p>In the future, we are planning to improve the design of Swoosh in various ways. For example, we believe it could be promising to replace the matrix multiplication <span class="math math-inline"><!></span> with a convolution, especially on inputs with translational symmetry such as images. There is a vast unexplored space of possible Swoosh architectures to discover, and we invite other researchers to join us in this exciting journey!</p> <div class="footnotes"><hr> <ol><li id="fn-1">We will release the full paper upon acceptance of this blog post.<a href="#fnref-1" class="footnote-backref">↩</a></li> <li id="fn-2">In fact, in an upcoming paper we will show that for Swoosh activation functions, there are universal approximation theorems with bounded width and depth 1, improving on a result by <a href="https://www.semanticscholar.org/paper/Lower-bounds-for-approximation-by-MLP-neural-Maiorov-Pinkus/4bdd1f845d26e488d67c0e4549cff17407b980ad" rel="nofollow">Maiorov et al.</a><a href="#fnref-2" class="footnote-backref">↩</a></li> <li id="fn-3">Or so I’ve heard, this has never happened to me personally.<a href="#fnref-3" class="footnote-backref">↩</a></li></ol></div>`,1);function ha(L,z){const T=X(z,["children","$$slots","$$events","$$legacy"]);E(L,Z(()=>T,W,{children:($,Q)=>{var y=K(),m=n(J(y),12),p=n(a(m)),A=a(p);t(A,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><mi>f</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>W</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\\operatorname{Swoosh}_{f}(x) := f(Wf(x) + b),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span>'),s(p);var i=n(p,2),I=a(i);t(I,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>'),s(i);var o=n(i,2),P=a(o);t(P,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x) := \\max(x, 0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span>'),s(o);var l=n(o,2),q=a(l);t(q,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>'),s(l);var r=n(l,2),F=a(r);t(F,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span>'),s(r);var h=n(r,2),U=a(h);t(U,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>'),s(h);var v=n(h,2),j=a(v);t(j,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span>'),s(v),e(),s(m);var c=n(m,2),g=n(a(c)),G=a(g);t(G,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">f = \\operatorname{Swoosh}_{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>'),s(g);var x=n(g,2),O=a(x);t(O,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span>'),s(x),e(),s(c);var w=n(c,2),b=a(w),R=a(b);t(R,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><msub><mrow><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo></mrow><mi>g</mi></msub></msub></mrow><annotation encoding="application/x-tex">f = \\operatorname{Swoosh}_{\\operatorname{Swoosh}_g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0418em;vertical-align:-0.3473em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop mtight"><span class="mord mathrm mtight">Swoosh</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3473em;"><span></span></span></span></span></span></span></span></span></span>'),s(b),s(w);var d=n(w,4),k=n(a(d),3),V=a(k);t(V,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">net</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>U</mi><mi mathvariant="normal">Swoosh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>V</mi><mi>x</mi><mo>+</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\\operatorname{net}(x) = U \\operatorname{Swoosh}(Vx + a) + b.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">net</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mord mathrm">Swoosh</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mord">.</span></span></span></span>'),s(k),s(d);var u=n(d,4),f=n(a(u)),D=a(f);t(D,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>'),s(f);var M=n(f,2),N=a(M);t(N,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span>'),s(M),e(3),s(u);var _=n(u,10),S=n(a(_)),B=a(S);t(B,()=>'<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Wx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span></span></span></span>'),s(S),e(),s(_),e(2),H($,y)},$$slots:{default:!0}}))}export{ha as component};
