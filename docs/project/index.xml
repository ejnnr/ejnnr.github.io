<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Erik Jenner</title>
    <link>https://ejenner.com/project/</link>
      <atom:link href="https://ejenner.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ejenner.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://ejenner.com/project/</link>
    </image>
    
    <item>
      <title>Extensions of Karger&#39;s algorithm</title>
      <link>https://ejenner.com/project/karger/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://ejenner.com/project/karger/</guid>
      <description>&lt;p&gt;Karger&amp;rsquo;s algorithm is a randomized algorithm for finding global mincuts
in graphs. For my Bachelor&amp;rsquo;s thesis, I aimed to find out if and how
Karger&amp;rsquo;s algorithm can be generalized to other types of problems,
for example $s$-$t$-mincuts or normalized cuts. It turns out that this
is &lt;em&gt;provably&lt;/em&gt; impossible in a formal sense. But in addition to this
impossibility result, my research also led me to a new algorithm
for seeded segmentation that uses Karger&amp;rsquo;s algorithm to sample many
different cuts and then takes their &amp;ldquo;average&amp;rdquo;. A paper with these findings
was accepted as an Oral at ICCV 2021.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML Reproducibility Challenge</title>
      <link>https://ejenner.com/project/fact-ai/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://ejenner.com/project/fact-ai/</guid>
      <description>&lt;p&gt;As part of the course &lt;em&gt;Fairness, Accountability, Confidentiality and Transparency in AI&lt;/em&gt;
at the University of Amsterdam, I replicated the paper
&lt;a href=&#34;https://arxiv.org/abs/2006.13114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fairness without Demographics through Adversarially Reweighted Learning&lt;/a&gt;
(Lahoti et al., 2020) together with three other Master&amp;rsquo;s students.
We also performed additional experiments to test whether the method transfers
from tabular to image data. We wrote a report with our findings for the
&lt;a href=&#34;https://paperswithcode.com/rc2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML Reproducibility Challenge 2020&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Steerable PDOs</title>
      <link>https://ejenner.com/project/steerable-pdos/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://ejenner.com/project/steerable-pdos/</guid>
      <description>&lt;p&gt;Equivariant deep learning has many similarities to theoretical physics,
which have already lead to ideas from physics being applied to the field.
But they typically use convolutions, whereas physics uses differential
operators. In an internship at &lt;a href=&#34;https://ivi.fnwi.uva.nl/quva/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QUVA Lab&lt;/a&gt;,
I developed the theory of equivariant differential operators &amp;ndash; &lt;em&gt;Steerable PDOs&lt;/em&gt;
&amp;ndash; in order to close this gap. I also implemented steerable PDOs as
an extension of the &lt;a href=&#34;https://github.com/QUVA-Lab/e2cnn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;e2cnn&lt;/em&gt; library&lt;/a&gt;
to allow others to easily apply them in their own work.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
